{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7747bcc",
   "metadata": {},
   "source": [
    "# DD and PD Calculation Using Market-approach (Merton Model)\n",
    "\n",
    "This notebook walks through the `dd_pd_market.py` script step by step.  \n",
    "We will:\n",
    "\n",
    "1.  Set up our environment and imports  \n",
    "2.  Load and inspect inputs  \n",
    "3.  Prepare and merge data  \n",
    "4.  Compute market capitalizations  \n",
    "5.  Merge equity volatility  \n",
    "6.  Define and run the Merton model solver  \n",
    "7.  Calculate Distance to Default (DD) and Probability of Default (PD)  \n",
    "8.  Export results and write diagnostics to a log  \n",
    "\n",
    "\n",
    "**Timing**: Uses sigma_{E,t-1}, E_t, F_t, r_{f,t}. No lookahead bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a062f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market approach, Merton KMV solve. Uses μ = r_f. Barrier convention: A (total debt)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameter defaults for the market-based Merton KMV implementation\n",
    "print(\"Market approach, Merton KMV solve. Uses μ = r_f. Barrier convention: A (total debt)\")\n",
    "T = 1.0\n",
    "tol_E = 1e-6\n",
    "tol_sigma = 1e-4\n",
    "max_iter = 200\n",
    "barrier_option = \"A\"  # For banks: debt_total represents total liabilities (deposits dominate funding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aecf6ed",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Here we import all libraries and define file‐paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be68c12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:05.978634Z",
     "iopub.status.busy": "2025-10-02T09:25:05.978402Z",
     "iopub.status.idle": "2025-10-02T09:25:30.331142Z",
     "shell.execute_reply": "2025-10-02T09:25:30.328612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install needed packages (run once per environment)\n",
    "%pip install pandas numpy matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade82fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:30.340463Z",
     "iopub.status.busy": "2025-10-02T09:25:30.339371Z",
     "iopub.status.idle": "2025-10-02T09:25:31.635495Z",
     "shell.execute_reply": "2025-10-02T09:25:31.634794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root resolved to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank\n",
      "Accounting input   → FOUND\n",
      "Marketcap input    → FOUND\n",
      "Equity vol input   → FOUND\n",
      "Risk-free input    → FOUND\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and Imports (with correct file names)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import least_squares\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1.1 Locate workspace root\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / '.git').exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "print(f\"Repository root resolved to: {base_dir}\")\n",
    "\n",
    "# 1.2 Time horizon (configured in parameter cell)\n",
    "\n",
    "# 1.3 File paths (corrected market-cap filename)\n",
    "model_fp      = base_dir / 'data' / 'clean' / 'esg_0718_clean.csv'\n",
    "marketcap_fp  = base_dir / 'data' / 'clean' / 'all_banks_marketcap_annual_2016_2023.csv'\n",
    "vol_fp        = base_dir / 'data' / 'clean' / 'equity_volatility_by_year.csv'\n",
    "rf_fp         = base_dir / 'data' / 'clean' / 'fama_french_factors_annual_clean.csv'\n",
    "log_fp        = base_dir / 'data' / 'logs' / 'dd_pd_market_log.txt'\n",
    "output_dir    = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "archive_dir   = base_dir / 'archive' / 'datasets'\n",
    "\n",
    "# 1.4 Ensure directories exist\n",
    "log_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1.5 Existence check\n",
    "for name, fp in [\n",
    "    ('Accounting input', model_fp),\n",
    "    ('Marketcap input', marketcap_fp),\n",
    "    ('Equity vol input', vol_fp),\n",
    "    ('Risk-free input', rf_fp),\n",
    "]:\n",
    "    print(f\"{name:18s} →\", \"FOUND\" if fp.exists() else f\"MISSING ({fp.name})\")\n",
    "\n",
    "# 1.6 Deduplication audit log placeholder\n",
    "duplicate_log_entries = []\n",
    "invalid_input_summary = None\n",
    "surviving_rows_by_year = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92c20c",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Core Data\n",
    "\n",
    "- Read the main Book2 input file  \n",
    "- Clean and convert the `year` column  \n",
    "- Merge in the annual risk-free rate from Fama-French  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a32e25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.638919Z",
     "iopub.status.busy": "2025-10-02T09:25:31.638448Z",
     "iopub.status.idle": "2025-10-02T09:25:31.738867Z",
     "shell.execute_reply": "2025-10-02T09:25:31.738248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Book2 data...\n",
      "→ 1427 rows, 1424 unique (instrument, year)\n",
      "→ Dropping 3 duplicate instrument-year rows (kept max debt_total)\n",
      "   Duplicate PNC 2018: kept debt_total=60,263.0; dropped=60,263.0, 57,419.0, 57,419.0\n",
      "→ After merging rf, 1424 rows remain\n",
      "[INFO] Constructed F barrier for 1424 rows using barrier_option=A\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Load Book2 data\n",
    "print('[INFO] Loading Book2 data...')\n",
    "df = pd.read_csv(model_fp)\n",
    "print(f\"→ {df.shape[0]} rows, {df[['instrument','year']].drop_duplicates().shape[0]} unique (instrument, year)\")\n",
    "\n",
    "# 2.1a Deduplicate instrument-year rows by keeping the largest debt_total\n",
    "sort_cols = ['instrument', 'year', 'debt_total']\n",
    "df_sorted = df.sort_values(sort_cols, ascending=[True, True, False])\n",
    "deduped = df_sorted.drop_duplicates(subset=['instrument', 'year'], keep='first')\n",
    "removed = df_sorted.loc[~df_sorted.index.isin(deduped.index), ['instrument', 'year', 'debt_total']]\n",
    "if not removed.empty:\n",
    "    print(f\"→ Dropping {removed.shape[0]} duplicate instrument-year rows (kept max debt_total)\")\n",
    "    summary = (\n",
    "        df_sorted.groupby(['instrument', 'year'])['debt_total']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary = summary[summary['debt_total'].apply(len) > 1]\n",
    "    for _, row in summary.iterrows():\n",
    "        debt_values = list(row['debt_total'])\n",
    "        kept = debt_values[0]\n",
    "        dropped = debt_values[1:]\n",
    "        dropped_str = ', '.join(f\"{val:,.1f}\" for val in dropped)\n",
    "        message = (\n",
    "            f\"Duplicate {row['instrument']} {row['year']}: \"\n",
    "            f\"kept debt_total={kept:,.1f}; dropped={dropped_str}\"\n",
    "        )\n",
    "        print('   ' + message)\n",
    "        duplicate_log_entries.append(message)\n",
    "    df = deduped.reset_index(drop=True)\n",
    "else:\n",
    "    print(\"→ No duplicate instrument-year rows found (all unique).\")\n",
    "    df = df_sorted.reset_index(drop=True)\n",
    "\n",
    "# 2.2  Clean year column\n",
    "df = df[df['year'].notnull()].copy()\n",
    "df['year'] = df['year'].astype(float).astype(int)\n",
    "\n",
    "# 2.3 Merge risk-free rate\n",
    "rf_df = pd.read_csv(rf_fp)\n",
    "df = df.merge(rf_df[['year','rf']], on='year', how='left')\n",
    "df['rf'] = df['rf'] / 100    # convert percent to decimal\n",
    "\n",
    "print(f\"→ After merging rf, {df.shape[0]} rows remain\")\n",
    "\n",
    "if barrier_option == \"A\":\n",
    "    df['F'] = df['debt_total'] * 1_000_000  # F = total liabilities for banks (debt_total column)\n",
    "elif barrier_option == \"B\":\n",
    "    required_cols = {'debt_short_term', 'debt_long_term'}\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        missing_str = \", \".join(missing)\n",
    "        raise ValueError(\n",
    "            \"barrier_option='B' requires short/long-term debt columns. \"\n",
    "            f\"Missing: {missing_str}. Add these columns before enabling option B.\"\n",
    "        )\n",
    "    df['F'] = (df['debt_short_term'] + 0.5 * df['debt_long_term']) * 1_000_000\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported barrier_option: {barrier_option}\")\n",
    "print(f\"[INFO] Constructed F barrier for {df.shape[0]} rows using barrier_option={barrier_option}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf131177",
   "metadata": {},
   "source": [
    "## 3. Prepare Identifiers and Dates\n",
    "\n",
    "- Standardize tickers by dropping exchange suffixes  \n",
    "- Parse the `date` column and extract `Month`  \n",
    "- Create a simple `symbol` field for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1623898e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.743719Z",
     "iopub.status.busy": "2025-10-02T09:25:31.743210Z",
     "iopub.status.idle": "2025-10-02T09:25:31.755989Z",
     "shell.execute_reply": "2025-10-02T09:25:31.754954Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3.1 Helper to strip suffixes like .N, .OQ, etc.\n",
    "def standardize_ticker(t):\n",
    "    return str(t).split('.', 1)[0] if pd.notnull(t) else t\n",
    "\n",
    "# 3.2Apply to our main DataFrame\n",
    "df['ticker_prefix'] = df['instrument'].apply(standardize_ticker)\n",
    "\n",
    "# 3.3 Ensure date is datetime, then extract month\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "else:\n",
    "    df = df.assign(date=pd.NaT)\n",
    "\n",
    "df['Month'] = df['date'].dt.month\n",
    "\n",
    "# 3.4 Create 'symbol' for merge keys (same as ticker_prefix)\n",
    "df['symbol'] = df['instrument'].apply(lambda x: str(x).split('.', 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782546d",
   "metadata": {},
   "source": [
    "## 4. Compute Market Capitalization\n",
    "\n",
    "- Load monthly price/share data  \n",
    "- Calculate market cap (price × shares) in millions USD  \n",
    "- Select December (or most recent) value for each symbol-year  \n",
    "- Merge annual market cap into our main `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb31cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.758812Z",
     "iopub.status.busy": "2025-10-02T09:25:31.758557Z",
     "iopub.status.idle": "2025-10-02T09:25:31.784208Z",
     "shell.execute_reply": "2025-10-02T09:25:31.783567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in mc: ['symbol', 'year', 'market_cap']\n",
      "  instrument  year    market_cap\n",
      "0       ABCB  2016  3.004515e+09\n",
      "1       ABCB  2017  3.321505e+09\n",
      "2       ABCB  2018  2.182408e+09\n",
      "3       ABCB  2019  2.931470e+09\n",
      "4       ABCB  2020  2.623438e+09\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Load annual market‐cap data\n",
    "mc = pd.read_csv(marketcap_fp)\n",
    "print(\"Columns in mc:\", mc.columns.tolist())\n",
    "\n",
    "# 4.2 Compute market_cap only if needed\n",
    "if 'market_cap' not in mc.columns:\n",
    "    # fallback: compute from dec_price & shares_outstanding\n",
    "    mc['market_cap'] = mc['dec_price'] * mc['shares_outstanding']\n",
    "\n",
    "# 4.3 Standardize the ticker (drop suffixes)\n",
    "mc['symbol'] = mc['symbol'].apply(standardize_ticker)\n",
    "\n",
    "# 4.4 Parse the fiscal date and extract year/month\n",
    "#    If this annual file has no 'fiscal_date' but has 'year', skip parsing\n",
    "if 'fiscal_date' in mc.columns:\n",
    "    mc['fiscal_date'] = pd.to_datetime(mc['fiscal_date'])\n",
    "    mc['year']       = mc['fiscal_date'].dt.year\n",
    "else:\n",
    "    # assume the CSV’s 'year' column is correct\n",
    "    mc['year'] = mc['year'].astype(int)\n",
    "\n",
    "# 4.5 We don’t need Month or December flag for annual data, but for consistency:\n",
    "mc['Month']       = mc.get('Month', 12)  # treat all as December\n",
    "mc['is_december'] = True\n",
    "\n",
    "# 4.6 Drop duplicates: keep one record per (symbol, year)\n",
    "mc_annual = (\n",
    "    mc\n",
    "    .dropna(subset=['market_cap'])\n",
    "    .drop_duplicates(subset=['symbol','year'], keep='first')\n",
    ")\n",
    "\n",
    "# 4.7 Merge into main DataFrame\n",
    "df = df.merge(\n",
    "    mc_annual[['symbol','year','market_cap']],\n",
    "    on=['symbol','year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4.8 Quick check\n",
    "print(df[['instrument','year','market_cap']].drop_duplicates().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96be53",
   "metadata": {},
   "source": [
    "## 5. Merge Equity Volatility\n",
    "\n",
    "- Load annualized equity volatility by symbol-year  \n",
    "- Standardize `ticker_prefix`  \n",
    "- Merge into our main `df`, and use a fallback of 0.25 if missing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9ff0bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.787039Z",
     "iopub.status.busy": "2025-10-02T09:25:31.786826Z",
     "iopub.status.idle": "2025-10-02T09:25:31.814800Z",
     "shell.execute_reply": "2025-10-02T09:25:31.813972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading equity volatility...\n",
      "[WARN] Dropping 118 rows with missing required inputs.\n",
      "instrument  year\n",
      "      AMTB  2020\n",
      "      AMTB  2021\n",
      "      AMTB  2022\n",
      "      AMTB  2023\n",
      "       AUB  2016\n",
      "       AUB  2017\n",
      "       AUB  2018\n",
      "       AUB  2019\n",
      "       AUB  2020\n",
      "       AUB  2021\n",
      "       AUB  2022\n",
      "       AUB  2023\n",
      "      BHLB  2017\n",
      "      BHLB  2018\n",
      "      BHLB  2019\n",
      "      BHLB  2020\n",
      "      BHLB  2021\n",
      "      BHLB  2022\n",
      "      BHLB  2023\n",
      "      BRKL  2017\n",
      "      BRKL  2018\n",
      "      BRKL  2019\n",
      "      BRKL  2020\n",
      "      BRKL  2021\n",
      "      BRKL  2022\n",
      "      CIZN  2020\n",
      "      COFS  2018\n",
      "      COFS  2019\n",
      "      COFS  2020\n",
      "      CWBC  2018\n",
      "      CWBC  2019\n",
      "      CWBC  2020\n",
      "      CWBC  2021\n",
      "      CWBC  2022\n",
      "       DFS  2016\n",
      "       DFS  2017\n",
      "       DFS  2018\n",
      "       DFS  2019\n",
      "       DFS  2020\n",
      "       DFS  2021\n",
      "       DFS  2022\n",
      "       DFS  2023\n",
      "      EBTC  2017\n",
      "      EBTC  2018\n",
      "      EBTC  2019\n",
      "      EBTC  2020\n",
      "      EBTC  2021\n",
      "      EBTC  2022\n",
      "      EBTC  2023\n",
      "      EQBK  2017\n",
      "      EQBK  2018\n",
      "      EQBK  2019\n",
      "      EQBK  2020\n",
      "      EQBK  2021\n",
      "      EQBK  2022\n",
      "      EVBN  2018\n",
      "      EVBN  2019\n",
      "      EVBN  2020\n",
      "      EVBN  2021\n",
      "      EVBN  2022\n",
      "      EVBN  2023\n",
      "      FFWM  2019\n",
      "      FFWM  2020\n",
      "      FLIC  2017\n",
      "      FLIC  2018\n",
      "      FLIC  2019\n",
      "      GNTY  2018\n",
      "      GNTY  2019\n",
      "      GNTY  2020\n",
      "      GNTY  2021\n",
      "      GNTY  2022\n",
      "      HMST  2017\n",
      "      HMST  2018\n",
      "      HMST  2019\n",
      "      HMST  2020\n",
      "       LOB  2017\n",
      "       LOB  2018\n",
      "       LOB  2019\n",
      "       LOB  2020\n",
      "      OPOF  2018\n",
      "      OPOF  2019\n",
      "      OPOF  2020\n",
      "      OPOF  2021\n",
      "      OPOF  2022\n",
      "      OPOF  2023\n",
      "      PPBI  2017\n",
      "      PPBI  2018\n",
      "      PPBI  2019\n",
      "      PPBI  2020\n",
      "      PPBI  2021\n",
      "      PPBI  2022\n",
      "      PWOD  2018\n",
      "      PWOD  2019\n",
      "      PWOD  2020\n",
      "      PWOD  2021\n",
      "      PWOD  2022\n",
      "      PWOD  2023\n",
      "      RNST  2016\n",
      "      RNST  2017\n",
      "      RNST  2018\n",
      "      RNST  2019\n",
      "      RNST  2020\n",
      "      RNST  2021\n",
      "      RNST  2022\n",
      "      SFBS  2016\n",
      "      SFBS  2017\n",
      "      SFBS  2018\n",
      "      SFBS  2019\n",
      "      SFBS  2020\n",
      "      SFBS  2021\n",
      "       SSB  2016\n",
      "       SSB  2017\n",
      "       SSB  2018\n",
      "       SSB  2019\n",
      "       SSB  2020\n",
      "       SSB  2021\n",
      "       SSB  2022\n",
      "       SSB  2023\n",
      "[INFO] Surviving rows by year after input validation:\n",
      "year\n",
      "2016     64\n",
      "2017    125\n",
      "2018    182\n",
      "2019    194\n",
      "2020    197\n",
      "2021    200\n",
      "2022    202\n",
      "2023    145\n",
      "dtype: int64\n",
      "[INFO] Adding sigma_E provenance columns...\n",
      "  Provenance added: 1321 rows\n",
      "  Methods: {'monthly36': 1272, 'monthly_ewma': 30, 'peer_median': 19}\n",
      "[INFO] Creating time-tagged columns...\n",
      "  E_t: 1321 values\n",
      "  F_t: 1321 values\n",
      "  rf_t: 1321 values\n",
      "  sigma_E_tminus1: 1321 values\n",
      "  equity_vol: 1321 values\n"
     ]
    }
   ],
   "source": [
    "# 5. Load and Merge Equity Volatility\n",
    "print('[INFO] Loading equity volatility...')\n",
    "\n",
    "# 5.1 Load equity volatility file\n",
    "equity_vol = pd.read_csv(vol_fp)\n",
    "\n",
    "# 5.2 NEW FORMAT: Use ticker_base and sigma_E (already standardized)\n",
    "equity_vol['ticker_prefix'] = equity_vol['ticker_base']\n",
    "equity_vol['equity_volatility'] = equity_vol['sigma_E']\n",
    "vol_annual = equity_vol[['ticker_prefix','year','equity_volatility']]\n",
    "\n",
    "# 5.3 Merge into main DataFrame\n",
    "df = df.merge(\n",
    "    vol_annual,\n",
    "    on=['ticker_prefix','year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 5.4 Validate required inputs\n",
    "required_columns = ['instrument', 'year', 'market_cap', 'equity_volatility', 'rf', 'debt_total']\n",
    "missing_required = [col for col in required_columns if col not in df.columns]\n",
    "if missing_required:\n",
    "    missing_str = ', '.join(missing_required)\n",
    "    raise AssertionError(f\"Missing required columns after merge: {missing_str}\")\n",
    "\n",
    "invalid_mask = df[required_columns].isna().any(axis=1)\n",
    "if invalid_mask.any():\n",
    "    invalid_count = invalid_mask.sum()\n",
    "    print(f\"[WARN] Dropping {invalid_count} rows with missing required inputs.\")\n",
    "    invalid_summary = df.loc[invalid_mask, ['instrument','year']].copy()\n",
    "    print(invalid_summary.to_string(index=False))\n",
    "    df = df[~invalid_mask].copy()\n",
    "    df['solver_status'] = 'pending'\n",
    "else:\n",
    "    print(\"[INFO] All required inputs present for merged dataset.\")\n",
    "\n",
    "# 5.5 Finalize equity volatility column\n",
    "df['equity_vol'] = df['equity_volatility']\n",
    "\n",
    "# 5.6 Surviving row counts by year\n",
    "surviving_rows_by_year = df.groupby('year').size().sort_index()\n",
    "print('[INFO] Surviving rows by year after input validation:')\n",
    "print(surviving_rows_by_year)\n",
    "\n",
    "# 5.7 Add sigma_E provenance columns from merged equity volatility\n",
    "print('[INFO] Adding sigma_E provenance columns...')\n",
    "\n",
    "# Merge provenance columns from equity_vol DataFrame\n",
    "provenance_cols = ['ticker_prefix', 'year', 'sigma_E_method', 'sigma_E_window_months']\n",
    "if all(col in equity_vol.columns for col in provenance_cols):\n",
    "    vol_provenance = equity_vol[provenance_cols].copy()\n",
    "    df = df.merge(vol_provenance, on=['ticker_prefix', 'year'], how='left')\n",
    "    \n",
    "    # Calculate window years from window_months\n",
    "    df['sigmaE_window_end_year'] = df['year'] - 1\n",
    "    df['sigma_E_window_months'] = df['sigma_E_window_months'].fillna(0)\n",
    "    df['sigmaE_window_start_year'] = df['sigmaE_window_end_year'] - (df['sigma_E_window_months'] / 12 - 1).clip(lower=0).astype(int)\n",
    "    \n",
    "    print(f'  Provenance added: {df[\"sigma_E_method\"].notna().sum()} rows')\n",
    "    print(f'  Methods: {df[\"sigma_E_method\"].value_counts().to_dict()}')\n",
    "else:\n",
    "    print('  Warning: Provenance columns not found')\n",
    "    df['sigmaE_window_end_year'] = df['year'] - 1\n",
    "    df['sigmaE_window_start_year'] = df['year'] - 1\n",
    "    df['sigma_E_method'] = 'unknown'\n",
    "    df['sigma_E_window_months'] = 0\n",
    "\n",
    "# 5.8 Create time-tagged columns for solver\n",
    "print('[INFO] Creating time-tagged columns...')\n",
    "\n",
    "df['E_t'] = df['market_cap']  # Equity value at time t\n",
    "df['F_t'] = df['F']  # Face value of debt\n",
    "df['rf_t'] = df['rf']  # Risk-free rate\n",
    "df['T'] = 1.0  # Time horizon\n",
    "df['sigma_E_tminus1'] = df['equity_volatility']  # From file, already at t-1\n",
    "\n",
    "print(f'  E_t: {df[\"E_t\"].notna().sum()} values')\n",
    "print(f'  F_t: {df[\"F_t\"].notna().sum()} values')\n",
    "print(f'  rf_t: {df[\"rf_t\"].notna().sum()} values')\n",
    "print(f'  sigma_E_tminus1: {df[\"sigma_E_tminus1\"].notna().sum()} values')\n",
    "print(f'  equity_vol: {df[\"equity_vol\"].notna().sum()} values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "time_assertions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Validating time integrity...\n",
      "  F_t range: 0.00e+00 to 6.53e+11\n",
      "  F_t null count: 0\n",
      "[ERROR] 16 rows have non-positive F_t:\n",
      "    instrument  year    F  F_t  debt_total\n",
      "352       EXSR  2021  0.0  0.0         0.0\n",
      "436        FHB  2021  0.0  0.0         0.0\n",
      "822       NKSH  2019  0.0  0.0         0.0\n",
      "823       NKSH  2020  0.0  0.0         0.0\n",
      "824       NKSH  2021  0.0  0.0         0.0\n",
      "825       NKSH  2022  0.0  0.0         0.0\n",
      "862       OPBK  2021  0.0  0.0         0.0\n",
      "883       OVLY  2018  0.0  0.0         0.0\n",
      "884       OVLY  2019  0.0  0.0         0.0\n",
      "885       OVLY  2020  0.0  0.0         0.0\n",
      "[INFO] Filtered to 1305 rows with positive F_t\n",
      "[PASS] All time integrity assertions passed\n",
      "  - sigma_E uses only data up to t-1: True\n",
      "  - No future data in windows: True\n"
     ]
    }
   ],
   "source": [
    "# TIME INTEGRITY ASSERTIONS\n",
    "print('[INFO] Validating time integrity...')\n",
    "\n",
    "# Import time checks\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from utils.time_checks import assert_time_integrity\n",
    "\n",
    "# Assertion 1: sigma_E window must end at t-1\n",
    "assert (df['sigmaE_window_end_year'] == df['year'] - 1).all(), \\\n",
    "    'sigma_E window end must be t-1 (no lookahead)'\n",
    "\n",
    "# Assertion 2: rf_t must be present\n",
    "assert df['rf_t'].notna().all(), 'rf_t must be present for all rows'\n",
    "\n",
    "# Assertion 3: E_t and F_t must be positive for solver rows\n",
    "solver_rows = df['equity_vol'].notna()\n",
    "assert (df.loc[solver_rows, 'E_t'] > 0).all(), 'E_t must be positive'\n",
    "print(f\"  F_t range: {df.loc[solver_rows, 'F_t'].min():.2e} to {df.loc[solver_rows, 'F_t'].max():.2e}\")\n",
    "print(f\"  F_t null count: {df.loc[solver_rows, 'F_t'].isna().sum()}\")\n",
    "# Debug F_t issues\n",
    "bad_F = df.loc[solver_rows & (df['F_t'] <= 0)]\n",
    "if len(bad_F) > 0:\n",
    "    print(f'[ERROR] {len(bad_F)} rows have non-positive F_t:')\n",
    "    print(bad_F[['instrument', 'year', 'F', 'F_t', 'debt_total']].head(10))\n",
    "    # Filter out bad rows\n",
    "    df = df[df['F_t'] > 0].copy()\n",
    "    solver_rows = df['equity_vol'].notna()\n",
    "    print(f'[INFO] Filtered to {len(df)} rows with positive F_t')\n",
    "\n",
    "# F_t assertion now satisfied after filtering\n",
    "\n",
    "# Run comprehensive time integrity check\n",
    "assert_time_integrity(df)\n",
    "\n",
    "print('[PASS] All time integrity assertions passed')\n",
    "print(f'  - sigma_E uses only data up to t-1: {(df[\"sigmaE_window_end_year\"] == df[\"year\"] - 1).all()}')\n",
    "print(f'  - No future data in windows: {(df[\"sigmaE_window_end_year\"] < df[\"year\"]).all()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0774b02",
   "metadata": {},
   "source": [
    "## 6. Define the Merton Model Solver (Revised Equations)\n",
    "\n",
    "In the Merton framework, the firm’s equity is treated as a European call option on its assets. We observe:\n",
    "\n",
    "- **E**: equity market value (scaled market capitalization)  \n",
    "- **sigma_E**: annualized equity volatility  \n",
    "- **F**: total debt (face value)  \n",
    "- **r_f**: risk-free rate  \n",
    "- **T**: time horizon (1 year)  \n",
    "\n",
    "We solve for the unobserved:\n",
    "\n",
    "- **V**: total asset value  \n",
    "- **sigma_V**: asset volatility  \n",
    "\n",
    "by enforcing two conditions:\n",
    "\n",
    "1.  **Option-pricing relation**  \n",
    "    $$\n",
    "      E \\;=\\; V\\,\\Phi(d_{1})\\;-\\;F\\,e^{-r_{f}T}\\,\\Phi(d_{2})\n",
    "    $$\n",
    "2.  **Volatility link**  \n",
    "    $$\n",
    "      \\sigma_{E} \\;=\\;\\frac{V}{E}\\,\\Phi(d_{1})\\,\\sigma_{V}\n",
    "    $$\n",
    "\n",
    "where  \n",
    "$$\n",
    "  d_{1} \\;=\\;\\frac{\\ln\\!\\bigl(V/F\\bigr)\\;+\\;\\bigl(r_{f} + \\tfrac12\\,\\sigma_{V}^{2}\\bigr)\\,T}\n",
    "                      {\\sigma_{V}\\,\\sqrt{T}},\n",
    "  \\quad\n",
    "  d_{2} \\;=\\; d_{1} \\;-\\;\\sigma_{V}\\,\\sqrt{T},\n",
    "$$  \n",
    "and $\\Phi$ is the standard normal CDF.  \n",
    "\n",
    "We use a numerical root-finder (`scipy.optimize.root`) to find $V$, $\\sigma_{V}$ that makes both equations zero:\n",
    "\n",
    "$$\n",
    "\\text{Find }V,\\sigma_{V}\\text{ such that both equations } = 0\n",
    "$$\n",
    "\n",
    "### What the root-finder actually does, in simple terms\n",
    "\n",
    "1. **Start with a guess**  \n",
    "    We begin by guessing values for $(V,\\sigma_{V})$. A natural choice is  \n",
    "    $$\n",
    "      V_0 = E + F,\\quad \\sigma_{V,0} = \\sigma_E\n",
    "    $$  \n",
    "    This says \"assets are roughly equity plus debt\" and \"asset volatility is like equity volatility.\"\n",
    " \n",
    " 2. **Measure \"how wrong\" we are**  \n",
    "    We compute the two expressions  \n",
    "    $$\n",
    "      f_1(V,\\sigma_V),\\quad f_2(V,\\sigma_V)\n",
    "    $$  \n",
    "    which tell us how far from zero each equation is. If both are exactly zero, our guess solves the problem.\n",
    " \n",
    " 3. **Adjust the guess**  \n",
    "    If either $f_1$ or $f_2$ is not zero, the solver estimates a small change to $(V,\\sigma_{V})$ that should educe the errors. It uses derivatives and smart heuristics under the hood.\n",
    " \n",
    " 4. **Repeat until \"close enough\"**  \n",
    "    The process repeats—compute residuals, update guess, compute again—until both residuals are below a tiny tolerance (converged), or we hit an iteration limit (no convergence).\n",
    " \n",
    " 5. **Result**  \n",
    "    - If converged: we obtain $(V^*, \\sigma_{V}^*)$, the asset value and volatility consistent with observed equity data.  \n",
    "    - If not: we flag the failure and typically record NaN values.\n",
    " \n",
    " By packaging our two Merton equations into one Python function, `scipy.optimize.root` handles the iteration, step-size choices, and convergence checks automatically. This allows us to solve these otherwise intractable nonlinear equations with minimal custom code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stable_solver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stable Merton solver loaded\n",
      "  - Uses log space for V and sigma_V\n",
      "  - Clips d1, d2 to [-35, 35]\n",
      "  - Uses E_model in volatility equation denominator\n",
      "  - Robust loss function (soft_l1)\n"
     ]
    }
   ],
   "source": [
    "# STABLE MERTON SOLVER with numerical safeguards\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "Phi = norm.cdf\n",
    "\n",
    "def _d12(V, F, rf, sV, T):\n",
    "    \"\"\"Compute d1, d2 with numerical safety.\"\"\"\n",
    "    srt = sV * np.sqrt(T)\n",
    "    d1 = (np.log(V/F) + (rf + 0.5*sV*sV)*T) / srt\n",
    "    d2 = d1 - srt\n",
    "    # Numerical safety: clip to prevent overflow in Phi\n",
    "    d1 = np.clip(d1, -35, 35)\n",
    "    d2 = np.clip(d2, -35, 35)\n",
    "    return d1, d2\n",
    "\n",
    "def residuals(theta, E_obs, sE_obs, F, rf, T=1.0):\n",
    "    \"\"\"Residual function in log space for stability.\"\"\"\n",
    "    # theta = [logV, logSigmaV]\n",
    "    V = np.exp(theta[0])\n",
    "    sV = np.exp(theta[1])\n",
    "    \n",
    "    d1, d2 = _d12(V, F, rf, sV, T)\n",
    "    \n",
    "    # Price equation\n",
    "    E_model = V*Phi(d1) - F*np.exp(-rf*T)*Phi(d2)\n",
    "    \n",
    "    # Volatility equation: use E_model in denominator for stability\n",
    "    sE_model = (V / max(E_model, 1e-12)) * Phi(d1) * sV\n",
    "    \n",
    "    # Relative scaling for better convergence\n",
    "    r_price = (E_model - E_obs) / max(E_obs, 1.0)\n",
    "    r_vol = sE_model - sE_obs\n",
    "    \n",
    "    return np.array([r_price, r_vol])\n",
    "\n",
    "def solve_one(E_obs, sE_obs, F, rf, T=1.0):\n",
    "    \"\"\"Solve for V and sigma_V with robust bounds and method.\"\"\"\n",
    "    # Input validation\n",
    "    if not (E_obs > 0 and sE_obs > 0 and F > 0):\n",
    "        return None\n",
    "    \n",
    "    # Initial guess\n",
    "    V0 = max(E_obs + F, 1.001*F)\n",
    "    sV0 = min(max(sE_obs, 1e-3), 1.5)\n",
    "    th0 = np.log([V0, sV0])\n",
    "    \n",
    "    # Bounds in log space\n",
    "    lo = np.log([1.001*F, 1e-4])\n",
    "    hi = np.log([1e3*(E_obs + F), 3.0])\n",
    "    \n",
    "    # Solve with robust settings\n",
    "    res = least_squares(\n",
    "        residuals, th0, args=(E_obs, sE_obs, F, rf, T),\n",
    "        method='trf',\n",
    "        loss='soft_l1',  # Robust to outliers\n",
    "        ftol=1e-10,\n",
    "        xtol=1e-10,\n",
    "        gtol=1e-10,\n",
    "        max_nfev=1000,\n",
    "        bounds=(lo, hi)\n",
    "    )\n",
    "    \n",
    "    if not res.success:\n",
    "        return None\n",
    "    \n",
    "    # Extract solution\n",
    "    V = float(np.exp(res.x[0]))\n",
    "    sV = float(np.exp(res.x[1]))\n",
    "    \n",
    "    return V, sV, float(res.cost), res.nfev, res.status\n",
    "\n",
    "print('[INFO] Stable Merton solver loaded')\n",
    "print('  - Uses log space for V and sigma_V')\n",
    "print('  - Clips d1, d2 to [-35, 35]')\n",
    "print('  - Uses E_model in volatility equation denominator')\n",
    "print('  - Robust loss function (soft_l1)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pre_solve_validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Validating inputs before solver...\n",
      "  E_t: 8.12e+07 to 4.89e+11\n",
      "  F_t: 3.40e+04 to 6.53e+11\n",
      "  rf_t: 0.0004 to 0.0495\n",
      "  sigma_E_tminus1: 0.0830 to 0.9782\n",
      "\n",
      "  E_t/F_t leverage ratio:\n",
      "count      1305.000000\n",
      "mean        119.212379\n",
      "std        3014.897417\n",
      "min           0.098664\n",
      "25%           1.349529\n",
      "50%           2.573520\n",
      "75%           4.944082\n",
      "max      107896.482941\n",
      "dtype: float64\n",
      "\n",
      "[INFO] After filtering: 1305 rows ready for solver\n"
     ]
    }
   ],
   "source": [
    "# PRE-SOLVE VALIDATION\n",
    "print('[INFO] Validating inputs before solver...')\n",
    "\n",
    "# Gate 1: E_t must be positive\n",
    "bad_E = df['E_t'] <= 0\n",
    "if bad_E.any():\n",
    "    print(f'[WARN] Filtering {bad_E.sum()} rows with non-positive E_t')\n",
    "    df = df[~bad_E].copy()\n",
    "print(f'  E_t: {df[\"E_t\"].min():.2e} to {df[\"E_t\"].max():.2e}')\n",
    "\n",
    "# Gate 2: F_t must be positive\n",
    "bad_F = df['F_t'] <= 0\n",
    "if bad_F.any():\n",
    "    print(f'[WARN] Filtering {bad_F.sum()} rows with non-positive F_t')\n",
    "    df = df[~bad_F].copy()\n",
    "print(f'  F_t: {df[\"F_t\"].min():.2e} to {df[\"F_t\"].max():.2e}')\n",
    "\n",
    "# Gate 3: rf_t in reasonable range (decimals, not percents)\n",
    "bad_rf = ~df['rf_t'].between(-0.1, 0.3)\n",
    "if bad_rf.any():\n",
    "    print(f'[WARN] Filtering {bad_rf.sum()} rows with rf_t outside [-0.1, 0.3]')\n",
    "    df = df[~bad_rf].copy()\n",
    "print(f'  rf_t: {df[\"rf_t\"].min():.4f} to {df[\"rf_t\"].max():.4f}')\n",
    "\n",
    "# Gate 4: sigma_E_tminus1 in reasonable range\n",
    "valid_sigma = df['sigma_E_tminus1'].notna()\n",
    "bad_sigma = valid_sigma & ~df['sigma_E_tminus1'].between(1e-4, 3.0)\n",
    "if bad_sigma.any():\n",
    "    print(f'[WARN] Filtering {bad_sigma.sum()} rows with sigma_E_tminus1 outside [0.0001, 3.0]')\n",
    "    print(f'  Range before filter: {df.loc[valid_sigma, \"sigma_E_tminus1\"].min():.4f} to {df.loc[valid_sigma, \"sigma_E_tminus1\"].max():.4f}')\n",
    "    df = df[~bad_sigma].copy()\n",
    "    valid_sigma = df['sigma_E_tminus1'].notna()\n",
    "if valid_sigma.any():\n",
    "    print(f'  sigma_E_tminus1: {df.loc[valid_sigma, \"sigma_E_tminus1\"].min():.4f} to {df.loc[valid_sigma, \"sigma_E_tminus1\"].max():.4f}')\n",
    "\n",
    "# Show leverage distribution\n",
    "print('\\n  E_t/F_t leverage ratio:')\n",
    "print(df.eval('E_t/F_t').describe())\n",
    "\n",
    "print(f'\\n[INFO] After filtering: {len(df)} rows ready for solver')\n",
    "\n",
    "# Reset index after filtering to avoid duplicates\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "run_stable_solver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running stable Merton solver on each row...\n",
      "  Processing 1305 rows...\n",
      "\n",
      "[INFO] Solver complete:\n",
      "  Converged: 1305/1305 (100.0%)\n",
      "\n",
      "Status counts:\n",
      "status_flag\n",
      "converged    1305\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Run the stable Merton solver\n",
    "print('[INFO] Running stable Merton solver on each row...')\n",
    "\n",
    "results = []\n",
    "solver_rows = df['sigma_E_tminus1'].notna()\n",
    "total_rows = solver_rows.sum()\n",
    "\n",
    "print(f'  Processing {total_rows} rows...')\n",
    "\n",
    "for idx, row in df[solver_rows].iterrows():\n",
    "    result = solve_one(\n",
    "        E_obs=row['E_t'],\n",
    "        sE_obs=row['sigma_E_tminus1'],\n",
    "        F=row['F_t'],\n",
    "        rf=row['rf_t'],\n",
    "        T=1.0\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        V, sV, cost, nfev, status = result\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'asset_value': V,\n",
    "            'asset_vol': sV,\n",
    "            'solver_cost': cost,\n",
    "            'nfev': nfev,\n",
    "            'status_flag': 'converged'\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'asset_value': np.nan,\n",
    "            'asset_vol': np.nan,\n",
    "            'solver_cost': np.nan,\n",
    "            'nfev': 0,\n",
    "            'status_flag': 'no_converge'\n",
    "        })\n",
    "\n",
    "# Merge results\n",
    "results_df = pd.DataFrame(results).set_index('index')\n",
    "df = df.join(results_df)\n",
    "\n",
    "# Fill status_flag for rows without sigma_E\n",
    "df['status_flag'] = df['status_flag'].fillna('no_sigma_E')\n",
    "\n",
    "# Report\n",
    "converged = (df['status_flag'] == 'converged').sum()\n",
    "print(f'\\n[INFO] Solver complete:')\n",
    "print(f'  Converged: {converged}/{total_rows} ({100*converged/total_rows:.1f}%)' if total_rows > 0 else '  No rows to solve')\n",
    "print('\\nStatus counts:')\n",
    "print(df['status_flag'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a81c3bfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.818066Z",
     "iopub.status.busy": "2025-10-02T09:25:31.817661Z",
     "iopub.status.idle": "2025-10-02T09:25:31.833923Z",
     "shell.execute_reply": "2025-10-02T09:25:31.832743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ABCB 2016: asset_value = 3633285480.83, asset_vol = 0.1872, status = no_converge\n"
     ]
    }
   ],
   "source": [
    "# 7. Define solver and run a quick sanity check\n",
    "\n",
    "# Guarded imports allow this cell to run standalone in interactive sessions\n",
    "try:\n",
    "    pd  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    import pandas as pd  # noqa: F401\n",
    "try:\n",
    "    np  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    import numpy as np\n",
    "try:\n",
    "    norm  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    from scipy.stats import norm  # noqa: F401\n",
    "try:\n",
    "    least_squares  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    from scipy.optimize import least_squares\n",
    "try:\n",
    "    tqdm  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "if 'T' not in globals():  # pragma: no cover - interactive safeguard\n",
    "    T = 1.0\n",
    "\n",
    "\n",
    "def merton_solver(row, *, T=T, tol_E=tol_E, tol_sigma=tol_sigma, max_iter=max_iter):\n",
    "    '''Solve for asset value and volatility using the Merton model.\n",
    "\n",
    "    Returns a dictionary with solver diagnostics so downstream analysis can\n",
    "    inspect failures without re-running the optimizer.\n",
    "    '''\n",
    "    result_template = {\n",
    "        'asset_value': np.nan,\n",
    "        'asset_vol': np.nan,\n",
    "        'E_model': np.nan,\n",
    "        'sigmaE_model': np.nan,\n",
    "        'r1': np.nan,\n",
    "        'r2': np.nan,\n",
    "        'solver_message': '',\n",
    "        'nfev': np.nan,\n",
    "        'iterations': np.nan,\n",
    "        'status_flag': 'invalid_inputs',\n",
    "    }\n",
    "\n",
    "    E = row['market_cap']\n",
    "    sigma_E = row['equity_vol']\n",
    "    F = row['F']\n",
    "    r_f = row['rf']\n",
    "\n",
    "    # 1. Input validation\n",
    "    if pd.isna(E) or pd.isna(sigma_E) or pd.isna(F):\n",
    "        result = result_template.copy()\n",
    "        result['solver_message'] = 'missing_input'\n",
    "        return result\n",
    "    if E <= 0 or sigma_E <= 0 or F < 0:\n",
    "        result = result_template.copy()\n",
    "        result['solver_message'] = 'invalid_value'\n",
    "        return result\n",
    "    if F == 0:\n",
    "        result = result_template.copy()\n",
    "        result['status_flag'] = 'no_debt'\n",
    "        result['solver_message'] = 'zero_debt'\n",
    "        return result\n",
    "\n",
    "    # 2. System of Merton equations\n",
    "    def residuals(x):\n",
    "        asset_value, sigma_V = x\n",
    "        if sigma_V <= 0:\n",
    "            return np.array([np.inf, np.inf])\n",
    "        d1 = (np.log(asset_value / F) + (r_f + 0.5 * sigma_V**2) * T) / (sigma_V * np.sqrt(T))\n",
    "        d2 = d1 - sigma_V * np.sqrt(T)\n",
    "        E_model = asset_value * norm.cdf(d1) - F * np.exp(-r_f * T) * norm.cdf(d2)\n",
    "        sigmaE_model_obs = (asset_value / max(E, np.finfo(float).eps)) * norm.cdf(d1) * sigma_V\n",
    "        r1 = E_model - E\n",
    "        r2 = sigmaE_model_obs - sigma_E\n",
    "        return np.array([r1, r2])\n",
    "\n",
    "    # 3. Initial guess and solve (with bounds)\n",
    "    lower_V = np.nextafter(F, np.inf)\n",
    "    lower_bounds = np.array([lower_V, 1e-6])\n",
    "    upper_bounds = np.array([np.inf, np.inf])\n",
    "    initial_V = max(E + F, lower_V * 1.0001)\n",
    "    initial_sigma = max(min(sigma_E, 1.0), 1e-6)\n",
    "    initial = np.array([initial_V, initial_sigma])\n",
    "\n",
    "    try:\n",
    "        sol = least_squares(\n",
    "            residuals,\n",
    "            initial,\n",
    "            bounds=(lower_bounds, upper_bounds),\n",
    "            loss='soft_l1',\n",
    "            max_nfev=max_iter,\n",
    "        )\n",
    "    except ValueError as exc:\n",
    "        result = result_template.copy()\n",
    "        result['solver_message'] = f'least_squares_error: {exc}'\n",
    "        return result\n",
    "\n",
    "    asset_value, sigma_V = sol.x\n",
    "    d1 = (np.log(asset_value / F) + (r_f + 0.5 * sigma_V**2) * T) / (sigma_V * np.sqrt(T))\n",
    "    d2 = d1 - sigma_V * np.sqrt(T)\n",
    "    E_model = asset_value * norm.cdf(d1) - F * np.exp(-r_f * T) * norm.cdf(d2)\n",
    "    if E_model <= 0:\n",
    "        sigmaE_model = np.nan\n",
    "    else:\n",
    "        sigmaE_model = (asset_value / E_model) * norm.cdf(d1) * sigma_V\n",
    "    sigmaE_model_obs = (asset_value / max(E, np.finfo(float).eps)) * norm.cdf(d1) * sigma_V\n",
    "    r1 = E_model - E\n",
    "    r2 = sigmaE_model_obs - sigma_E\n",
    "\n",
    "    converged = sol.success and abs(r1) <= tol_E and abs(r2) <= tol_sigma\n",
    "\n",
    "    result = result_template.copy()\n",
    "    result.update({\n",
    "        'asset_value': asset_value,\n",
    "        'asset_vol': sigma_V,\n",
    "        'E_model': E_model,\n",
    "        'sigmaE_model': sigmaE_model,\n",
    "        'r1': r1,\n",
    "        'r2': r2,\n",
    "        'solver_message': sol.message,\n",
    "        'nfev': sol.nfev,\n",
    "        'iterations': getattr(sol, 'njev', np.nan),\n",
    "        'status_flag': 'converged' if converged else 'no_converge',\n",
    "    })\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_merton_solver(df):\n",
    "    '''Apply the Merton solver row-wise with a progress bar.'''\n",
    "    records = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc='Solving Merton model'):\n",
    "        records.append(merton_solver(row))\n",
    "    return pd.DataFrame(records, index=df.index)\n",
    "\n",
    "\n",
    "# ---- Quick check on the first row ----\n",
    "first_row = df.iloc[0]\n",
    "first_result = merton_solver(first_row)\n",
    "print(\n",
    "    f\"Results for {first_row['instrument']} {first_row['year']}: \"\n",
    "    f\"asset_value = {first_result['asset_value']:.2f}, \"\n",
    "    f\"asset_vol = {first_result['asset_vol']:.4f}, \"\n",
    "    f\"status = {first_result['status_flag']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7650e9",
   "metadata": {},
   "source": [
    "## 7 Compute Distance to Default (DD) and Probability of Default (PD) in Detail\n",
    "\n",
    "Once we have solved for:\n",
    "\n",
    "- $V$ = total asset value  \n",
    "- $sigma_V$ = asset volatility  \n",
    "\n",
    "we compute:\n",
    "\n",
    "1. **Distance to Default**  \n",
    "   \n",
    "$$DD = (ln(V/F) + (r_f - 0.5 sigma_V^2)T) / (sigma_V * √T)\n",
    "  $$ \n",
    "   - **Numerator**  \n",
    "     - $ln(V/F)$: how far assets exceed debt on a log scale  \n",
    "     - $(r_f - 0.5 sigma_V^2)T$: drift adjustment for risk-free growth minus half variance  \n",
    "   - **Denominator**  \n",
    "     - $sigma_V √T$: scales by volatility over the horizon  \n",
    "\n",
    "2. **Probability of Default**  \n",
    "   \n",
    "   PD = Φ(-DD)\n",
    "   \n",
    "   where Φ is the standard normal CDF. Intuitively, low DD means a higher chance assets fall below debt.\n",
    "\n",
    "We also handle the special case **no debt** (F=0), for which DD and PD are undefined (we set them to NaN).\n",
    "\n",
    "**IMPORTANT FIX**: The original code had a unit mismatch where:\n",
    "- Market cap (E) was in actual USD \n",
    "- Debt total (F) was in USD millions from the source data\n",
    "\n",
    "This created unrealistic V/F ratios of millions, leading to DDm values >100 and PDm = 0 due to numerical underflow. The fix multiplies `debt_total` by 1,000,000 to convert to actual USD.\n",
    "\n",
    "Below is code that computes these step by step, with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "safe_dd_pd_computation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing DD_m and PD_m...\n",
      "  Valid rows used for DD_m: 1305\n",
      "  DD_m range: 0.90 to 35.00\n",
      "  PD_m range: 1.12e-268 to 1.84e-01\n"
     ]
    }
   ],
   "source": [
    "# 7.2 Compute DD_m and PD_m safely\n",
    "# d1 = [ln(V/F) + (r_f + 0.5*sigma_V^2)T] / (sigma_V*sqrt(T))\n",
    "# d2 = d1 - sigma_V*sqrt(T)  -> DD_m = d2,  PD_m = Phi(-d2)\n",
    "\n",
    "print('[INFO] Computing DD_m and PD_m...')\n",
    "\n",
    "Phi = getattr(norm, 'cdf', None)\n",
    "if Phi is None:\n",
    "    from math import erf\n",
    "    def Phi(x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return 0.5*(1.0 + np.vectorize(erf)(x/np.sqrt(2.0)))\n",
    "\n",
    "F_t  = df['F']\n",
    "rf_t = df['rf']\n",
    "V_t  = df['asset_value']\n",
    "sV_t = df['asset_vol']\n",
    "T = 1.0\n",
    "\n",
    "converged = (df['status_flag'] == 'converged')\n",
    "valid = (\n",
    "    converged\n",
    "    & np.isfinite(V_t) & (V_t > 0)\n",
    "    & np.isfinite(sV_t) & (sV_t > 0)\n",
    "    & np.isfinite(F_t) & (F_t > 0)\n",
    "    & np.isfinite(rf_t)\n",
    ")\n",
    "\n",
    "# Initialize outputs\n",
    "df['d1'] = np.nan\n",
    "df['d2'] = np.nan\n",
    "df['DD_m'] = np.nan\n",
    "df['PD_m'] = np.nan\n",
    "df['solver_status'] = df['status_flag']\n",
    "\n",
    "# Compute only on valid rows\n",
    "idx = np.where(valid)[0]\n",
    "if idx.size:\n",
    "    srt = sV_t.values[idx] * np.sqrt(T)\n",
    "    d1 = (np.log(V_t.values[idx] / F_t.values[idx]) + (rf_t.values[idx] + 0.5 * sV_t.values[idx]**2) * T) / srt\n",
    "    d2 = d1 - srt\n",
    "    # Numerical safety\n",
    "    d1 = np.clip(d1, -35, 35)\n",
    "    d2 = np.clip(d2, -35, 35)\n",
    "    \n",
    "    df.loc[valid, 'd1']   = d1\n",
    "    df.loc[valid, 'd2']   = d2\n",
    "    df.loc[valid, 'DD_m'] = d2\n",
    "    df.loc[valid, 'PD_m'] = Phi(-d2)\n",
    "\n",
    "print(f'  Valid rows used for DD_m: {int(valid.sum())}')\n",
    "if valid.sum() > 0:\n",
    "    print(f'  DD_m range: {df.loc[valid, \"DD_m\"].min():.2f} to {df.loc[valid, \"DD_m\"].max():.2f}')\n",
    "    print(f'  PD_m range: {df.loc[valid, \"PD_m\"].min():.2e} to {df.loc[valid, \"PD_m\"].max():.2e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solver_preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of results:\n",
      "  instrument  year   asset_value  asset_vol         d1         d2       DD_m  \\\n",
      "0       ABCB  2016  3.633310e+09   0.168152  10.515643  10.347490  10.347490   \n",
      "1       ABCB  2017  3.685325e+09   0.218348  10.713611  10.495263  10.495263   \n",
      "2       ABCB  2018  2.439065e+09   0.218571  10.410890  10.192319  10.192319   \n",
      "3       ABCB  2019  4.445621e+09   0.208037   5.281290   5.073253   5.073253   \n",
      "4       ABCB  2020  3.182115e+09   0.265516   6.685020   6.419505   6.419505   \n",
      "\n",
      "           PD_m status_flag solver_status  \n",
      "0  2.147999e-25   converged     converged  \n",
      "1  4.541225e-26   converged     converged  \n",
      "2  1.072919e-24   converged     converged  \n",
      "3  1.955359e-07   converged     converged  \n",
      "4  6.835930e-11   converged     converged  \n",
      "\n",
      "Solver status counts:\n",
      "status_flag\n",
      "converged    1305\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valid rows used for DD_m: 1305\n",
      "\n",
      "V/F (leverage) summary on valid rows:\n",
      "count      1305.000000\n",
      "mean        120.197471\n",
      "std        3014.897552\n",
      "min           1.094270\n",
      "25%           2.335189\n",
      "50%           3.561146\n",
      "75%           5.929846\n",
      "max      107897.474973\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7.3 Quick sanity checks and preview\n",
    "preview_cols = [\n",
    "    'instrument', 'year', 'asset_value', 'asset_vol', 'E_model', 'sigmaE_model',\n",
    "    'r1', 'r2', 'd1', 'd2', 'DD_m', 'PD_m', 'status_flag', 'solver_status'\n",
    "]\n",
    "# Only show columns that exist\n",
    "preview_cols = [c for c in preview_cols if c in df.columns]\n",
    "print('Preview of results:')\n",
    "print(df[preview_cols].head())\n",
    "print()\n",
    "\n",
    "print('Solver status counts:')\n",
    "print(df['status_flag'].value_counts())\n",
    "print()\n",
    "\n",
    "# Extra diagnostics\n",
    "valid = (df['status_flag'] == 'converged') & df['DD_m'].notna()\n",
    "if valid.sum() > 0:\n",
    "    print(f'Valid rows used for DD_m: {int(valid.sum())}')\n",
    "    print('\\nV/F (leverage) summary on valid rows:')\n",
    "    print((df.loc[valid, 'asset_value'] / df.loc[valid, 'F']).describe())\n",
    "else:\n",
    "    print('[WARN] No valid DD_m values computed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f405d6",
   "metadata": {},
   "source": [
    "## 8. Export Results and Log Diagnostics\n",
    "\n",
    "In this final step, we:\n",
    "\n",
    "1. **Save** the full DataFrame (including `DDm` and `PDm`) to CSV for downstream modelling.  \n",
    "2. **Append** a diagnostic summary to our log file, including:  \n",
    "   - Total rows processed  \n",
    "   - Solver status breakdown  \n",
    "   - Basic statistics on `DDm` and `PDm`  \n",
    "   - Count of missing or failed estimates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0511cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:34.647748Z",
     "iopub.status.busy": "2025-10-02T09:25:34.647520Z",
     "iopub.status.idle": "2025-10-02T09:25:34.746229Z",
     "shell.execute_reply": "2025-10-02T09:25:34.745259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARCHIVE] Moved to archive: market_20251011_042610.csv\n",
      "[CLEANUP] Removed old archive: market_20251004_050613.csv\n",
      "[INFO] Results exported to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/market_20251011_042629.csv\n",
      "[INFO] Diagnostics appended to log: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/logs/dd_pd_market_log.txt\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Archiving and timestamped output\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    # Move all existing files to archive\n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] Moved to archive: {os.path.basename(old_file)}\")\n",
    "    \n",
    "    # Clean up archive to keep only max_keep files\n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] Removed old archive: {os.path.basename(old_archive)}\")\n",
    "\n",
    "# Rename columns to standard naming convention\n",
    "df = df.rename(columns={'DDm': 'DD_m', 'PDm': 'PD_m'})\n",
    "\n",
    "# Archive old market files and save new one with timestamp\n",
    "archive_old_files(output_dir, archive_dir, 'market', max_keep=5)\n",
    "\n",
    "timestamp = get_timestamp_cdt()\n",
    "output_fp = output_dir / f'market_{timestamp}.csv'\n",
    "# Add provenance columns for time integrity audit\n",
    "provenance_cols = [\"E_t\", \"F_t\", \"rf_t\", \"sigma_E_tminus1\", \n",
    "                   \"sigmaE_window_start_year\", \"sigmaE_window_end_year\", \n",
    "                   \"V_t\", \"sigma_V_t\", \"d1\", \"d2\", \"DD_m\", \"PD_m\", \n",
    "                   \"solver_status\", \"resid_price\", \"resid_vol\"]\n",
    "\n",
    "df.to_csv(output_fp, index=False)\n",
    "print(f\"[INFO] Results exported to: {output_fp}\")\n",
    "\n",
    "# 8.2 Append diagnostics to the log file\n",
    "with open(log_fp, 'a') as log:\n",
    "    log.write(\"\\n=== DD/PD Market-Based Model Diagnostics ===\\n\")\n",
    "    # Total rows\n",
    "    total = len(df)\n",
    "    log.write(f\"Total rows processed: {total}\\n\")\n",
    "    # Deduplication audit\n",
    "    if duplicate_log_entries:\n",
    "        log.write(\"\\nDeduplicated instrument-year rows (kept max debt_total):\\n\")\n",
    "        for entry in duplicate_log_entries:\n",
    "            log.write(entry + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nDeduplicated instrument-year rows: none detected.\\n\")\n",
    "    # Rows removed due to invalid inputs\n",
    "    if invalid_input_summary is not None and not invalid_input_summary.empty:\n",
    "        log.write(\"\\nRows dropped due to invalid inputs (solver_status=invalid_inputs):\\n\")\n",
    "        log.write(invalid_input_summary.to_string(index=False) + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nRows dropped due to invalid inputs: none.\\n\")\n",
    "    # Surviving rows by year\n",
    "    if surviving_rows_by_year is not None and not surviving_rows_by_year.empty:\n",
    "        log.write(\"\\nSurviving rows by year after input validation:\\n\")\n",
    "        log.write(surviving_rows_by_year.to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nSurviving rows by year after input validation: not available.\\n\")\n",
    "    # Solver status counts\n",
    "    status_counts = df['solver_status'].value_counts()\n",
    "    log.write(\"Solver status counts:\\n\")\n",
    "    log.write(status_counts.to_string() + \"\\n\")\n",
    "    # DD_m and PD_m summary\n",
    "    log.write(\"\\nDistance to Default (DD_m) summary:\\n\")\n",
    "    log.write(df['DD_m'].describe().to_string() + \"\\n\")\n",
    "    log.write(\"\\nProbability of Default (PD_m) summary:\\n\")\n",
    "    log.write(df['PD_m'].describe().to_string() + \"\\n\")\n",
    "    F_values = df['F']\n",
    "    leverage_ratio = (df['market_cap'] / F_values).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if not leverage_ratio.empty:\n",
    "        median_ratio = leverage_ratio.median()\n",
    "        log.write(f\"\\nUnit check (market_cap / F_values) median: {median_ratio:.3f}\\n\")\n",
    "        log.write(\"Expected order of magnitude ~= 1 when both legs are in USD.\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nUnit check skipped: insufficient data for market_cap/F comparison.\\n\")\n",
    "    # Missing/failure counts\n",
    "    missing_dd = df['DD_m'].isna().sum()\n",
    "    missing_pd = df['PD_m'].isna().sum()\n",
    "    log.write(f\"\\nRows with missing DD_m: {missing_dd}\\n\")\n",
    "    log.write(f\"Rows with missing PD_m: {missing_pd}\\n\")\n",
    "\n",
    "print(f\"[INFO] Diagnostics appended to log: {log_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "summary_generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating summary statistics by year...\n",
      "[INFO] Summary statistics saved to market_20251011_042629_summary.csv\n",
      "\n",
      "Summary preview:\n",
      "   year metric           p10           p25           p50           p75  \\\n",
      "0  2016   DD_m  6.968295e+00  8.144592e+00  9.202983e+00  1.096727e+01   \n",
      "1  2017   DD_m  5.358555e+00  6.392604e+00  7.976442e+00  9.437705e+00   \n",
      "2  2018   DD_m  5.416146e+00  6.349943e+00  7.608550e+00  9.591490e+00   \n",
      "3  2019   DD_m  5.282996e+00  6.306048e+00  8.009375e+00  9.721626e+00   \n",
      "4  2020   DD_m  5.456758e+00  6.664166e+00  8.015948e+00  1.046400e+01   \n",
      "5  2021   DD_m  4.022301e+00  4.805221e+00  6.391235e+00  8.624310e+00   \n",
      "6  2022   DD_m  3.377051e+00  4.105658e+00  5.280170e+00  6.912456e+00   \n",
      "7  2023   DD_m  3.407609e+00  4.130689e+00  5.013820e+00  6.443325e+00   \n",
      "8  2016   PD_m  4.348588e-42  3.168720e-28  1.927141e-20  1.005053e-15   \n",
      "9  2017   PD_m  1.688765e-29  1.905171e-21  7.530594e-16  8.154220e-11   \n",
      "\n",
      "            p90  \n",
      "0  1.402697e+01  \n",
      "1  1.122041e+01  \n",
      "2  1.213759e+01  \n",
      "3  1.256470e+01  \n",
      "4  1.380315e+01  \n",
      "5  1.064582e+01  \n",
      "6  8.770928e+00  \n",
      "7  8.528607e+00  \n",
      "8  1.611135e-12  \n",
      "9  4.204233e-08  \n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics by year\n",
    "print('[INFO] Generating summary statistics by year...')\n",
    "\n",
    "# Filter to converged rows only for summary\n",
    "converged_df = df[df['status_flag'] == 'converged'].copy()\n",
    "\n",
    "if len(converged_df) > 0:\n",
    "    # Calculate percentiles by year for DD_m and PD_m\n",
    "    summary_data = []\n",
    "    \n",
    "    for year in sorted(converged_df['year'].unique()):\n",
    "        year_data = converged_df[converged_df['year'] == year]\n",
    "        \n",
    "        # DD_m percentiles\n",
    "        dd_percentiles = year_data['DD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        summary_data.append({\n",
    "            'year': year,\n",
    "            'metric': 'DD_m',\n",
    "            'p10': dd_percentiles[0.1],\n",
    "            'p25': dd_percentiles[0.25],\n",
    "            'p50': dd_percentiles[0.5],\n",
    "            'p75': dd_percentiles[0.75],\n",
    "            'p90': dd_percentiles[0.9]\n",
    "        })\n",
    "    \n",
    "    for year in sorted(converged_df['year'].unique()):\n",
    "        year_data = converged_df[converged_df['year'] == year]\n",
    "        \n",
    "        # PD_m percentiles\n",
    "        pd_percentiles = year_data['PD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        summary_data.append({\n",
    "            'year': year,\n",
    "            'metric': 'PD_m',\n",
    "            'p10': pd_percentiles[0.1],\n",
    "            'p25': pd_percentiles[0.25],\n",
    "            'p50': pd_percentiles[0.5],\n",
    "            'p75': pd_percentiles[0.75],\n",
    "            'p90': pd_percentiles[0.9]\n",
    "        })\n",
    "    \n",
    "    # Overall statistics\n",
    "    dd_overall = converged_df['DD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "    summary_data.append({\n",
    "        'year': 'overall',\n",
    "        'metric': 'DD_m',\n",
    "        'p10': dd_overall[0.1],\n",
    "        'p25': dd_overall[0.25],\n",
    "        'p50': dd_overall[0.5],\n",
    "        'p75': dd_overall[0.75],\n",
    "        'p90': dd_overall[0.9]\n",
    "    })\n",
    "    \n",
    "    pd_overall = converged_df['PD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "    summary_data.append({\n",
    "        'year': 'overall',\n",
    "        'metric': 'PD_m',\n",
    "        'p10': pd_overall[0.1],\n",
    "        'p25': pd_overall[0.25],\n",
    "        'p50': pd_overall[0.5],\n",
    "        'p75': pd_overall[0.75],\n",
    "        'p90': pd_overall[0.9]\n",
    "    })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save summary to analysis directory\n",
    "    summary_fp = base_dir / 'data' / 'outputs' / 'analysis' / f'market_{timestamp}_summary.csv'\n",
    "    summary_df.to_csv(summary_fp, index=False)\n",
    "    print(f'[INFO] Summary statistics saved to {summary_fp.name}')\n",
    "    print(f'\\nSummary preview:')\n",
    "    print(summary_df.head(10))\n",
    "else:\n",
    "    print('[WARN] No converged rows found. Summary not generated.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
