{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7747bcc",
   "metadata": {},
   "source": [
    "# DD and PD Calculation Using Market-approach (Merton Model)\n",
    "\n",
    "This notebook walks through the `dd_pd_market.py` script step by step.  \n",
    "We will:\n",
    "\n",
    "1.  Set up our environment and imports  \n",
    "2.  Load and inspect inputs  \n",
    "3.  Prepare and merge data  \n",
    "4.  Compute market capitalizations  \n",
    "5.  Merge equity volatility  \n",
    "6.  Define and run the Merton model solver  \n",
    "7.  Calculate Distance to Default (DD) and Probability of Default (PD)  \n",
    "8.  Export results and write diagnostics to a log  \n",
    "\n",
    "\n",
    "**Timing**: Uses sigma_{E,t-1}, E_t, F_t, r_{f,t}. No lookahead bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a062f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market approach, Merton KMV solve. Uses μ = r_f. Barrier convention: A (total debt)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameter defaults for the market-based Merton KMV implementation\n",
    "print(\"Market approach, Merton KMV solve. Uses μ = r_f. Barrier convention: A (total debt)\")\n",
    "T = 1.0\n",
    "tol_E = 1e-6\n",
    "tol_sigma = 1e-4\n",
    "max_iter = 200\n",
    "barrier_option = \"A\"  # For banks: debt_total represents total liabilities (deposits dominate funding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aecf6ed",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Here we import all libraries and define file‐paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be68c12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:05.978634Z",
     "iopub.status.busy": "2025-10-02T09:25:05.978402Z",
     "iopub.status.idle": "2025-10-02T09:25:30.331142Z",
     "shell.execute_reply": "2025-10-02T09:25:30.328612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install needed packages (run once per environment)\n",
    "%pip install pandas numpy matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade82fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:30.340463Z",
     "iopub.status.busy": "2025-10-02T09:25:30.339371Z",
     "iopub.status.idle": "2025-10-02T09:25:31.635495Z",
     "shell.execute_reply": "2025-10-02T09:25:31.634794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root resolved to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank\n",
      "Accounting input   → FOUND\n",
      "Marketcap input    → FOUND\n",
      "Equity vol input   → FOUND\n",
      "Risk-free input    → FOUND\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and Imports (with correct file names)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import least_squares\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1.1 Locate workspace root\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / '.git').exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "print(f\"Repository root resolved to: {base_dir}\")\n",
    "\n",
    "# 1.2 Time horizon (configured in parameter cell)\n",
    "\n",
    "# 1.3 File paths (corrected market-cap filename)\n",
    "model_fp      = base_dir / 'data' / 'clean' / 'esg_0718_clean.csv'\n",
    "marketcap_fp  = base_dir / 'data' / 'clean' / 'all_banks_marketcap_annual_2016_2023.csv'\n",
    "vol_fp        = base_dir / 'data' / 'clean' / 'equity_volatility_by_year_DAILY.csv'\n",
    "rf_fp         = base_dir / 'data' / 'clean' / 'fama_french_factors_annual_clean.csv'\n",
    "log_fp        = base_dir / 'data' / 'logs' / 'dd_pd_market_log.txt'\n",
    "output_dir    = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "archive_dir   = base_dir / 'archive' / 'datasets'\n",
    "\n",
    "# 1.4 Ensure directories exist\n",
    "log_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1.5 Existence check\n",
    "for name, fp in [\n",
    "    ('Accounting input', model_fp),\n",
    "    ('Marketcap input', marketcap_fp),\n",
    "    ('Equity vol input', vol_fp),\n",
    "    ('Risk-free input', rf_fp),\n",
    "]:\n",
    "    print(f\"{name:18s} →\", \"FOUND\" if fp.exists() else f\"MISSING ({fp.name})\")\n",
    "\n",
    "# 1.6 Deduplication audit log placeholder\n",
    "duplicate_log_entries = []\n",
    "invalid_input_summary = None\n",
    "surviving_rows_by_year = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92c20c",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Core Data\n",
    "\n",
    "- Read the main Book2 input file  \n",
    "- Clean and convert the `year` column  \n",
    "- Merge in the annual risk-free rate from Fama-French  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a32e25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.638919Z",
     "iopub.status.busy": "2025-10-02T09:25:31.638448Z",
     "iopub.status.idle": "2025-10-02T09:25:31.738867Z",
     "shell.execute_reply": "2025-10-02T09:25:31.738248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Book2 data...\n",
      "→ 1427 rows, 1424 unique (instrument, year)\n",
      "→ Dropping 3 duplicate instrument-year rows (kept max debt_total)\n",
      "   Duplicate PNC 2018: kept debt_total=60,263.0; dropped=60,263.0, 57,419.0, 57,419.0\n",
      "→ After merging rf, 1424 rows remain\n",
      "[INFO] Constructed F barrier for 1424 rows using barrier_option=A\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Load Book2 data\n",
    "print('[INFO] Loading Book2 data...')\n",
    "df = pd.read_csv(model_fp)\n",
    "print(f\"→ {df.shape[0]} rows, {df[['instrument','year']].drop_duplicates().shape[0]} unique (instrument, year)\")\n",
    "\n",
    "# 2.1a Deduplicate instrument-year rows by keeping the largest debt_total\n",
    "sort_cols = ['instrument', 'year', 'debt_total']\n",
    "df_sorted = df.sort_values(sort_cols, ascending=[True, True, False])\n",
    "deduped = df_sorted.drop_duplicates(subset=['instrument', 'year'], keep='first')\n",
    "removed = df_sorted.loc[~df_sorted.index.isin(deduped.index), ['instrument', 'year', 'debt_total']]\n",
    "if not removed.empty:\n",
    "    print(f\"→ Dropping {removed.shape[0]} duplicate instrument-year rows (kept max debt_total)\")\n",
    "    summary = (\n",
    "        df_sorted.groupby(['instrument', 'year'])['debt_total']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary = summary[summary['debt_total'].apply(len) > 1]\n",
    "    for _, row in summary.iterrows():\n",
    "        debt_values = list(row['debt_total'])\n",
    "        kept = debt_values[0]\n",
    "        dropped = debt_values[1:]\n",
    "        dropped_str = ', '.join(f\"{val:,.1f}\" for val in dropped)\n",
    "        message = (\n",
    "            f\"Duplicate {row['instrument']} {row['year']}: \"\n",
    "            f\"kept debt_total={kept:,.1f}; dropped={dropped_str}\"\n",
    "        )\n",
    "        print('   ' + message)\n",
    "        duplicate_log_entries.append(message)\n",
    "    df = deduped.reset_index(drop=True)\n",
    "else:\n",
    "    print(\"→ No duplicate instrument-year rows found (all unique).\")\n",
    "    df = df_sorted.reset_index(drop=True)\n",
    "\n",
    "# 2.2  Clean year column\n",
    "df = df[df['year'].notnull()].copy()\n",
    "df['year'] = df['year'].astype(float).astype(int)\n",
    "\n",
    "# 2.3 Merge risk-free rate\n",
    "rf_df = pd.read_csv(rf_fp)\n",
    "df = df.merge(rf_df[['year','rf']], on='year', how='left')\n",
    "df['rf'] = df['rf'] / 100    # convert percent to decimal\n",
    "\n",
    "print(f\"→ After merging rf, {df.shape[0]} rows remain\")\n",
    "\n",
    "if barrier_option == \"A\":\n",
    "    df['F'] = df['debt_total'] * 1_000_000  # F = total liabilities for banks (debt_total column)\n",
    "elif barrier_option == \"B\":\n",
    "    required_cols = {'debt_short_term', 'debt_long_term'}\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        missing_str = \", \".join(missing)\n",
    "        raise ValueError(\n",
    "            \"barrier_option='B' requires short/long-term debt columns. \"\n",
    "            f\"Missing: {missing_str}. Add these columns before enabling option B.\"\n",
    "        )\n",
    "    df['F'] = (df['debt_short_term'] + 0.5 * df['debt_long_term']) * 1_000_000\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported barrier_option: {barrier_option}\")\n",
    "print(f\"[INFO] Constructed F barrier for {df.shape[0]} rows using barrier_option={barrier_option}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf131177",
   "metadata": {},
   "source": [
    "## 3. Prepare Identifiers and Dates\n",
    "\n",
    "- Standardize tickers by dropping exchange suffixes  \n",
    "- Parse the `date` column and extract `Month`  \n",
    "- Create a simple `symbol` field for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1623898e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.743719Z",
     "iopub.status.busy": "2025-10-02T09:25:31.743210Z",
     "iopub.status.idle": "2025-10-02T09:25:31.755989Z",
     "shell.execute_reply": "2025-10-02T09:25:31.754954Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3.1 Helper to strip suffixes like .N, .OQ, etc.\n",
    "def standardize_ticker(t):\n",
    "    return str(t).split('.', 1)[0] if pd.notnull(t) else t\n",
    "\n",
    "# 3.2Apply to our main DataFrame\n",
    "df['ticker_prefix'] = df['instrument'].apply(standardize_ticker)\n",
    "\n",
    "# 3.3 Ensure date is datetime, then extract month\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "else:\n",
    "    df = df.assign(date=pd.NaT)\n",
    "\n",
    "df['Month'] = df['date'].dt.month\n",
    "\n",
    "# 3.4 Create 'symbol' for merge keys (same as ticker_prefix)\n",
    "df['symbol'] = df['instrument'].apply(lambda x: str(x).split('.', 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782546d",
   "metadata": {},
   "source": [
    "## 4. Compute Market Capitalization\n",
    "\n",
    "- Load monthly price/share data  \n",
    "- Calculate market cap (price × shares) in millions USD  \n",
    "- Select December (or most recent) value for each symbol-year  \n",
    "- Merge annual market cap into our main `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb31cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.758812Z",
     "iopub.status.busy": "2025-10-02T09:25:31.758557Z",
     "iopub.status.idle": "2025-10-02T09:25:31.784208Z",
     "shell.execute_reply": "2025-10-02T09:25:31.783567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in mc: ['symbol', 'year', 'market_cap']\n",
      "  instrument  year    market_cap\n",
      "0       ABCB  2016  3.004515e+09\n",
      "1       ABCB  2017  3.321505e+09\n",
      "2       ABCB  2018  2.182408e+09\n",
      "3       ABCB  2019  2.931470e+09\n",
      "4       ABCB  2020  2.623438e+09\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Load annual market‐cap data\n",
    "mc = pd.read_csv(marketcap_fp)\n",
    "print(\"Columns in mc:\", mc.columns.tolist())\n",
    "\n",
    "# 4.2 Compute market_cap only if needed\n",
    "if 'market_cap' not in mc.columns:\n",
    "    # fallback: compute from dec_price & shares_outstanding\n",
    "    mc['market_cap'] = mc['dec_price'] * mc['shares_outstanding']\n",
    "\n",
    "# 4.3 Standardize the ticker (drop suffixes)\n",
    "mc['symbol'] = mc['symbol'].apply(standardize_ticker)\n",
    "\n",
    "# 4.4 Parse the fiscal date and extract year/month\n",
    "#    If this annual file has no 'fiscal_date' but has 'year', skip parsing\n",
    "if 'fiscal_date' in mc.columns:\n",
    "    mc['fiscal_date'] = pd.to_datetime(mc['fiscal_date'])\n",
    "    mc['year']       = mc['fiscal_date'].dt.year\n",
    "else:\n",
    "    # assume the CSV’s 'year' column is correct\n",
    "    mc['year'] = mc['year'].astype(int)\n",
    "\n",
    "# 4.5 We don’t need Month or December flag for annual data, but for consistency:\n",
    "mc['Month']       = mc.get('Month', 12)  # treat all as December\n",
    "mc['is_december'] = True\n",
    "\n",
    "# 4.6 Drop duplicates: keep one record per (symbol, year)\n",
    "mc_annual = (\n",
    "    mc\n",
    "    .dropna(subset=['market_cap'])\n",
    "    .drop_duplicates(subset=['symbol','year'], keep='first')\n",
    ")\n",
    "\n",
    "# 4.7 Merge into main DataFrame\n",
    "df = df.merge(\n",
    "    mc_annual[['symbol','year','market_cap']],\n",
    "    on=['symbol','year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4.8 Quick check\n",
    "print(df[['instrument','year','market_cap']].drop_duplicates().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96be53",
   "metadata": {},
   "source": [
    "## 5. Merge Equity Volatility (Daily Returns, 252-Day Window)\n",
    "\n",
    "- Load equity volatility calculated from daily returns (Bharath & Shumway 2008)  \n",
    "- 252-day window from year t-1, annualized using √252  \n",
    "- Merge into main `df` by ticker and year  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9ff0bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.787039Z",
     "iopub.status.busy": "2025-10-02T09:25:31.786826Z",
     "iopub.status.idle": "2025-10-02T09:25:31.814800Z",
     "shell.execute_reply": "2025-10-02T09:25:31.813972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading equity volatility (daily-based, 252-day window)...\n",
      "\n",
      "[INFO] Applying strict data quality filter: Dropping bank-years without complete volatility...\n",
      "  Initial bank-years: 1,427\n",
      "  With complete equity volatility: 1,362 (95.4%)\n",
      "  Dropped (incomplete data): 65 (4.6%)\n",
      "  → Only complete observations will proceed to Merton solver\n",
      "\n",
      "  Bank-years retained by year:\n",
      "    2016: 68 banks\n",
      "    2017: 131 banks\n",
      "    2018: 190 banks\n",
      "    2019: 203 banks\n",
      "    2020: 207 banks\n",
      "    2021: 207 banks\n",
      "    2022: 208 banks\n",
      "    2023: 148 banks\n",
      "[WARN] Dropping 2 rows with missing required inputs.\n",
      "instrument  year\n",
      "      COFS  2018\n",
      "      COFS  2019\n",
      "[INFO] Surviving rows by year after input validation:\n",
      "year\n",
      "2016     68\n",
      "2017    131\n",
      "2018    189\n",
      "2019    202\n",
      "2020    207\n",
      "2021    207\n",
      "2022    208\n",
      "2023    148\n",
      "dtype: int64\n",
      "[INFO] Adding sigma_E provenance columns...\n",
      "  Warning: Provenance columns not found\n",
      "[INFO] Creating time-tagged columns...\n",
      "  E_t: 1360 values\n",
      "  F_t: 1360 values\n",
      "  rf_t: 1360 values\n",
      "  sigma_E_tminus1: 1360 values\n",
      "  equity_vol: 1360 values\n"
     ]
    }
   ],
   "source": [
    "# 5. Load and Merge Equity Volatility (Daily, 252-day window)\n",
    "print('[INFO] Loading equity volatility (daily-based, 252-day window)...')\n",
    "\n",
    "# 5.1 Load DAILY equity volatility file\n",
    "equity_vol = pd.read_csv(vol_fp)\n",
    "\n",
    "# 5.2 DAILY FORMAT: Use ticker and sigma_E from daily calculations\n",
    "equity_vol['ticker_prefix'] = equity_vol['ticker']\n",
    "equity_vol['equity_volatility'] = equity_vol['sigma_E']\n",
    "vol_annual = equity_vol[['ticker_prefix','year','equity_volatility']]\n",
    "\n",
    "# 5.3 Merge into main DataFrame\n",
    "df = df.merge(\n",
    "    vol_annual,\n",
    "    on=['ticker_prefix','year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# DATA QUALITY FILTER: Drop bank-years without complete volatility\n",
    "print('\\n[INFO] Applying strict data quality filter: Dropping bank-years without complete volatility...')\n",
    "initial_count = len(df)\n",
    "df = df[df['equity_volatility'].notna()].copy()\n",
    "filtered_count = len(df)\n",
    "dropped_count = initial_count - filtered_count\n",
    "\n",
    "print(f'  Initial bank-years: {initial_count:,}')\n",
    "print(f'  With complete equity volatility: {filtered_count:,} ({filtered_count/initial_count*100:.1f}%)')\n",
    "print(f'  Dropped (incomplete data): {dropped_count:,} ({dropped_count/initial_count*100:.1f}%)')\n",
    "print(f'  → Only complete observations will proceed to Merton solver')\n",
    "\n",
    "# Show retained bank-years by year\n",
    "if dropped_count > 0:\n",
    "    retained_by_year = df.groupby('year').size()\n",
    "    print(f'\\n  Bank-years retained by year:')\n",
    "    for year, count in retained_by_year.items():\n",
    "        print(f'    {year}: {count} banks')\n",
    "\n",
    "# 5.4 Validate required inputs\n",
    "required_columns = ['instrument', 'year', 'market_cap', 'equity_volatility', 'rf', 'debt_total']\n",
    "missing_required = [col for col in required_columns if col not in df.columns]\n",
    "if missing_required:\n",
    "    missing_str = ', '.join(missing_required)\n",
    "    raise AssertionError(f\"Missing required columns after merge: {missing_str}\")\n",
    "\n",
    "invalid_mask = df[required_columns].isna().any(axis=1)\n",
    "if invalid_mask.any():\n",
    "    invalid_count = invalid_mask.sum()\n",
    "    print(f\"[WARN] Dropping {invalid_count} rows with missing required inputs.\")\n",
    "    invalid_summary = df.loc[invalid_mask, ['instrument','year']].copy()\n",
    "    print(invalid_summary.to_string(index=False))\n",
    "    df = df[~invalid_mask].copy()\n",
    "    df['solver_status'] = 'pending'\n",
    "else:\n",
    "    print(\"[INFO] All required inputs present for merged dataset.\")\n",
    "\n",
    "# 5.5 Finalize equity volatility column\n",
    "df['equity_vol'] = df['equity_volatility']\n",
    "\n",
    "# 5.6 Surviving row counts by year\n",
    "surviving_rows_by_year = df.groupby('year').size().sort_index()\n",
    "print('[INFO] Surviving rows by year after input validation:')\n",
    "print(surviving_rows_by_year)\n",
    "\n",
    "# 5.7 Add sigma_E provenance columns from merged equity volatility\n",
    "print('[INFO] Adding sigma_E provenance columns...')\n",
    "\n",
    "# Merge provenance columns from equity_vol DataFrame\n",
    "provenance_cols = ['ticker_prefix', 'year', 'sigma_E_method', 'sigma_E_window_months']\n",
    "if all(col in equity_vol.columns for col in provenance_cols):\n",
    "    vol_provenance = equity_vol[provenance_cols].copy()\n",
    "    df = df.merge(vol_provenance, on=['ticker_prefix', 'year'], how='left')\n",
    "    \n",
    "    # Calculate window years from window_months\n",
    "    df['sigmaE_window_end_year'] = df['year'] - 1\n",
    "    df['sigma_E_window_months'] = df['sigma_E_window_months'].fillna(0)\n",
    "    df['sigmaE_window_start_year'] = df['sigmaE_window_end_year'] - (df['sigma_E_window_months'] / 12 - 1).clip(lower=0).astype(int)\n",
    "    \n",
    "    print(f'  Provenance added: {df[\"sigma_E_method\"].notna().sum()} rows')\n",
    "    print(f'  Methods: {df[\"sigma_E_method\"].value_counts().to_dict()}')\n",
    "else:\n",
    "    print('  Warning: Provenance columns not found')\n",
    "    df['sigmaE_window_end_year'] = df['year'] - 1\n",
    "    df['sigmaE_window_start_year'] = df['year'] - 1\n",
    "    df['sigma_E_method'] = 'unknown'\n",
    "    df['sigma_E_window_months'] = 0\n",
    "\n",
    "# 5.8 Create time-tagged columns for solver\n",
    "print('[INFO] Creating time-tagged columns...')\n",
    "\n",
    "df['E_t'] = df['market_cap']  # Equity value at time t\n",
    "df['F_t'] = df['F']  # Face value of debt\n",
    "df['rf_t'] = df['rf']  # Risk-free rate\n",
    "df['T'] = 1.0  # Time horizon\n",
    "df['sigma_E_tminus1'] = df['equity_volatility']  # From file, already at t-1\n",
    "\n",
    "print(f'  E_t: {df[\"E_t\"].notna().sum()} values')\n",
    "print(f'  F_t: {df[\"F_t\"].notna().sum()} values')\n",
    "print(f'  rf_t: {df[\"rf_t\"].notna().sum()} values')\n",
    "print(f'  sigma_E_tminus1: {df[\"sigma_E_tminus1\"].notna().sum()} values')\n",
    "print(f'  equity_vol: {df[\"equity_vol\"].notna().sum()} values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "time_assertions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Validating time integrity...\n",
      "  F_t range: 0.00e+00 to 6.53e+11\n",
      "  F_t null count: 0\n",
      "[ERROR] 16 rows have non-positive F_t:\n",
      "    instrument  year    F  F_t  debt_total\n",
      "413       EXSR  2021  0.0  0.0         0.0\n",
      "499        FHB  2021  0.0  0.0         0.0\n",
      "901       NKSH  2019  0.0  0.0         0.0\n",
      "902       NKSH  2020  0.0  0.0         0.0\n",
      "903       NKSH  2021  0.0  0.0         0.0\n",
      "904       NKSH  2022  0.0  0.0         0.0\n",
      "941       OPBK  2021  0.0  0.0         0.0\n",
      "968       OVLY  2018  0.0  0.0         0.0\n",
      "969       OVLY  2019  0.0  0.0         0.0\n",
      "970       OVLY  2020  0.0  0.0         0.0\n",
      "[INFO] Filtered to 1344 rows with positive F_t\n",
      "[PASS] All time integrity assertions passed\n",
      "  - sigma_E uses only data up to t-1: True\n",
      "  - No future data in windows: True\n"
     ]
    }
   ],
   "source": [
    "# TIME INTEGRITY ASSERTIONS\n",
    "print('[INFO] Validating time integrity...')\n",
    "\n",
    "# Import time checks\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from utils.time_checks import assert_time_integrity\n",
    "\n",
    "# Assertion 1: sigma_E window must end at t-1\n",
    "assert (df['sigmaE_window_end_year'] == df['year'] - 1).all(), \\\n",
    "    'sigma_E window end must be t-1 (no lookahead)'\n",
    "\n",
    "# Assertion 2: rf_t must be present\n",
    "assert df['rf_t'].notna().all(), 'rf_t must be present for all rows'\n",
    "\n",
    "# Assertion 3: E_t and F_t must be positive for solver rows\n",
    "solver_rows = df['equity_vol'].notna()\n",
    "assert (df.loc[solver_rows, 'E_t'] > 0).all(), 'E_t must be positive'\n",
    "print(f\"  F_t range: {df.loc[solver_rows, 'F_t'].min():.2e} to {df.loc[solver_rows, 'F_t'].max():.2e}\")\n",
    "print(f\"  F_t null count: {df.loc[solver_rows, 'F_t'].isna().sum()}\")\n",
    "# Debug F_t issues\n",
    "bad_F = df.loc[solver_rows & (df['F_t'] <= 0)]\n",
    "if len(bad_F) > 0:\n",
    "    print(f'[ERROR] {len(bad_F)} rows have non-positive F_t:')\n",
    "    print(bad_F[['instrument', 'year', 'F', 'F_t', 'debt_total']].head(10))\n",
    "    # Filter out bad rows\n",
    "    df = df[df['F_t'] > 0].copy()\n",
    "    solver_rows = df['equity_vol'].notna()\n",
    "    print(f'[INFO] Filtered to {len(df)} rows with positive F_t')\n",
    "\n",
    "# F_t assertion now satisfied after filtering\n",
    "\n",
    "# Run comprehensive time integrity check\n",
    "assert_time_integrity(df)\n",
    "\n",
    "print('[PASS] All time integrity assertions passed')\n",
    "print(f'  - sigma_E uses only data up to t-1: {(df[\"sigmaE_window_end_year\"] == df[\"year\"] - 1).all()}')\n",
    "print(f'  - No future data in windows: {(df[\"sigmaE_window_end_year\"] < df[\"year\"]).all()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0774b02",
   "metadata": {},
   "source": [
    "## 6. Define the Merton Model Solver (Revised Equations)\n",
    "\n",
    "In the Merton framework, the firm’s equity is treated as a European call option on its assets. We observe:\n",
    "\n",
    "- **E**: equity market value (scaled market capitalization)  \n",
    "- **sigma_E**: annualized equity volatility  \n",
    "- **F**: total debt (face value)  \n",
    "- **r_f**: risk-free rate  \n",
    "- **T**: time horizon (1 year)  \n",
    "\n",
    "We solve for the unobserved:\n",
    "\n",
    "- **V**: total asset value  \n",
    "- **sigma_V**: asset volatility  \n",
    "\n",
    "by enforcing two conditions:\n",
    "\n",
    "1.  **Option-pricing relation**  \n",
    "    $$\n",
    "      E \\;=\\; V\\,\\Phi(d_{1})\\;-\\;F\\,e^{-r_{f}T}\\,\\Phi(d_{2})\n",
    "    $$\n",
    "2.  **Volatility link**  \n",
    "    $$\n",
    "      \\sigma_{E} \\;=\\;\\frac{V}{E}\\,\\Phi(d_{1})\\,\\sigma_{V}\n",
    "    $$\n",
    "\n",
    "where  \n",
    "$$\n",
    "  d_{1} \\;=\\;\\frac{\\ln\\!\\bigl(V/F\\bigr)\\;+\\;\\bigl(r_{f} + \\tfrac12\\,\\sigma_{V}^{2}\\bigr)\\,T}\n",
    "                      {\\sigma_{V}\\,\\sqrt{T}},\n",
    "  \\quad\n",
    "  d_{2} \\;=\\; d_{1} \\;-\\;\\sigma_{V}\\,\\sqrt{T},\n",
    "$$  \n",
    "and $\\Phi$ is the standard normal CDF.  \n",
    "\n",
    "We use a numerical root-finder (`scipy.optimize.root`) to find $V$, $\\sigma_{V}$ that makes both equations zero:\n",
    "\n",
    "$$\n",
    "\\text{Find }V,\\sigma_{V}\\text{ such that both equations } = 0\n",
    "$$\n",
    "\n",
    "### What the root-finder actually does, in simple terms\n",
    "\n",
    "1. **Start with a guess**  \n",
    "    We begin by guessing values for $(V,\\sigma_{V})$. A natural choice is  \n",
    "    $$\n",
    "      V_0 = E + F,\\quad \\sigma_{V,0} = \\sigma_E\n",
    "    $$  \n",
    "    This says \"assets are roughly equity plus debt\" and \"asset volatility is like equity volatility.\"\n",
    " \n",
    " 2. **Measure \"how wrong\" we are**  \n",
    "    We compute the two expressions  \n",
    "    $$\n",
    "      f_1(V,\\sigma_V),\\quad f_2(V,\\sigma_V)\n",
    "    $$  \n",
    "    which tell us how far from zero each equation is. If both are exactly zero, our guess solves the problem.\n",
    " \n",
    " 3. **Adjust the guess**  \n",
    "    If either $f_1$ or $f_2$ is not zero, the solver estimates a small change to $(V,\\sigma_{V})$ that should educe the errors. It uses derivatives and smart heuristics under the hood.\n",
    " \n",
    " 4. **Repeat until \"close enough\"**  \n",
    "    The process repeats—compute residuals, update guess, compute again—until both residuals are below a tiny tolerance (converged), or we hit an iteration limit (no convergence).\n",
    " \n",
    " 5. **Result**  \n",
    "    - If converged: we obtain $(V^*, \\sigma_{V}^*)$, the asset value and volatility consistent with observed equity data.  \n",
    "    - If not: we flag the failure and typically record NaN values.\n",
    " \n",
    " By packaging our two Merton equations into one Python function, `scipy.optimize.root` handles the iteration, step-size choices, and convergence checks automatically. This allows us to solve these otherwise intractable nonlinear equations with minimal custom code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stable_solver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stable Merton solver loaded\n",
      "  - Uses log space for V and sigma_V\n",
      "  - Clips d1, d2 to [-35, 35]\n",
      "  - Uses E_model in volatility equation denominator\n",
      "  - Robust loss function (soft_l1)\n"
     ]
    }
   ],
   "source": [
    "# STABLE MERTON SOLVER with numerical safeguards\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "Phi = norm.cdf\n",
    "\n",
    "def _d12(V, F, rf, sV, T):\n",
    "    \"\"\"Compute d1, d2 with numerical safety.\"\"\"\n",
    "    srt = sV * np.sqrt(T)\n",
    "    d1 = (np.log(V/F) + (rf + 0.5*sV*sV)*T) / srt\n",
    "    d2 = d1 - srt\n",
    "    # Numerical safety: clip to prevent overflow in Phi\n",
    "    d1 = np.clip(d1, -35, 35)\n",
    "    d2 = np.clip(d2, -35, 35)\n",
    "    return d1, d2\n",
    "\n",
    "def residuals(theta, E_obs, sE_obs, F, rf, T=1.0):\n",
    "    \"\"\"Residual function in log space for stability.\"\"\"\n",
    "    # theta = [logV, logSigmaV]\n",
    "    V = np.exp(theta[0])\n",
    "    sV = np.exp(theta[1])\n",
    "    \n",
    "    d1, d2 = _d12(V, F, rf, sV, T)\n",
    "    \n",
    "    # Price equation\n",
    "    E_model = V*Phi(d1) - F*np.exp(-rf*T)*Phi(d2)\n",
    "    \n",
    "    # Volatility equation: use E_model in denominator for stability\n",
    "    sE_model = (V / max(E_model, 1e-12)) * Phi(d1) * sV\n",
    "    \n",
    "    # Relative scaling for better convergence\n",
    "    r_price = (E_model - E_obs) / max(E_obs, 1.0)\n",
    "    r_vol = sE_model - sE_obs\n",
    "    \n",
    "    return np.array([r_price, r_vol])\n",
    "\n",
    "def solve_one(E_obs, sE_obs, F, rf, T=1.0):\n",
    "    \"\"\"Solve for V and sigma_V with robust bounds and method.\"\"\"\n",
    "    # Input validation\n",
    "    if not (E_obs > 0 and sE_obs > 0 and F > 0):\n",
    "        return None\n",
    "    \n",
    "    # Initial guess\n",
    "    V0 = max(E_obs + F, 1.001*F)\n",
    "    sV0 = min(max(sE_obs, 1e-3), 1.5)\n",
    "    th0 = np.log([V0, sV0])\n",
    "    \n",
    "    # Bounds in log space\n",
    "    lo = np.log([1.001*F, 1e-4])\n",
    "    hi = np.log([1e3*(E_obs + F), 3.0])\n",
    "    \n",
    "    # Solve with robust settings\n",
    "    res = least_squares(\n",
    "        residuals, th0, args=(E_obs, sE_obs, F, rf, T),\n",
    "        method='trf',\n",
    "        loss='soft_l1',  # Robust to outliers\n",
    "        ftol=1e-10,\n",
    "        xtol=1e-10,\n",
    "        gtol=1e-10,\n",
    "        max_nfev=1000,\n",
    "        bounds=(lo, hi)\n",
    "    )\n",
    "    \n",
    "    if not res.success:\n",
    "        return None\n",
    "    \n",
    "    # Extract solution\n",
    "    V = float(np.exp(res.x[0]))\n",
    "    sV = float(np.exp(res.x[1]))\n",
    "    \n",
    "    return V, sV, float(res.cost), res.nfev, res.status\n",
    "\n",
    "print('[INFO] Stable Merton solver loaded')\n",
    "print('  - Uses log space for V and sigma_V')\n",
    "print('  - Clips d1, d2 to [-35, 35]')\n",
    "print('  - Uses E_model in volatility equation denominator')\n",
    "print('  - Robust loss function (soft_l1)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pre_solve_validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Validating inputs before solver...\n",
      "  E_t: 8.12e+07 to 4.89e+11\n",
      "  F_t: 1.40e+04 to 6.53e+11\n",
      "  rf_t: 0.0004 to 0.0495\n",
      "  sigma_E_tminus1: 0.1445 to 0.9880\n",
      "\n",
      "  E_t/F_t leverage ratio:\n",
      "count      1344.000000\n",
      "mean        162.528717\n",
      "std        3415.977975\n",
      "min           0.098664\n",
      "25%           1.386046\n",
      "50%           2.667520\n",
      "75%           5.142831\n",
      "max      107896.482941\n",
      "dtype: float64\n",
      "\n",
      "[INFO] After filtering: 1344 rows ready for solver\n"
     ]
    }
   ],
   "source": [
    "# PRE-SOLVE VALIDATION\n",
    "print('[INFO] Validating inputs before solver...')\n",
    "\n",
    "# Gate 1: E_t must be positive\n",
    "bad_E = df['E_t'] <= 0\n",
    "if bad_E.any():\n",
    "    print(f'[WARN] Filtering {bad_E.sum()} rows with non-positive E_t')\n",
    "    df = df[~bad_E].copy()\n",
    "print(f'  E_t: {df[\"E_t\"].min():.2e} to {df[\"E_t\"].max():.2e}')\n",
    "\n",
    "# Gate 2: F_t must be positive\n",
    "bad_F = df['F_t'] <= 0\n",
    "if bad_F.any():\n",
    "    print(f'[WARN] Filtering {bad_F.sum()} rows with non-positive F_t')\n",
    "    df = df[~bad_F].copy()\n",
    "print(f'  F_t: {df[\"F_t\"].min():.2e} to {df[\"F_t\"].max():.2e}')\n",
    "\n",
    "# Gate 3: rf_t in reasonable range (decimals, not percents)\n",
    "bad_rf = ~df['rf_t'].between(-0.1, 0.3)\n",
    "if bad_rf.any():\n",
    "    print(f'[WARN] Filtering {bad_rf.sum()} rows with rf_t outside [-0.1, 0.3]')\n",
    "    df = df[~bad_rf].copy()\n",
    "print(f'  rf_t: {df[\"rf_t\"].min():.4f} to {df[\"rf_t\"].max():.4f}')\n",
    "\n",
    "# Gate 4: sigma_E_tminus1 in reasonable range\n",
    "valid_sigma = df['sigma_E_tminus1'].notna()\n",
    "bad_sigma = valid_sigma & ~df['sigma_E_tminus1'].between(1e-4, 3.0)\n",
    "if bad_sigma.any():\n",
    "    print(f'[WARN] Filtering {bad_sigma.sum()} rows with sigma_E_tminus1 outside [0.0001, 3.0]')\n",
    "    print(f'  Range before filter: {df.loc[valid_sigma, \"sigma_E_tminus1\"].min():.4f} to {df.loc[valid_sigma, \"sigma_E_tminus1\"].max():.4f}')\n",
    "    df = df[~bad_sigma].copy()\n",
    "    valid_sigma = df['sigma_E_tminus1'].notna()\n",
    "if valid_sigma.any():\n",
    "    print(f'  sigma_E_tminus1: {df.loc[valid_sigma, \"sigma_E_tminus1\"].min():.4f} to {df.loc[valid_sigma, \"sigma_E_tminus1\"].max():.4f}')\n",
    "\n",
    "# Show leverage distribution\n",
    "print('\\n  E_t/F_t leverage ratio:')\n",
    "print(df.eval('E_t/F_t').describe())\n",
    "\n",
    "print(f'\\n[INFO] After filtering: {len(df)} rows ready for solver')\n",
    "\n",
    "# Reset index after filtering to avoid duplicates\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "run_stable_solver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running stable Merton solver on each row...\n",
      "  Processing 1344 rows...\n",
      "\n",
      "[INFO] Solver complete:\n",
      "  Converged: 1344/1344 (100.0%)\n",
      "\n",
      "Status counts:\n",
      "status_flag\n",
      "converged    1344\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Run the stable Merton solver\n",
    "print('[INFO] Running stable Merton solver on each row...')\n",
    "\n",
    "results = []\n",
    "solver_rows = df['sigma_E_tminus1'].notna()\n",
    "total_rows = solver_rows.sum()\n",
    "\n",
    "print(f'  Processing {total_rows} rows...')\n",
    "\n",
    "for idx, row in df[solver_rows].iterrows():\n",
    "    result = solve_one(\n",
    "        E_obs=row['E_t'],\n",
    "        sE_obs=row['sigma_E_tminus1'],\n",
    "        F=row['F_t'],\n",
    "        rf=row['rf_t'],\n",
    "        T=1.0\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        V, sV, cost, nfev, status = result\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'asset_value': V,\n",
    "            'asset_vol': sV,\n",
    "            'solver_cost': cost,\n",
    "            'nfev': nfev,\n",
    "            'status_flag': 'converged'\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'asset_value': np.nan,\n",
    "            'asset_vol': np.nan,\n",
    "            'solver_cost': np.nan,\n",
    "            'nfev': 0,\n",
    "            'status_flag': 'no_converge'\n",
    "        })\n",
    "\n",
    "# Merge results\n",
    "results_df = pd.DataFrame(results).set_index('index')\n",
    "df = df.join(results_df)\n",
    "\n",
    "# Fill status_flag for rows without sigma_E\n",
    "df['status_flag'] = df['status_flag'].fillna('no_sigma_E')\n",
    "\n",
    "# Report\n",
    "converged = (df['status_flag'] == 'converged').sum()\n",
    "print(f'\\n[INFO] Solver complete:')\n",
    "print(f'  Converged: {converged}/{total_rows} ({100*converged/total_rows:.1f}%)' if total_rows > 0 else '  No rows to solve')\n",
    "print('\\nStatus counts:')\n",
    "print(df['status_flag'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a81c3bfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:31.818066Z",
     "iopub.status.busy": "2025-10-02T09:25:31.817661Z",
     "iopub.status.idle": "2025-10-02T09:25:31.833923Z",
     "shell.execute_reply": "2025-10-02T09:25:31.832743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ABCB 2016: asset_value = 3633285480.83, asset_vol = 0.2187, status = no_converge\n"
     ]
    }
   ],
   "source": [
    "# 7. Define solver and run a quick sanity check\n",
    "\n",
    "# Guarded imports allow this cell to run standalone in interactive sessions\n",
    "try:\n",
    "    pd  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    import pandas as pd  # noqa: F401\n",
    "try:\n",
    "    np  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    import numpy as np\n",
    "try:\n",
    "    norm  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    from scipy.stats import norm  # noqa: F401\n",
    "try:\n",
    "    least_squares  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    from scipy.optimize import least_squares\n",
    "try:\n",
    "    tqdm  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover - interactive safeguard\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "if 'T' not in globals():  # pragma: no cover - interactive safeguard\n",
    "    T = 1.0\n",
    "\n",
    "\n",
    "def merton_solver(row, *, T=T, tol_E=tol_E, tol_sigma=tol_sigma, max_iter=max_iter):\n",
    "    '''Solve for asset value and volatility using the Merton model.\n",
    "\n",
    "    Returns a dictionary with solver diagnostics so downstream analysis can\n",
    "    inspect failures without re-running the optimizer.\n",
    "    '''\n",
    "    result_template = {\n",
    "        'asset_value': np.nan,\n",
    "        'asset_vol': np.nan,\n",
    "        'E_model': np.nan,\n",
    "        'sigmaE_model': np.nan,\n",
    "        'r1': np.nan,\n",
    "        'r2': np.nan,\n",
    "        'solver_message': '',\n",
    "        'nfev': np.nan,\n",
    "        'iterations': np.nan,\n",
    "        'status_flag': 'invalid_inputs',\n",
    "    }\n",
    "\n",
    "    E = row['market_cap']\n",
    "    sigma_E = row['equity_vol']\n",
    "    F = row['F']\n",
    "    r_f = row['rf']\n",
    "\n",
    "    # 1. Input validation\n",
    "    if pd.isna(E) or pd.isna(sigma_E) or pd.isna(F):\n",
    "        result = result_template.copy()\n",
    "        result['solver_message'] = 'missing_input'\n",
    "        return result\n",
    "    if E <= 0 or sigma_E <= 0 or F < 0:\n",
    "        result = result_template.copy()\n",
    "        result['solver_message'] = 'invalid_value'\n",
    "        return result\n",
    "    if F == 0:\n",
    "        result = result_template.copy()\n",
    "        result['status_flag'] = 'no_debt'\n",
    "        result['solver_message'] = 'zero_debt'\n",
    "        return result\n",
    "\n",
    "    # 2. System of Merton equations\n",
    "    def residuals(x):\n",
    "        asset_value, sigma_V = x\n",
    "        if sigma_V <= 0:\n",
    "            return np.array([np.inf, np.inf])\n",
    "        d1 = (np.log(asset_value / F) + (r_f + 0.5 * sigma_V**2) * T) / (sigma_V * np.sqrt(T))\n",
    "        d2 = d1 - sigma_V * np.sqrt(T)\n",
    "        E_model = asset_value * norm.cdf(d1) - F * np.exp(-r_f * T) * norm.cdf(d2)\n",
    "        sigmaE_model_obs = (asset_value / max(E, np.finfo(float).eps)) * norm.cdf(d1) * sigma_V\n",
    "        r1 = E_model - E\n",
    "        r2 = sigmaE_model_obs - sigma_E\n",
    "        return np.array([r1, r2])\n",
    "\n",
    "    # 3. Initial guess and solve (with bounds)\n",
    "    lower_V = np.nextafter(F, np.inf)\n",
    "    lower_bounds = np.array([lower_V, 1e-6])\n",
    "    upper_bounds = np.array([np.inf, np.inf])\n",
    "    initial_V = max(E + F, lower_V * 1.0001)\n",
    "    initial_sigma = max(min(sigma_E, 1.0), 1e-6)\n",
    "    initial = np.array([initial_V, initial_sigma])\n",
    "\n",
    "    try:\n",
    "        sol = least_squares(\n",
    "            residuals,\n",
    "            initial,\n",
    "            bounds=(lower_bounds, upper_bounds),\n",
    "            loss='soft_l1',\n",
    "            max_nfev=max_iter,\n",
    "        )\n",
    "    except ValueError as exc:\n",
    "        result = result_template.copy()\n",
    "        result['solver_message'] = f'least_squares_error: {exc}'\n",
    "        return result\n",
    "\n",
    "    asset_value, sigma_V = sol.x\n",
    "    d1 = (np.log(asset_value / F) + (r_f + 0.5 * sigma_V**2) * T) / (sigma_V * np.sqrt(T))\n",
    "    d2 = d1 - sigma_V * np.sqrt(T)\n",
    "    E_model = asset_value * norm.cdf(d1) - F * np.exp(-r_f * T) * norm.cdf(d2)\n",
    "    if E_model <= 0:\n",
    "        sigmaE_model = np.nan\n",
    "    else:\n",
    "        sigmaE_model = (asset_value / E_model) * norm.cdf(d1) * sigma_V\n",
    "    sigmaE_model_obs = (asset_value / max(E, np.finfo(float).eps)) * norm.cdf(d1) * sigma_V\n",
    "    r1 = E_model - E\n",
    "    r2 = sigmaE_model_obs - sigma_E\n",
    "\n",
    "    converged = sol.success and abs(r1) <= tol_E and abs(r2) <= tol_sigma\n",
    "\n",
    "    result = result_template.copy()\n",
    "    result.update({\n",
    "        'asset_value': asset_value,\n",
    "        'asset_vol': sigma_V,\n",
    "        'E_model': E_model,\n",
    "        'sigmaE_model': sigmaE_model,\n",
    "        'r1': r1,\n",
    "        'r2': r2,\n",
    "        'solver_message': sol.message,\n",
    "        'nfev': sol.nfev,\n",
    "        'iterations': getattr(sol, 'njev', np.nan),\n",
    "        'status_flag': 'converged' if converged else 'no_converge',\n",
    "    })\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_merton_solver(df):\n",
    "    '''Apply the Merton solver row-wise with a progress bar.'''\n",
    "    records = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc='Solving Merton model'):\n",
    "        records.append(merton_solver(row))\n",
    "    return pd.DataFrame(records, index=df.index)\n",
    "\n",
    "\n",
    "# ---- Quick check on the first row ----\n",
    "first_row = df.iloc[0]\n",
    "first_result = merton_solver(first_row)\n",
    "print(\n",
    "    f\"Results for {first_row['instrument']} {first_row['year']}: \"\n",
    "    f\"asset_value = {first_result['asset_value']:.2f}, \"\n",
    "    f\"asset_vol = {first_result['asset_vol']:.4f}, \"\n",
    "    f\"status = {first_result['status_flag']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7650e9",
   "metadata": {},
   "source": [
    "## 7 Compute Distance to Default (DD) and Probability of Default (PD) in Detail\n",
    "\n",
    "Once we have solved for:\n",
    "\n",
    "- $V$ = total asset value  \n",
    "- $sigma_V$ = asset volatility  \n",
    "\n",
    "we compute:\n",
    "\n",
    "1. **Distance to Default**  \n",
    "   \n",
    "$$DD = (ln(V/F) + (r_f - 0.5 sigma_V^2)T) / (sigma_V * √T)\n",
    "  $$ \n",
    "   - **Numerator**  \n",
    "     - $ln(V/F)$: how far assets exceed debt on a log scale  \n",
    "     - $(r_f - 0.5 sigma_V^2)T$: drift adjustment for risk-free growth minus half variance  \n",
    "   - **Denominator**  \n",
    "     - $sigma_V √T$: scales by volatility over the horizon  \n",
    "\n",
    "2. **Probability of Default**  \n",
    "   \n",
    "   PD = Φ(-DD)\n",
    "   \n",
    "   where Φ is the standard normal CDF. Intuitively, low DD means a higher chance assets fall below debt.\n",
    "\n",
    "We also handle the special case **no debt** (F=0), for which DD and PD are undefined (we set them to NaN).\n",
    "\n",
    "**IMPORTANT FIX**: The original code had a unit mismatch where:\n",
    "- Market cap (E) was in actual USD \n",
    "- Debt total (F) was in USD millions from the source data\n",
    "\n",
    "This created unrealistic V/F ratios of millions, leading to DDm values >100 and PDm = 0 due to numerical underflow. The fix multiplies `debt_total` by 1,000,000 to convert to actual USD.\n",
    "\n",
    "Below is code that computes these step by step, with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "safe_dd_pd_computation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing DD_m and PD_m...\n",
      "  Valid rows used for DD_m: 1344\n",
      "  DD_m range: 0.86 to 35.00\n",
      "  PD_m range: 1.12e-268 to 1.94e-01\n"
     ]
    }
   ],
   "source": [
    "# 7.2 Compute DD_m and PD_m safely\n",
    "# d1 = [ln(V/F) + (r_f + 0.5*sigma_V^2)T] / (sigma_V*sqrt(T))\n",
    "# d2 = d1 - sigma_V*sqrt(T)  -> DD_m = d2,  PD_m = Phi(-d2)\n",
    "\n",
    "print('[INFO] Computing DD_m and PD_m...')\n",
    "\n",
    "Phi = getattr(norm, 'cdf', None)\n",
    "if Phi is None:\n",
    "    from math import erf\n",
    "    def Phi(x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return 0.5*(1.0 + np.vectorize(erf)(x/np.sqrt(2.0)))\n",
    "\n",
    "F_t  = df['F']\n",
    "rf_t = df['rf']\n",
    "V_t  = df['asset_value']\n",
    "sV_t = df['asset_vol']\n",
    "T = 1.0\n",
    "\n",
    "converged = (df['status_flag'] == 'converged')\n",
    "valid = (\n",
    "    converged\n",
    "    & np.isfinite(V_t) & (V_t > 0)\n",
    "    & np.isfinite(sV_t) & (sV_t > 0)\n",
    "    & np.isfinite(F_t) & (F_t > 0)\n",
    "    & np.isfinite(rf_t)\n",
    ")\n",
    "\n",
    "# Initialize outputs\n",
    "df['d1'] = np.nan\n",
    "df['d2'] = np.nan\n",
    "df['DD_m'] = np.nan\n",
    "df['PD_m'] = np.nan\n",
    "df['solver_status'] = df['status_flag']\n",
    "\n",
    "# Compute only on valid rows\n",
    "idx = np.where(valid)[0]\n",
    "if idx.size:\n",
    "    srt = sV_t.values[idx] * np.sqrt(T)\n",
    "    d1 = (np.log(V_t.values[idx] / F_t.values[idx]) + (rf_t.values[idx] + 0.5 * sV_t.values[idx]**2) * T) / srt\n",
    "    d2 = d1 - srt\n",
    "    # Numerical safety\n",
    "    d1 = np.clip(d1, -35, 35)\n",
    "    d2 = np.clip(d2, -35, 35)\n",
    "    \n",
    "    df.loc[valid, 'd1']   = d1\n",
    "    df.loc[valid, 'd2']   = d2\n",
    "    df.loc[valid, 'DD_m'] = d2\n",
    "    df.loc[valid, 'PD_m'] = Phi(-d2)\n",
    "\n",
    "print(f'  Valid rows used for DD_m: {int(valid.sum())}')\n",
    "if valid.sum() > 0:\n",
    "    print(f'  DD_m range: {df.loc[valid, \"DD_m\"].min():.2f} to {df.loc[valid, \"DD_m\"].max():.2f}')\n",
    "    print(f'  PD_m range: {df.loc[valid, \"PD_m\"].min():.2e} to {df.loc[valid, \"PD_m\"].max():.2e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solver_preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of results:\n",
      "  instrument  year   asset_value  asset_vol         d1        d2      DD_m  \\\n",
      "0       ABCB  2016  3.633310e+09   0.197770   8.968234  8.770464  8.770464   \n",
      "1       ABCB  2017  3.685325e+09   0.229487  10.204427  9.974939  9.974939   \n",
      "2       ABCB  2018  2.439065e+09   0.232307   9.808610  9.576303  9.576303   \n",
      "3       ABCB  2019  4.445621e+09   0.188803   5.799107  5.610304  5.610304   \n",
      "4       ABCB  2020  3.182115e+09   0.209984   8.390052  8.180068  8.180068   \n",
      "\n",
      "           PD_m status_flag solver_status  \n",
      "0  8.896580e-19   converged     converged  \n",
      "1  9.811087e-24   converged     converged  \n",
      "2  5.029064e-22   converged     converged  \n",
      "3  1.009855e-08   converged     converged  \n",
      "4  1.418415e-16   converged     converged  \n",
      "\n",
      "Solver status counts:\n",
      "status_flag\n",
      "converged    1344\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valid rows used for DD_m: 1344\n",
      "\n",
      "V/F (leverage) summary on valid rows:\n",
      "count      1344.000000\n",
      "mean        163.513786\n",
      "std        3415.978008\n",
      "min           1.094271\n",
      "25%           2.374747\n",
      "50%           3.645329\n",
      "75%           6.138727\n",
      "max      107897.474973\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7.3 Quick sanity checks and preview\n",
    "preview_cols = [\n",
    "    'instrument', 'year', 'asset_value', 'asset_vol', 'E_model', 'sigmaE_model',\n",
    "    'r1', 'r2', 'd1', 'd2', 'DD_m', 'PD_m', 'status_flag', 'solver_status'\n",
    "]\n",
    "# Only show columns that exist\n",
    "preview_cols = [c for c in preview_cols if c in df.columns]\n",
    "print('Preview of results:')\n",
    "print(df[preview_cols].head())\n",
    "print()\n",
    "\n",
    "print('Solver status counts:')\n",
    "print(df['status_flag'].value_counts())\n",
    "print()\n",
    "\n",
    "# Extra diagnostics\n",
    "valid = (df['status_flag'] == 'converged') & df['DD_m'].notna()\n",
    "if valid.sum() > 0:\n",
    "    print(f'Valid rows used for DD_m: {int(valid.sum())}')\n",
    "    print('\\nV/F (leverage) summary on valid rows:')\n",
    "    print((df.loc[valid, 'asset_value'] / df.loc[valid, 'F']).describe())\n",
    "else:\n",
    "    print('[WARN] No valid DD_m values computed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f405d6",
   "metadata": {},
   "source": [
    "## 8. Export Results and Log Diagnostics\n",
    "\n",
    "In this final step, we:\n",
    "\n",
    "1. **Save** the full DataFrame (including `DDm` and `PDm`) to CSV for downstream modelling.  \n",
    "2. **Append** a diagnostic summary to our log file, including:  \n",
    "   - Total rows processed  \n",
    "   - Solver status breakdown  \n",
    "   - Basic statistics on `DDm` and `PDm`  \n",
    "   - Count of missing or failed estimates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0511cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T09:25:34.647748Z",
     "iopub.status.busy": "2025-10-02T09:25:34.647520Z",
     "iopub.status.idle": "2025-10-02T09:25:34.746229Z",
     "shell.execute_reply": "2025-10-02T09:25:34.745259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARCHIVE] Moved to archive: market_20251011_042629.csv\n",
      "[CLEANUP] Removed old archive: market_20251005_032750.csv\n",
      "[INFO] Results exported to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/market_20251014_022125.csv\n",
      "[INFO] Diagnostics appended to log: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/logs/dd_pd_market_log.txt\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Archiving and timestamped output\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    # Move all existing files to archive\n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] Moved to archive: {os.path.basename(old_file)}\")\n",
    "    \n",
    "    # Clean up archive to keep only max_keep files\n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] Removed old archive: {os.path.basename(old_archive)}\")\n",
    "\n",
    "# Rename columns to standard naming convention\n",
    "df = df.rename(columns={'DDm': 'DD_m', 'PDm': 'PD_m'})\n",
    "\n",
    "# Archive old market files and save new one with timestamp\n",
    "archive_old_files(output_dir, archive_dir, 'market', max_keep=5)\n",
    "\n",
    "timestamp = get_timestamp_cdt()\n",
    "output_fp = output_dir / f'market_{timestamp}.csv'\n",
    "# Add provenance columns for time integrity audit\n",
    "provenance_cols = [\"E_t\", \"F_t\", \"rf_t\", \"sigma_E_tminus1\", \n",
    "                   \"sigmaE_window_start_year\", \"sigmaE_window_end_year\", \n",
    "                   \"V_t\", \"sigma_V_t\", \"d1\", \"d2\", \"DD_m\", \"PD_m\", \n",
    "                   \"solver_status\", \"resid_price\", \"resid_vol\"]\n",
    "\n",
    "df.to_csv(output_fp, index=False)\n",
    "print(f\"[INFO] Results exported to: {output_fp}\")\n",
    "\n",
    "# 8.2 Append diagnostics to the log file\n",
    "with open(log_fp, 'a') as log:\n",
    "    log.write(\"\\n=== DD/PD Market-Based Model Diagnostics ===\\n\")\n",
    "    # Total rows\n",
    "    total = len(df)\n",
    "    log.write(f\"Total rows processed: {total}\\n\")\n",
    "    # Deduplication audit\n",
    "    if duplicate_log_entries:\n",
    "        log.write(\"\\nDeduplicated instrument-year rows (kept max debt_total):\\n\")\n",
    "        for entry in duplicate_log_entries:\n",
    "            log.write(entry + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nDeduplicated instrument-year rows: none detected.\\n\")\n",
    "    # Rows removed due to invalid inputs\n",
    "    if invalid_input_summary is not None and not invalid_input_summary.empty:\n",
    "        log.write(\"\\nRows dropped due to invalid inputs (solver_status=invalid_inputs):\\n\")\n",
    "        log.write(invalid_input_summary.to_string(index=False) + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nRows dropped due to invalid inputs: none.\\n\")\n",
    "    # Surviving rows by year\n",
    "    if surviving_rows_by_year is not None and not surviving_rows_by_year.empty:\n",
    "        log.write(\"\\nSurviving rows by year after input validation:\\n\")\n",
    "        log.write(surviving_rows_by_year.to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nSurviving rows by year after input validation: not available.\\n\")\n",
    "    # Solver status counts\n",
    "    status_counts = df['solver_status'].value_counts()\n",
    "    log.write(\"Solver status counts:\\n\")\n",
    "    log.write(status_counts.to_string() + \"\\n\")\n",
    "    # DD_m and PD_m summary\n",
    "    log.write(\"\\nDistance to Default (DD_m) summary:\\n\")\n",
    "    log.write(df['DD_m'].describe().to_string() + \"\\n\")\n",
    "    log.write(\"\\nProbability of Default (PD_m) summary:\\n\")\n",
    "    log.write(df['PD_m'].describe().to_string() + \"\\n\")\n",
    "    F_values = df['F']\n",
    "    leverage_ratio = (df['market_cap'] / F_values).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if not leverage_ratio.empty:\n",
    "        median_ratio = leverage_ratio.median()\n",
    "        log.write(f\"\\nUnit check (market_cap / F_values) median: {median_ratio:.3f}\\n\")\n",
    "        log.write(\"Expected order of magnitude ~= 1 when both legs are in USD.\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nUnit check skipped: insufficient data for market_cap/F comparison.\\n\")\n",
    "    # Missing/failure counts\n",
    "    missing_dd = df['DD_m'].isna().sum()\n",
    "    missing_pd = df['PD_m'].isna().sum()\n",
    "    log.write(f\"\\nRows with missing DD_m: {missing_dd}\\n\")\n",
    "    log.write(f\"Rows with missing PD_m: {missing_pd}\\n\")\n",
    "\n",
    "print(f\"[INFO] Diagnostics appended to log: {log_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "summary_generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating summary statistics by year...\n",
      "[INFO] Summary statistics saved to market_20251014_022125_summary.csv\n",
      "\n",
      "Summary preview:\n",
      "   year metric           p10           p25           p50           p75  \\\n",
      "0  2016   DD_m  5.476170e+00  6.515713e+00  7.843591e+00  9.107503e+00   \n",
      "1  2017   DD_m  4.708746e+00  5.567261e+00  7.115652e+00  8.551055e+00   \n",
      "2  2018   DD_m  5.098445e+00  5.931710e+00  7.065331e+00  8.304750e+00   \n",
      "3  2019   DD_m  5.244325e+00  6.091519e+00  7.658579e+00  8.980456e+00   \n",
      "4  2020   DD_m  5.235378e+00  6.194743e+00  7.559966e+00  9.755017e+00   \n",
      "5  2021   DD_m  1.928283e+00  2.321138e+00  2.879043e+00  3.619382e+00   \n",
      "6  2022   DD_m  4.050001e+00  4.566353e+00  5.665929e+00  6.809619e+00   \n",
      "7  2023   DD_m  4.378479e+00  4.947104e+00  5.950258e+00  7.694768e+00   \n",
      "8  2016   PD_m  1.432983e-29  4.216354e-20  2.189579e-15  5.020230e-11   \n",
      "9  2017   PD_m  8.457630e-25  9.406359e-18  5.569258e-13  1.308464e-08   \n",
      "\n",
      "            p90  \n",
      "0  1.140263e+01  \n",
      "1  1.021542e+01  \n",
      "2  1.146149e+01  \n",
      "3  1.205152e+01  \n",
      "4  1.320659e+01  \n",
      "5  4.556160e+00  \n",
      "6  8.629619e+00  \n",
      "7  9.357699e+00  \n",
      "8  2.195974e-08  \n",
      "9  1.246226e-06  \n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics by year\n",
    "print('[INFO] Generating summary statistics by year...')\n",
    "\n",
    "# Filter to converged rows only for summary\n",
    "converged_df = df[df['status_flag'] == 'converged'].copy()\n",
    "\n",
    "if len(converged_df) > 0:\n",
    "    # Calculate percentiles by year for DD_m and PD_m\n",
    "    summary_data = []\n",
    "    \n",
    "    for year in sorted(converged_df['year'].unique()):\n",
    "        year_data = converged_df[converged_df['year'] == year]\n",
    "        \n",
    "        # DD_m percentiles\n",
    "        dd_percentiles = year_data['DD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        summary_data.append({\n",
    "            'year': year,\n",
    "            'metric': 'DD_m',\n",
    "            'p10': dd_percentiles[0.1],\n",
    "            'p25': dd_percentiles[0.25],\n",
    "            'p50': dd_percentiles[0.5],\n",
    "            'p75': dd_percentiles[0.75],\n",
    "            'p90': dd_percentiles[0.9]\n",
    "        })\n",
    "    \n",
    "    for year in sorted(converged_df['year'].unique()):\n",
    "        year_data = converged_df[converged_df['year'] == year]\n",
    "        \n",
    "        # PD_m percentiles\n",
    "        pd_percentiles = year_data['PD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        summary_data.append({\n",
    "            'year': year,\n",
    "            'metric': 'PD_m',\n",
    "            'p10': pd_percentiles[0.1],\n",
    "            'p25': pd_percentiles[0.25],\n",
    "            'p50': pd_percentiles[0.5],\n",
    "            'p75': pd_percentiles[0.75],\n",
    "            'p90': pd_percentiles[0.9]\n",
    "        })\n",
    "    \n",
    "    # Overall statistics\n",
    "    dd_overall = converged_df['DD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "    summary_data.append({\n",
    "        'year': 'overall',\n",
    "        'metric': 'DD_m',\n",
    "        'p10': dd_overall[0.1],\n",
    "        'p25': dd_overall[0.25],\n",
    "        'p50': dd_overall[0.5],\n",
    "        'p75': dd_overall[0.75],\n",
    "        'p90': dd_overall[0.9]\n",
    "    })\n",
    "    \n",
    "    pd_overall = converged_df['PD_m'].quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "    summary_data.append({\n",
    "        'year': 'overall',\n",
    "        'metric': 'PD_m',\n",
    "        'p10': pd_overall[0.1],\n",
    "        'p25': pd_overall[0.25],\n",
    "        'p50': pd_overall[0.5],\n",
    "        'p75': pd_overall[0.75],\n",
    "        'p90': pd_overall[0.9]\n",
    "    })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save summary to analysis directory\n",
    "    summary_fp = base_dir / 'data' / 'outputs' / 'analysis' / f'market_{timestamp}_summary.csv'\n",
    "    summary_df.to_csv(summary_fp, index=False)\n",
    "    print(f'[INFO] Summary statistics saved to {summary_fp.name}')\n",
    "    print(f'\\nSummary preview:')\n",
    "    print(summary_df.head(10))\n",
    "else:\n",
    "    print('[WARN] No converged rows found. Summary not generated.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
