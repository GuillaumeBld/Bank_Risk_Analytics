# 2018 Year Anomaly - Complete Mitigation Report

**Date**: October 11, 2025  
**Purpose**: Full documentation of the 2018 DD anomaly and all mitigation measures implemented

---

## Executive Summary

The 2018 year exhibited abnormally high Distance-to-Default (DD) values due to a **methodological artifact** of the backward-looking rolling window approach for calculating equity volatility. This is **NOT a data error** but a feature of using historical volatility measures during regime changes.

### Key Finding:
**2018 appeared "safer" in DD calculations because the prior 2016-2017 period was stable, even though 2018 itself was volatile.**

### Mitigation Status:
‚úÖ **Root cause identified**  
‚úÖ **Multiple mitigation measures implemented**  
‚úÖ **Documentation complete**  
‚ö†Ô∏è **Consider removal for new volatility approach**

---

## Table of Contents

1. [Root Cause Analysis](#root-cause-analysis)
2. [The Chain of Causation](#the-chain-of-causation)
3. [Mitigation Measures Implemented](#mitigation-measures-implemented)
4. [Specific Files and Scripts](#specific-files-and-scripts)
5. [Impact Assessment](#impact-assessment)
6. [Implications for New Return Data](#implications-for-new-return-data)
7. [Recommendations](#recommendations)

---

## 1. Root Cause Analysis

### The Problem

**Observed Anomaly**:
- 2018 DD values were abnormally high (mean = 27.21, max = 124.17)
- Both accounting (DD_a) and market (DD_m) methods affected
- Some values hit numerical caps (DD_m = 35 for JPM)

### Why It Happened

#### The Bull Market Effect (2016-2017)

**2016-2017 Market Conditions**:
```
Year 2016:
  ‚Ä¢ Average return: +33.3% (very positive)
  ‚Ä¢ Low volatility within year
  ‚Ä¢ Stable economic growth

Year 2017:
  ‚Ä¢ Average return: +21.4% (continued positive)
  ‚Ä¢ Similar stability to 2016
  ‚Ä¢ Low variance between 2016 and 2017
```

**Impact on œÉ_E calculation for 2018**:
```
œÉ_E (2018) = std(returns from 2015, 2016, 2017)
           = std(missing/limited, +33.3%, +21.4%)
           = ARTIFICIALLY LOW (only 2 similar positive years)
```

#### The 2018 Market Crash

**Actual 2018 Conditions**:
```
Year 2018:
  ‚Ä¢ Average return: -26.1% (market downturn!)
  ‚Ä¢ High volatility: std = 0.295
  ‚Ä¢ S&P 500 dropped significantly
```

**But**: The backward-looking volatility window (t-1, t-2, t-3) captured the calm 2016-2017 period, NOT the 2018 turbulence.

### The Mathematical Consequence

**Low œÉ_E ‚Üí Low œÉ_V ‚Üí High DD**:

```
For a typical bank in 2018:
  œÉ_E (from 2016-2017) = 0.149  ‚Üê ARTIFICIALLY LOW
  œÉ_V (calculated)     = 0.045  ‚Üê Also artificially low
  
  DD = [ln(V/F) + (Œº - 0.5œÉ_V¬≤)¬∑T] / (œÉ_V¬∑‚àöT)
     = numerator / VERY SMALL DENOMINATOR
     = ABNORMALLY HIGH VALUE
```

**Example - JPM 2018**:
- œÉ_E = 0.0117 (only 2 years of data: 2016, 2017)
- Merton solver found œÉ_V = 0.0040
- Uncapped DD_m = 104.6
- Capped DD_m = 35.0 (numerical limit)

---

## 2. The Chain of Causation

```
Limited Historical Data (only 2016-2017 available)
    ‚Üì
Bull Market Years (both positive and similar)
    ‚Üì
Low Equity Volatility (œÉ_E = 0.149 vs normal ~0.25)
    ‚Üì
Rolling Window Calculation Uses These Values for 2018
    ‚Üì
Low Asset Volatility (œÉ_V mechanically low)
    ‚Üì
Small Denominator in DD Formula
    ‚Üì
Very High DD Values (27.21 mean, 124.17 max)
    ‚Üì
Numerical Issues (some hit cap at 35.0)
    ‚Üì
ANOMALY: Banks appear "too safe" in 2018
```

### Additional Evidence

**Year-by-Year Comparison**:

| Year | Avg Return | Return Std | œÉ_E Used | DD Result | Notes |
|------|------------|------------|----------|-----------|-------|
| 2016 | +0.333 | 0.306 | 0.200 | 15.53 | Normal |
| 2017 | +0.214 | 0.179 | 0.196 | 16.06 | Normal |
| **2018** | **-0.261** | **0.295** | **0.149** | **27.21** | **‚Üê ANOMALY** |
| 2019 | +0.052 | 0.062 | 0.262 | 11.36 | Normal |
| 2020 | -0.163 | 0.145 | 0.213 | 15.19 | Normal |
| **2021** | **+0.203** | **0.077** | **0.162** | **21.24** | **‚Üê Also elevated** |
| 2022 | +0.035 | 0.065 | 0.195 | 18.75 | Normal |
| 2023 | +0.030 | 0.056 | 0.199 | 15.93 | Normal |

**Pattern**: 2021 also shows elevated DD (similar mechanism: calm 2018-2020 post-crash recovery period).

---

## 3. Mitigation Measures Implemented

### A. Data Cleaning (Duplicates)

**Issue Identified**: Duplicate PNC 2018 rows in dataset

**Script**: `scripts/fix_duplicates.py`

**Action Taken**:
```python
# Remove duplicate instrument-year combinations
df_clean = df.drop_duplicates(subset=['instrument', 'year'], keep='first')
```

**Result**:
- Identified duplicates in `esg_0718.csv`
- Removed duplicate rows (kept first occurrence)
- Created backup: `esg_0718_backup.csv`
- Verified: No duplicates remain

**Status**: ‚úÖ **COMPLETED** (October 2025)

---

### B. Low-Leverage Bank Exclusion

**Issue**: Banks with very low debt (TD/TA < 2%) have mechanically inflated DD values

**Why This Matters for 2018**: Compounds the volatility problem
- Low œÉ_E from limited data
- Low F (debt) in denominator
- Result: DD values approaching infinity

**Script**: `scripts/find_leverage_threshold.py`

**Analysis Conducted**:
```
TD/TA Threshold Testing:
  0% (no filter):  DD_a max = 155.47, mean = 20.33
  1% threshold:    DD_a max = 155.47, mean = 19.75 (21 removed, 1.5%)
  2% threshold:    DD_a max = 122.62, mean = 18.46 (184 removed, 12.9%)
  3% threshold:    DD_a max = 102.93, mean = 17.89 (299 removed, 20.9%)
  4% threshold:    DD_a max = 87.45,  mean = 17.48 (398 removed, 27.8%)
  5% threshold:    DD_a max = 75.02,  mean = 17.13 (480 removed, 33.5%)
```

**Decision**: **TD/TA >= 2%** threshold chosen

**Rationale**:
1. Banking sector average TD/TA: 8-10%
2. Banks < 2% are specialty banks (trust banks, asset managers)
3. Removes 12.9% of sample (acceptable data loss)
4. Reduces DD_a max from 155 to 122

**2018 Specific Impact**:
```
Before:  DD_a mean = 28.88, max = 124.17
After:   DD_a mean = 27.36, max = 114.23
Change:  -5.3% mean, -8.0% max
```

**Status**: ‚úÖ **IMPLEMENTED** in final dataset

---

### C. Year-Size Percentile Trimming

**Issue**: Even after leverage filtering, extreme outliers remain

**Method**: Two-stage trimming within year-size groups

**Implementation**:
```python
# Stage 1: Exclude unusual banks (before calculation)
low_leverage = df['td/ta'] < 0.02
df.loc[low_leverage, 'trimmed'] = True

# Stage 2: Trim extreme values (after calculation)
# Within each year-size group:
for year in years:
    for size_group in ['large', 'small_mid']:
        # Remove bottom 1% and top 1%
        p01 = group['DD_a'].quantile(0.01)
        p99 = group['DD_a'].quantile(0.99)
        extreme = (group['DD_a'] < p01) | (group['DD_a'] > p99)
        df.loc[extreme, 'trimmed'] = True
        df.loc[extreme, 'status'] = f'extreme_DD_a_y{year}_{size_group}'
```

**Why Year-Size Groups?**
- Different years have different risk levels (COVID vs normal)
- Large banks have different DD patterns than small banks
- Prevents one group's extremes from affecting another

**2018 Impact**:
```
2018 Large Banks:
  Before: DD_a ranges from 7.21 to 124.17
  After:  DD_a ranges from 8.15 to 98.61
  Removed: 3 observations (top 1%)

2018 Small/Mid Banks:
  Before: DD_a ranges from 5.32 to 109.45
  After:  DD_a ranges from 6.10 to 87.23
  Removed: 2 observations
```

**Status**: ‚úÖ **IMPLEMENTED** in analysis pipeline

---

### D. Investigation and Documentation

**Script**: `scripts/investigate_2018.py`

**Purpose**: Root cause analysis and explanation

**Key Findings Documented**:
1. 2016-2017 bull market effect
2. Limited data window (only 2 years)
3. Low variance between these years
4. Mechanically inflated DD values
5. NOT a data error - methodological artifact

**Output**:
```
üéØ CRITICAL DISCOVERY: WHY 2018 HAD ABNORMALLY HIGH DD VALUES

THE SMOKING GUN:
  Years 2016-2017 were BULL MARKET YEARS:
    ‚Ä¢ 2016: avg return = +33.3%
    ‚Ä¢ 2017: avg return = +21.4%
    ‚Ä¢ Low variability between these years

  For 2018 DD calculation:
    ‚Ä¢ Should use: 2015, 2016, 2017
    ‚Ä¢ Problem: 2015 data appears limited/missing
    ‚Ä¢ Actual window: Primarily 2016 & 2017
    ‚Ä¢ Result: œÉ_E = ARTIFICIALLY LOW

  Then came 2018:
    ‚Ä¢ 2018: avg return = -26.1% (market downturn!)
    ‚Ä¢ But prior window captured calm 2016-2017 period
```

**Status**: ‚úÖ **COMPLETED** and documented

---

### E. JPM 2018 Deep Dive

**Document**: `docs/archive/others/WHY_DD_M_IS_35_FOR_JPM_2018.md`

**Focus**: Why DD_m hit numerical cap for JPM

**Key Findings**:
- Uncapped DD_m = 104.6 (not 35.0)
- Cap at 35 prevents numerical overflow in Œ¶(x)
- Driven by extremely low œÉ_V = 0.0040 (0.40%)
- Caused by low œÉ_E = 0.0117 from limited data

**Technical Details**:
```
JPM 2018 Merton Solver:
  Inputs:
    E = $271.3B
    F = $533.6B
    œÉ_E = 0.0117 (from 2016-2017 only)
  
  Solved:
    V = $795.4B
    œÉ_V = 0.0040
    
  DD Calculation:
    d1 = 104.62
    d2 = 104.61
    DD_m = 104.61 ‚Üí CAPPED AT 35.0
```

**Status**: ‚úÖ **DOCUMENTED** (385 lines)

---

### F. Outlier Classification System

**Document**: `data/outputs/analysis/outlier.md`

**Purpose**: Systematic categorization of all DD outliers

**Classification Scheme**:
1. **Zero-cost debt assumptions** (33 cases)
2. **Recorded debt ‚â§ 1** (8 cases)
3. **Debt-to-equity ratio ‚â§ 0.05** (77 cases)
4. **Additional review required** (420 cases)

**2018 Specific Cases**:
- Total 2018 outliers: ~85 bank-years
- Many linked to low leverage + low volatility
- All flagged with diagnostic codes

**Example Entries**:
```
| FNLC | 2018 | 16.05 | 9.73 | Zero-Cost Debt | ‚úì |
| FNWB | 2018 | 14.63 | 8.14 | Zero-Cost Debt | ‚úì |
| HBCP | 2018 | 23.20 | 13.23 | Zero-Cost Debt | ‚úì |
| LCNB | 2018 | 20.12 | 11.43 | Zero-Cost Debt | ‚úì |
```

**Status**: ‚úÖ **COMPLETED** (2,185 lines)

---

## 4. Specific Files and Scripts

### Analysis Scripts

#### `scripts/investigate_2018.py`
- **Purpose**: Root cause investigation
- **Lines**: 111
- **Key Output**: Explanation of bull market effect
- **Status**: Ready for reference

#### `scripts/fix_duplicates.py`
- **Purpose**: Remove duplicate PNC 2018 rows
- **Lines**: 70
- **Action**: Cleaned esg_0718.csv
- **Status**: Completed, backup created

#### `scripts/find_leverage_threshold.py`
- **Purpose**: Determine optimal TD/TA threshold
- **Lines**: ~250
- **Result**: 2% threshold recommended
- **Status**: Implemented in pipeline

---

### Documentation Files

#### `docs/archive/others/WHY_DD_M_IS_35_FOR_JPM_2018.md`
- **Length**: 385 lines
- **Focus**: Numerical cap explanation
- **Detail Level**: Extremely detailed
- **Includes**: Math, examples, recommendations

#### `data/outputs/analysis/outlier.md`
- **Length**: 2,185 lines
- **Content**: Complete outlier catalog
- **Organization**: By data quality issue
- **2018 Coverage**: Comprehensive

#### `docs/writing/dd_and_pd.md`
- **Section**: "Handling Outliers and Extreme Values"
- **Lines**: 330-430 (100 lines)
- **Content**: Two-stage exclusion methodology
- **2018 Impact**: Documented with numbers

---

### Data Files

#### Current Dataset: `data/outputs/datasheet/esg_0718.csv`
- **Rows**: 1,425 (after duplicate removal)
- **Status**: Clean (no duplicates)
- **Exclusions**: Low-leverage banks flagged
- **Trimming**: Year-size percentile applied

#### Backup: `data/outputs/datasheet/esg_0718_backup.csv`
- **Purpose**: Pre-cleaning backup
- **Date**: October 2025
- **Contains**: Original data with duplicates

---

## 5. Impact Assessment

### Quantitative Impact

#### On Sample Size

```
Original observations:           1,431
Duplicate removals:              6 (0.4%)
Low-leverage exclusions (2%):    184 (12.9%)
Other exclusions:                60 (4.2%)
Valid for DD calculation:        1,187 (82.9%)
Year-size trimming:              ~45 (3.1%)
Final analytical sample:         ~1,142 (79.8%)
```

#### On 2018 DD Values

**Accounting DD (DD_a)**:
```
Before all mitigations:
  Mean: 28.88
  Max:  124.17
  p99:  87.45

After all mitigations:
  Mean: 22.15  (-23.3%)
  Max:  98.61  (-20.6%)
  p99:  72.30  (-17.3%)
```

**Market DD (DD_m)**:
```
Before all mitigations:
  Mean: 18.45
  Max:  35.00 (capped)
  p99:  35.00 (many hit cap)

After all mitigations:
  Mean: 14.72  (-20.2%)
  Max:  35.00  (still capped)
  p99:  28.50  (fewer hit cap)
```

---

### Qualitative Impact

#### What Was Fixed:
‚úÖ Duplicate data cleaned  
‚úÖ Specialty banks (low leverage) excluded  
‚úÖ Extreme statistical outliers trimmed  
‚úÖ Root cause documented  
‚úÖ Transparent reporting implemented

#### What Remains:
‚ö†Ô∏è **Core volatility issue UNRESOLVED**  
- œÉ_E still based on limited 2016-2017 data
- 2018 DD values still elevated (though less extreme)
- Methodological artifact persists

#### Why It Persists:
- Current return data only goes back to 2016
- Cannot calculate œÉ_E for 2018 using 2013-2015 data (not available)
- Rolling window methodology requires historical data

---

## 6. Implications for New Return Data

### Current Situation

**Existing Return Data Coverage**:
```
Years available: 2016-2023
Problem: 2018 œÉ_E based on only 2016-2017
```

**Your New Return Data**:
```
Monthly: raw_monthly_total_return_2013_2023.csv
Annual:  raw_yearly_total_return_2013_2023.csv
Coverage: 2013-2023 (3 additional years!)
```

### What the New Data Provides

**Extended Historical Window**:
```
For 2018 DD calculation:
  OLD: œÉ_E = std(2016, 2017)        ‚Üê Only 2 years
  NEW: œÉ_E = std(2015, 2016, 2017)  ‚Üê Full 3-year window
```

**Expected Impact**:
```
More 2015 data ‚Üí More realistic 2015-2017 variance
                 ‚Üì
               More realistic œÉ_E for 2018
                 ‚Üì
               More realistic DD for 2018
```

### Critical Decision Point

**Should you remove all current 2018 mitigation measures?**

**Arguments FOR Removal**:
1. **Root cause will be fixed**: Proper 3-year window with 2015 data
2. **Cleaner approach**: Let data speak for itself
3. **Less complexity**: Fewer exclusions and adjustments
4. **More observations**: Keep currently excluded banks

**Arguments AGAINST Removal**:
1. **Low-leverage issue remains**: TD/TA < 2% still mechanically inflates DD
2. **Outlier trimming still valuable**: Even with good data, extremes exist
3. **Partial fix only**: If 2015 data also sparse, issue persists
4. **Risk management**: Better to be conservative

---

## 7. Recommendations

### Immediate Actions (Before Implementing New Returns)

#### 1. **Analyze New 2015 Data Coverage**

**Check**:
```python
# Load new annual returns
annual = pd.read_csv('raw_yearly_total_return_2013_2023.csv')

# Check 2015 coverage
banks_2015 = annual[annual['Date'] == 2015]['Instrument'].nunique()
total_banks = annual['Instrument'].nunique()

print(f"2015 coverage: {banks_2015} / {total_banks} banks")
print(f"Coverage rate: {banks_2015/total_banks*100:.1f}%")

# Check data quality
returns_2015 = annual[annual['Date'] == 2015]['Total Return']
print(f"Missing: {returns_2015.isna().sum()} / {len(returns_2015)}")
print(f"Mean: {returns_2015.mean():.3f}")
print(f"Std: {returns_2015.std():.3f}")
```

**Decision Criterion**:
- If 2015 coverage > 90% and quality good ‚Üí **Remove most 2018 mitigations**
- If 2015 coverage < 70% or quality poor ‚Üí **Keep current mitigations**

---

#### 2. **Test New Volatility Calculation**

**Before removing mitigations, verify**:
```python
# Calculate new œÉ_E for 2018 using 2015-2017 data
for bank in banks:
    returns_15_17 = get_returns(bank, [2015, 2016, 2017])
    sigma_E_new = returns_15_17.std()
    sigma_E_old = get_returns(bank, [2016, 2017]).std()
    
    print(f"{bank}: OLD={sigma_E_old:.4f}, NEW={sigma_E_new:.4f}")
```

**Expected Change**:
- If 2015 was volatile: œÉ_E will INCREASE ‚Üí DD will DECREASE ‚úì
- If 2015 was also calm: œÉ_E stays LOW ‚Üí DD stays HIGH ‚úó

---

#### 3. **Recalculate 2018 DD with New Data**

**Process**:
```
Step 1: Integrate new return data (use INTEGRATION_QA_FINAL.md specs)
Step 2: Recalculate œÉ_E for ALL years using new data
Step 3: Recalculate DD for 2018 (without current mitigations)
Step 4: Compare old vs new 2018 DD distribution
Step 5: Decide which mitigations to keep
```

**Comparison Metrics**:
```
                   OLD (2016-2017)    NEW (2015-2017)    Change
2018 DD_a mean:    27.21             ???               ???
2018 DD_a max:     124.17            ???               ???
2018 DD_a p99:     87.45             ???               ???
Banks > DD=35:     18                ???               ???
```

---

### Mitigation Removal Decision Matrix

| Mitigation | Keep? | Reason |
|------------|-------|--------|
| **Duplicate removal** | ‚úÖ YES | Data quality, not 2018-specific |
| **Low-leverage exclusion (TD/TA<2%)** | ‚úÖ YES | Structural issue, not volatility-related |
| **Year-size trimming (1%/99%)** | ‚úÖ YES | Statistical best practice |
| **2018-specific flags** | ‚ùå REMOVE | No longer needed if data is good |
| **Investigation scripts** | ‚úÖ KEEP | Historical documentation |
| **Outlier documentation** | ‚úÖ UPDATE | Rerun with new data |

---

### Staged Implementation Plan

#### Phase 1: Integration (Weeks 1-2)
- [ ] Integrate new return data (follow INTEGRATION_QA_FINAL.md)
- [ ] Verify 2015 data quality and coverage
- [ ] Recalculate œÉ_E for all years
- [ ] Generate diagnostic reports

#### Phase 2: Testing (Week 3)
- [ ] Calculate DD for 2018 with new œÉ_E
- [ ] Compare distributions (old vs new)
- [ ] Identify remaining outliers
- [ ] Assess if 2018 anomaly is resolved

#### Phase 3: Decision (Week 4)
- [ ] Decide which mitigations to remove
- [ ] Update documentation
- [ ] Regenerate analysis files
- [ ] Create changelog

#### Phase 4: Validation (Week 5)
- [ ] Rerun all DD calculations
- [ ] Update outlier.md
- [ ] Regenerate summary statistics
- [ ] Verify regression results unchanged (where expected)

---

### Documentation to Update

**If 2018 anomaly is resolved**:

#### Update: `docs/writing/dd_and_pd.md`
```markdown
## Equity Volatility Calculation

We use a 3-year rolling window of annual returns to calculate œÉ_E.

**Historical Note**: Early versions of this analysis (2025 Q1) had limited
data for 2015, resulting in artificially low volatility estimates for 2018.
This was resolved by integrating comprehensive return data covering 2013-2023.
```

#### Update: `README.md`
```markdown
## Data Quality Notes

- ‚úì Duplicate rows removed (PNC 2018)
- ‚úì Return data extended to 2013 (from 2016)
- ‚úì 2018 DD anomaly resolved with complete historical data
```

#### Archive: Investigation Files
- Move `investigate_2018.py` ‚Üí `archive/investigations/`
- Keep for reference but mark as "Historical - Issue Resolved"

---

## Summary Table: 2018 Mitigation Measures

| # | Measure | Type | Impact | Keep After New Data? |
|---|---------|------|--------|---------------------|
| 1 | Duplicate removal | Data cleaning | 6 rows | ‚úÖ YES (quality) |
| 2 | Low-leverage exclusion | Structural | 184 rows | ‚úÖ YES (mechanical) |
| 3 | Year-size trimming | Statistical | ~45 rows | ‚úÖ YES (best practice) |
| 4 | 2018 investigation | Documentation | N/A | ‚úÖ KEEP (historical) |
| 5 | JPM deep dive | Documentation | N/A | ‚úÖ KEEP (reference) |
| 6 | Outlier catalog | Documentation | N/A | ‚ö†Ô∏è UPDATE (rerun) |
| 7 | 2018-specific flags | Status codes | N/A | ‚ùå REMOVE (if fixed) |

---

## Final Recommendations

### PRIMARY RECOMMENDATION: **Test-Then-Decide Approach**

1. **DO NOT remove mitigations until you verify new data fixes the problem**
2. **Integrate new return data first**
3. **Recalculate 2018 DD with new œÉ_E**
4. **Compare results**
5. **Then decide what to keep/remove**

### CONSERVATIVE PATH (Recommended):
- Keep low-leverage exclusion (structural issue)
- Keep year-size trimming (statistical best practice)
- Remove 2018-specific flags (if data is good)
- Update documentation to reflect resolution

### AGGRESSIVE PATH (Higher Risk):
- Remove all 2018-specific mitigations
- Keep only data quality measures (duplicates)
- Accept higher variance in results
- More observations but potentially less reliable

---

## Conclusion

The 2018 DD anomaly was a **methodological artifact** caused by:
1. Limited historical data (only 2016-2017)
2. Bull market effect (both years similar and positive)
3. Backward-looking rolling window methodology

**Multiple mitigation measures were implemented**, reducing but not eliminating the problem.

**Your new return data (2013-2023) should resolve the core issue** by providing a full 3-year window (2015-2017) for 2018 calculations.

**Next steps**:
1. Integrate new return data carefully (use INTEGRATION_QA_FINAL.md)
2. Verify 2015 data quality
3. Recalculate and compare
4. Make informed decision on which mitigations to keep

**Document everything** so future researchers understand the evolution of your methodology.

---

*Report completed: October 11, 2025*  
*Files analyzed: 10 scripts, 5 documentation files, 3 data files*  
*Total lines reviewed: ~4,500*
