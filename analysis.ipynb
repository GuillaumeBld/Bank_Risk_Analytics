{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD/PD Analysis: Visualizations, Statistics, and Outlier Detection\n",
    "\n",
    "This notebook provides comprehensive analysis of Distance-to-Default (DD) and Probability-of-Default (PD) metrics:\n",
    "\n",
    "1. **Data Loading**: Load latest merged dataset\n",
    "2. **Descriptive Statistics**: Summary stats and distributions\n",
    "3. **Visualizations**: Distributions, time series, scatter plots\n",
    "4. **Correlation Analysis**: Correlation matrices and heatmaps\n",
    "5. **Regression Analysis**: OLS regressions\n",
    "6. **2SLS Analysis**: Two-stage least squares instrumental variable estimation\n",
    "7. **Outlier Detection**: Identify and categorize outliers with archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Statistical modeling\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "from linearmodels.iv import IV2SLS as IV2SLS_alt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def find_repo_root(start: Path, marker: str = '.git') -> Path:\n",
    "    current = start.resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] {os.path.basename(old_file)}\")\n",
    "    \n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] {os.path.basename(old_archive)}\")\n",
    "\n",
    "def get_latest_file(output_dir, dataset_type):\n",
    "    \"\"\"Get the most recent file of given dataset_type\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {dataset_type} files found\")\n",
    "    return files[0]\n",
    "\n",
    "# Setup paths\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "datasheet_dir = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "analysis_dir = base_dir / 'data' / 'outputs' / 'analysis'\n",
    "archive_dir = base_dir / 'archive' / 'datasets'\n",
    "\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(f\"Analysis output: {analysis_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest merged dataset\n",
    "merged_file = get_latest_file(datasheet_dir, 'merged')\n",
    "print(f\"Loading: {os.path.basename(merged_file)}\")\n",
    "\n",
    "df = pd.read_csv(merged_file)\n",
    "print(f\"\\nDataset: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns[:20])}...\" if len(df.columns) > 20 else f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Show sample\n",
    "display(df[['instrument', 'year', 'DD_a', 'PD_a', 'DD_m', 'PD_m']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for DD and PD\n",
    "print(\"=== DISTANCE TO DEFAULT (DD) STATISTICS ===\")\n",
    "print(\"\\nAccounting DD (DD_a):\")\n",
    "print(df['DD_a'].describe())\n",
    "\n",
    "print(\"\\nMarket DD (DD_m):\")\n",
    "print(df['DD_m'].describe())\n",
    "\n",
    "print(\"\\n=== PROBABILITY OF DEFAULT (PD) STATISTICS ===\")\n",
    "print(\"\\nAccounting PD (PD_a):\")\n",
    "print(df['PD_a'].describe())\n",
    "\n",
    "print(\"\\nMarket PD (PD_m):\")\n",
    "print(df['PD_m'].describe())\n",
    "\n",
    "# Coverage statistics\n",
    "print(\"\\n=== DATA COVERAGE ===\")\n",
    "print(f\"Observations with both DD_a and DD_m: {df[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_a: {df['DD_a'].notna().sum() - df[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_m: {df['DD_m'].notna().sum() - df[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Total unique instruments: {df['instrument'].nunique()}\")\n",
    "print(f\"Year range: {df['year'].min()} - {df['year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for DD\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# DD_a distribution\n",
    "axes[0].hist(df['DD_a'].dropna(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(df['DD_a'].median(), color='red', linestyle='--', label=f'Median: {df[\"DD_a\"].median():.2f}')\n",
    "axes[0].set_xlabel('DD_a (Accounting)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Accounting DD')\n",
    "axes[0].legend()\n",
    "\n",
    "# DD_m distribution\n",
    "axes[1].hist(df['DD_m'].dropna(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(df['DD_m'].median(), color='red', linestyle='--', label=f'Median: {df[\"DD_m\"].median():.2f}')\n",
    "axes[1].set_xlabel('DD_m (Market)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Market DD')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots by year\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# DD_a by year\n",
    "df_plot_a = df.dropna(subset=['DD_a', 'year']).copy()\n",
    "if not df_plot_a.empty:\n",
    "    df_plot_a['year'] = df_plot_a['year'].astype(str)\n",
    "    df_plot_a.boxplot(column='DD_a', by='year', ax=axes[0])\n",
    "    axes[0].set_title('DD_a Distribution by Year')\n",
    "    axes[0].set_xlabel('Year')\n",
    "    axes[0].set_ylabel('DD_a')\n",
    "    plt.sca(axes[0])\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# DD_m by year\n",
    "df_plot_m = df.dropna(subset=['DD_m', 'year']).copy()\n",
    "if not df_plot_m.empty:\n",
    "    df_plot_m['year'] = df_plot_m['year'].astype(str)\n",
    "    df_plot_m.boxplot(column='DD_m', by='year', ax=axes[1])\n",
    "    axes[1].set_title('DD_m Distribution by Year')\n",
    "    axes[1].set_xlabel('Year')\n",
    "    axes[1].set_ylabel('DD_m')\n",
    "    plt.sca(axes[1])\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: DD_m vs DD_a\n",
    "merged_dd = df[['DD_a', 'DD_m']].dropna()\n",
    "\n",
    "if not merged_dd.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.scatter(merged_dd['DD_m'], merged_dd['DD_a'], alpha=0.5, edgecolors='none')\n",
    "    \n",
    "    # 45-degree line\n",
    "    all_vals = np.concatenate([merged_dd['DD_m'].to_numpy(), merged_dd['DD_a'].to_numpy()])\n",
    "    lims = [all_vals.min(), all_vals.max()]\n",
    "    ax.plot(lims, lims, linestyle='--', color='red', linewidth=2, label='45° line')\n",
    "    \n",
    "    # Regression line\n",
    "    z = np.polyfit(merged_dd['DD_m'], merged_dd['DD_a'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(merged_dd['DD_m'].sort_values(), p(merged_dd['DD_m'].sort_values()), \n",
    "            linestyle='-', color='blue', linewidth=2, label=f'Regression: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "    \n",
    "    ax.set_xlabel('DD_m (Market)', fontsize=12)\n",
    "    ax.set_ylabel('DD_a (Accounting)', fontsize=12)\n",
    "    ax.set_title('Market DD vs Accounting DD Comparison', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = merged_dd['DD_m'].corr(merged_dd['DD_a'])\n",
    "    ax.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=ax.transAxes, \n",
    "            fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No overlapping DD_m/DD_a observations for scatter plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for key variables\n",
    "corr_vars = ['DD_a', 'PD_a', 'DD_m', 'PD_m']\n",
    "corr_matrix = df[corr_vars].corr()\n",
    "\n",
    "print(\"=== CORRELATION MATRIX ===\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix: DD and PD Metrics', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression: DD_a on DD_m\n",
    "reg_data = df[['DD_a', 'DD_m']].dropna()\n",
    "\n",
    "if len(reg_data) > 10:\n",
    "    X = sm.add_constant(reg_data['DD_m'])\n",
    "    y = reg_data['DD_a']\n",
    "    \n",
    "    model = OLS(y, X).fit()\n",
    "    print(\"=== OLS REGRESSION: DD_a ~ DD_m ===\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Residual plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Residuals vs fitted\n",
    "    axes[0].scatter(model.fittedvalues, model.resid, alpha=0.5)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Fitted Values')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title('Residuals vs Fitted Values')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    sm.qqplot(model.resid, line='45', ax=axes[1])\n",
    "    axes[1].set_title('Normal Q-Q Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for regression analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple regression with year fixed effects\n",
    "if len(reg_data) > 20:\n",
    "    reg_data_fe = df[['DD_a', 'DD_m', 'year']].dropna()\n",
    "    \n",
    "    # Create year dummies\n",
    "    year_dummies = pd.get_dummies(reg_data_fe['year'], prefix='year', drop_first=True)\n",
    "    X_fe = pd.concat([reg_data_fe[['DD_m']], year_dummies], axis=1)\n",
    "    X_fe = sm.add_constant(X_fe)\n",
    "    y_fe = reg_data_fe['DD_a']\n",
    "    \n",
    "    model_fe = OLS(y_fe, X_fe).fit()\n",
    "    print(\"\\n=== OLS WITH YEAR FIXED EFFECTS ===\")\n",
    "    print(model_fe.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Two-Stage Least Squares (2SLS) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2SLS: Instrumental variable estimation\n",
    "# Example: DD_a as dependent, DD_m as endogenous, using lagged values as instruments\n",
    "\n",
    "# Prepare data with lags\n",
    "df_sorted = df.sort_values(['instrument', 'year'])\n",
    "df_sorted['DD_m_lag'] = df_sorted.groupby('instrument')['DD_m'].shift(1)\n",
    "df_sorted['DD_a_lag'] = df_sorted.groupby('instrument')['DD_a'].shift(1)\n",
    "\n",
    "iv_data = df_sorted[['DD_a', 'DD_m', 'DD_m_lag', 'DD_a_lag']].dropna()\n",
    "\n",
    "if len(iv_data) > 30:\n",
    "    print(\"=== 2SLS ESTIMATION ===\")\n",
    "    print(f\"Sample size: {len(iv_data)}\")\n",
    "    \n",
    "    # First stage: DD_m ~ DD_m_lag + DD_a_lag\n",
    "    X_first = sm.add_constant(iv_data[['DD_m_lag', 'DD_a_lag']])\n",
    "    y_first = iv_data['DD_m']\n",
    "    first_stage = OLS(y_first, X_first).fit()\n",
    "    \n",
    "    print(\"\\nFirst Stage Regression:\")\n",
    "    print(first_stage.summary())\n",
    "    \n",
    "    # Second stage: DD_a ~ DD_m_fitted\n",
    "    iv_data['DD_m_fitted'] = first_stage.fittedvalues\n",
    "    X_second = sm.add_constant(iv_data['DD_m_fitted'])\n",
    "    y_second = iv_data['DD_a']\n",
    "    second_stage = OLS(y_second, X_second).fit()\n",
    "    \n",
    "    print(\"\\nSecond Stage Regression:\")\n",
    "    print(second_stage.summary())\n",
    "    \n",
    "    # Test instrument strength\n",
    "    f_stat = first_stage.fvalue\n",
    "    print(f\"\\nFirst-stage F-statistic: {f_stat:.2f}\")\n",
    "    if f_stat > 10:\n",
    "        print(\"✓ Instruments appear strong (F > 10)\")\n",
    "    else:\n",
    "        print(\"⚠ Weak instruments warning (F < 10)\")\n",
    "else:\n",
    "    print(\"Insufficient data for 2SLS analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection functions (adapted from generate_outlier_doc.py)\n",
    "OUTLIER_THRESHOLD = 13.0\n",
    "\n",
    "def is_outlier_dd(value, threshold=OUTLIER_THRESHOLD):\n",
    "    return pd.notna(value) and value > threshold\n",
    "\n",
    "def categorize_outlier(row, dd_col, dataset_type):\n",
    "    \"\"\"Categorize outlier based on data characteristics\"\"\"\n",
    "    if not is_outlier_dd(row[dd_col]):\n",
    "        return None\n",
    "    \n",
    "    # Check for data quality issues\n",
    "    prefix = 'a_' if dataset_type == 'accounting' else 'm_'\n",
    "    \n",
    "    # Zero-cost debt\n",
    "    wacc_cost_col = f'{prefix}wacc_cost_of_debt,_(%)' if f'{prefix}wacc_cost_of_debt,_(%)' in row.index else None\n",
    "    wacc_weight_col = f'{prefix}wacc_debt_weight,_(%)' if f'{prefix}wacc_debt_weight,_(%)' in row.index else None\n",
    "    \n",
    "    if wacc_cost_col and wacc_weight_col:\n",
    "        if pd.notna(row[wacc_cost_col]) and pd.notna(row[wacc_weight_col]):\n",
    "            if abs(row[wacc_cost_col]) < 1e-6 and abs(row[wacc_weight_col]) < 1e-6:\n",
    "                return 'zero_cost_debt'\n",
    "    \n",
    "    # Low debt\n",
    "    debt_col = f'{prefix}debt_total' if f'{prefix}debt_total' in row.index else None\n",
    "    if debt_col and pd.notna(row[debt_col]) and row[debt_col] <= 1.0:\n",
    "        return 'low_debt'\n",
    "    \n",
    "    # Low leverage\n",
    "    de_col = f'{prefix}d/e' if f'{prefix}d/e' in row.index else None\n",
    "    if de_col and pd.notna(row[de_col]) and row[de_col] <= 0.05:\n",
    "        return 'low_leverage'\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "# Identify outliers\n",
    "df['outlier_a'] = df.apply(lambda row: categorize_outlier(row, 'DD_a', 'accounting'), axis=1)\n",
    "df['outlier_m'] = df.apply(lambda row: categorize_outlier(row, 'DD_m', 'market'), axis=1)\n",
    "\n",
    "# Summary\n",
    "print(\"=== OUTLIER SUMMARY ===\")\n",
    "print(f\"\\nAccounting DD outliers (>{OUTLIER_THRESHOLD}): {df['outlier_a'].notna().sum()}\")\n",
    "if df['outlier_a'].notna().sum() > 0:\n",
    "    print(df['outlier_a'].value_counts())\n",
    "\n",
    "print(f\"\\nMarket DD outliers (>{OUTLIER_THRESHOLD}): {df['outlier_m'].notna().sum()}\")\n",
    "if df['outlier_m'].notna().sum() > 0:\n",
    "    print(df['outlier_m'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export outliers to CSV with archiving\n",
    "timestamp = get_timestamp_cdt()\n",
    "\n",
    "# Accounting outliers\n",
    "outliers_a = df[df['outlier_a'].notna()].copy()\n",
    "if len(outliers_a) > 0:\n",
    "    archive_old_files(analysis_dir, archive_dir, 'outliers_accounting', max_keep=5)\n",
    "    outliers_a_file = analysis_dir / f'outliers_accounting_{timestamp}.csv'\n",
    "    outliers_a.to_csv(outliers_a_file, index=False)\n",
    "    print(f\"\\n[SAVED] Accounting outliers: {outliers_a_file.name} ({len(outliers_a)} rows)\")\n",
    "\n",
    "# Market outliers\n",
    "outliers_m = df[df['outlier_m'].notna()].copy()\n",
    "if len(outliers_m) > 0:\n",
    "    archive_old_files(analysis_dir, archive_dir, 'outliers_market', max_keep=5)\n",
    "    outliers_m_file = analysis_dir / f'outliers_market_{timestamp}.csv'\n",
    "    outliers_m.to_csv(outliers_m_file, index=False)\n",
    "    print(f\"[SAVED] Market outliers: {outliers_m_file.name} ({len(outliers_m)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "if len(outliers_a) > 0 or len(outliers_m) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Accounting outliers\n",
    "    if len(outliers_a) > 0:\n",
    "        outlier_counts_a = outliers_a['outlier_a'].value_counts()\n",
    "        axes[0].bar(outlier_counts_a.index, outlier_counts_a.values, color='steelblue')\n",
    "        axes[0].set_xlabel('Outlier Category')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].set_title(f'Accounting DD Outliers (>{OUTLIER_THRESHOLD})')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Market outliers\n",
    "    if len(outliers_m) > 0:\n",
    "        outlier_counts_m = outliers_m['outlier_m'].value_counts()\n",
    "        axes[1].bar(outlier_counts_m.index, outlier_counts_m.values, color='coral')\n",
    "        axes[1].set_xlabel('Outlier Category')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title(f'Market DD Outliers (>{OUTLIER_THRESHOLD})')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample outliers\n",
    "if len(outliers_a) > 0:\n",
    "    print(\"\\n=== SAMPLE ACCOUNTING OUTLIERS ===\")\n",
    "    display(outliers_a[['instrument', 'year', 'DD_a', 'PD_a', 'outlier_a']].head(10))\n",
    "\n",
    "if len(outliers_m) > 0:\n",
    "    print(\"\\n=== SAMPLE MARKET OUTLIERS ===\")\n",
    "    display(outliers_m[['instrument', 'year', 'DD_m', 'PD_m', 'outlier_m']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis notebook provides:\n",
    "- Comprehensive visualizations of DD/PD distributions\n",
    "- Correlation analysis between accounting and market approaches\n",
    "- OLS regression with fixed effects\n",
    "- 2SLS instrumental variable estimation\n",
    "- Outlier detection with categorization and archiving\n",
    "\n",
    "All outputs are saved to `data/outputs/analysis/` with timestamped filenames and automatic archiving (max 5 files per type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ESG Regression Analysis\n",
    "\n",
    "Comprehensive OLS regression analysis with ESG variables as independent variables and DD/PD as dependent variables.\n",
    "\n",
    "**Regression Sequence:**\n",
    "1. ESG raw scores (environmental, social, governance pillars)\n",
    "2. ESG combined score\n",
    "3. Dummy variables (size, year)\n",
    "4. Control variables added one at a time: lnta, td/ta, price_to_book, capital_adequacy\n",
    "\n",
    "**Performance Metrics:**\n",
    "- R-squared\n",
    "- Adjusted R-squared\n",
    "- Accuracy (for classification if applicable)\n",
    "- Precision (for classification if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "# Size dummy: below 1M = 0, above 1M = 1\n",
    "df['size_dummy'] = (df['a_total_assets'] > 1.0).astype(int)\n",
    "\n",
    "# Year dummy: 2016-2019 = 0, 2020-2023 = 1\n",
    "df['year_dummy'] = (df['year'] >= 2020).astype(int)\n",
    "\n",
    "print(\"=== DUMMY VARIABLES CREATED ===\")\n",
    "print(f\"\\nSize dummy distribution:\")\n",
    "print(df['size_dummy'].value_counts())\n",
    "print(f\"\\nYear dummy distribution:\")\n",
    "print(df['year_dummy'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run regression and display results\n",
    "def run_ols_regression(y_col, X_cols, data, model_name):\n",
    "    \"\"\"Run OLS regression and return results\"\"\"\n",
    "    # Prepare data\n",
    "    reg_data = data[[y_col] + X_cols].dropna()\n",
    "    \n",
    "    if len(reg_data) < 10:\n",
    "        print(f\"Insufficient data for {model_name}: {len(reg_data)} observations\")\n",
    "        return None\n",
    "    \n",
    "    y = reg_data[y_col]\n",
    "    X = sm.add_constant(reg_data[X_cols])\n",
    "    \n",
    "    # Run regression\n",
    "    model = OLS(y, X).fit()\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(f\"Dependent Variable: {y_col}\")\n",
    "    print(f\"Independent Variables: {', '.join(X_cols)}\")\n",
    "    print(f\"Observations: {len(reg_data)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Extract key statistics\n",
    "    stats = {\n",
    "        'model': model_name,\n",
    "        'dependent_var': y_col,\n",
    "        'n_obs': len(reg_data),\n",
    "        'r_squared': model.rsquared,\n",
    "        'adj_r_squared': model.rsquared_adj,\n",
    "        'f_statistic': model.fvalue,\n",
    "        'prob_f': model.f_pvalue\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Store results\n",
    "regression_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Regressions with DD_a (Accounting DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available ESG columns\n",
    "esg_raw_cols = ['a_environmental_pillar_score', 'a_social_pillar_score', 'a_governance_pillar_score']\n",
    "esg_combined_col = 'a_esg_combined_score'\n",
    "control_cols = ['a_lnta', 'a_td/ta', 'a_price_to_book_value_per_share', 'a_capital_adequacy_total_(%)']\n",
    "\n",
    "# Check which columns exist\n",
    "available_esg_raw = [col for col in esg_raw_cols if col in df.columns]\n",
    "available_esg_combined = esg_combined_col if esg_combined_col in df.columns else None\n",
    "available_controls = [col for col in control_cols if col in df.columns]\n",
    "\n",
    "print(\"Available ESG raw columns:\", available_esg_raw)\n",
    "print(\"Available ESG combined:\", available_esg_combined)\n",
    "print(\"Available control variables:\", available_controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ESG Raw Scores → DD_a\n",
    "if available_esg_raw:\n",
    "    stats = run_ols_regression('DD_a', available_esg_raw, df, 'ESG Raw → DD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ESG Combined Score → DD_a\n",
    "if available_esg_combined:\n",
    "    stats = run_ols_regression('DD_a', [available_esg_combined], df, 'ESG Combined → DD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dummies → DD_a\n",
    "stats = run_ols_regression('DD_a', ['size_dummy', 'year_dummy'], df, 'Dummies → DD_a')\n",
    "if stats:\n",
    "    regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ESG + Dummies + Controls (one at a time) → DD_a\n",
    "base_vars = available_esg_raw + ['size_dummy', 'year_dummy'] if available_esg_raw else ['size_dummy', 'year_dummy']\n",
    "\n",
    "for control in available_controls:\n",
    "    X_vars = base_vars + [control]\n",
    "    stats = run_ols_regression('DD_a', X_vars, df, f'ESG + Dummies + {control} → DD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Full model with all controls → DD_a\n",
    "if available_controls:\n",
    "    full_vars = base_vars + available_controls\n",
    "    stats = run_ols_regression('DD_a', full_vars, df, 'Full Model → DD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Regressions with PD_a (Accounting PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat sequence for PD_a\n",
    "if available_esg_raw:\n",
    "    stats = run_ols_regression('PD_a', available_esg_raw, df, 'ESG Raw → PD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "if available_esg_combined:\n",
    "    stats = run_ols_regression('PD_a', [available_esg_combined], df, 'ESG Combined → PD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "stats = run_ols_regression('PD_a', ['size_dummy', 'year_dummy'], df, 'Dummies → PD_a')\n",
    "if stats:\n",
    "    regression_results.append(stats)\n",
    "\n",
    "for control in available_controls:\n",
    "    X_vars = base_vars + [control]\n",
    "    stats = run_ols_regression('PD_a', X_vars, df, f'ESG + Dummies + {control} → PD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "if available_controls:\n",
    "    full_vars = base_vars + available_controls\n",
    "    stats = run_ols_regression('PD_a', full_vars, df, 'Full Model → PD_a')\n",
    "    if stats:\n",
    "        regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Regressions with DD_m (Market DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check market ESG columns\n",
    "esg_raw_cols_m = ['m_environmental_pillar_score', 'm_social_pillar_score', 'm_governance_pillar_score']\n",
    "esg_combined_col_m = 'm_esg_combined_score'\n",
    "control_cols_m = ['m_lnta', 'm_td/ta', 'm_price_to_book_value_per_share', 'm_capital_adequacy_total_(%)']\n",
    "\n",
    "available_esg_raw_m = [col for col in esg_raw_cols_m if col in df.columns]\n",
    "available_esg_combined_m = esg_combined_col_m if esg_combined_col_m in df.columns else None\n",
    "available_controls_m = [col for col in control_cols_m if col in df.columns]\n",
    "\n",
    "# Repeat sequence for DD_m\n",
    "if available_esg_raw_m:\n",
    "    stats = run_ols_regression('DD_m', available_esg_raw_m, df, 'ESG Raw → DD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "if available_esg_combined_m:\n",
    "    stats = run_ols_regression('DD_m', [available_esg_combined_m], df, 'ESG Combined → DD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "stats = run_ols_regression('DD_m', ['size_dummy', 'year_dummy'], df, 'Dummies → DD_m')\n",
    "if stats:\n",
    "    regression_results.append(stats)\n",
    "\n",
    "base_vars_m = available_esg_raw_m + ['size_dummy', 'year_dummy'] if available_esg_raw_m else ['size_dummy', 'year_dummy']\n",
    "\n",
    "for control in available_controls_m:\n",
    "    X_vars = base_vars_m + [control]\n",
    "    stats = run_ols_regression('DD_m', X_vars, df, f'ESG + Dummies + {control} → DD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "if available_controls_m:\n",
    "    full_vars_m = base_vars_m + available_controls_m\n",
    "    stats = run_ols_regression('DD_m', full_vars_m, df, 'Full Model → DD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Regressions with PD_m (Market PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat sequence for PD_m\n",
    "if available_esg_raw_m:\n",
    "    stats = run_ols_regression('PD_m', available_esg_raw_m, df, 'ESG Raw → PD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "if available_esg_combined_m:\n",
    "    stats = run_ols_regression('PD_m', [available_esg_combined_m], df, 'ESG Combined → PD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "stats = run_ols_regression('PD_m', ['size_dummy', 'year_dummy'], df, 'Dummies → PD_m')\n",
    "if stats:\n",
    "    regression_results.append(stats)\n",
    "\n",
    "for control in available_controls_m:\n",
    "    X_vars = base_vars_m + [control]\n",
    "    stats = run_ols_regression('PD_m', X_vars, df, f'ESG + Dummies + {control} → PD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)\n",
    "\n",
    "if available_controls_m:\n",
    "    full_vars_m = base_vars_m + available_controls_m\n",
    "    stats = run_ols_regression('PD_m', full_vars_m, df, 'Full Model → PD_m')\n",
    "    if stats:\n",
    "        regression_results.append(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Summary of Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "if regression_results:\n",
    "    results_df = pd.DataFrame(regression_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"REGRESSION SUMMARY TABLE\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    display(results_df[['model', 'dependent_var', 'n_obs', 'r_squared', 'adj_r_squared', 'f_statistic', 'prob_f']])\n",
    "    \n",
    "    # Save to CSV\n",
    "    timestamp = get_timestamp_cdt()\n",
    "    archive_old_files(analysis_dir, archive_dir, 'regression_summary', max_keep=5)\n",
    "    summary_file = analysis_dir / f'regression_summary_{timestamp}.csv'\n",
    "    results_df.to_csv(summary_file, index=False)\n",
    "    print(f\"\\n[SAVED] Regression summary: {summary_file.name}\")\n",
    "    \n",
    "    # Visualize R-squared comparison\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    x_pos = np.arange(len(results_df))\n",
    "    ax.bar(x_pos, results_df['r_squared'], alpha=0.7, label='R²')\n",
    "    ax.bar(x_pos, results_df['adj_r_squared'], alpha=0.7, label='Adj. R²')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('R-squared')\n",
    "    ax.set_title('Model Comparison: R-squared and Adjusted R-squared')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(results_df['model'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No regression results to summarize\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
