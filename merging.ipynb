{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Accounting and Market DD/PD Datasets\n",
    "\n",
    "This notebook merges the accounting-based and market-based distance-to-default (DD) and probability-of-default (PD) datasets into a single combined dataset.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load the latest accounting and market datasets from `data/outputs/datasheet/`\n",
    "2. Merge on `['instrument', 'year']`\n",
    "3. Apply clear labeling to distinguish accounting vs market variables\n",
    "4. Save timestamped merged dataset with archiving (max 5 archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank\n",
      "Output directory: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet\n",
      "Archive directory: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/archive/datasets\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Find repository root\n",
    "def find_repo_root(start: Path, marker: str = '.git') -> Path:\n",
    "    current = start.resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "output_dir = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "archive_dir = base_dir / 'archive' / 'datasets'\n",
    "\n",
    "print(f\"Repository root: {base_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Archive directory: {archive_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    # Move all existing files to archive\n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] Moved to archive: {os.path.basename(old_file)}\")\n",
    "    \n",
    "    # Clean up archive to keep only max_keep files\n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] Removed old archive: {os.path.basename(old_archive)}\")\n",
    "\n",
    "def get_latest_file(output_dir, dataset_type):\n",
    "    \"\"\"Get the most recent file of given dataset_type\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {dataset_type} files found in {output_dir}\")\n",
    "    return files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading accounting data from: accounting_20251011_042604.csv\n",
      "Loading market data from: market_20251011_042629.csv\n",
      "\n",
      "Accounting dataset: 1431 rows\n",
      "Market dataset: 1305 rows\n"
     ]
    }
   ],
   "source": [
    "# Load latest accounting and market datasets\n",
    "accounting_file = get_latest_file(output_dir, 'accounting')\n",
    "market_file = get_latest_file(output_dir, 'market')\n",
    "\n",
    "print(f\"Loading accounting data from: {os.path.basename(accounting_file)}\")\n",
    "print(f\"Loading market data from: {os.path.basename(market_file)}\")\n",
    "\n",
    "df_accounting = pd.read_csv(accounting_file)\n",
    "df_market = pd.read_csv(market_file)\n",
    "\n",
    "print(f\"\\nAccounting dataset: {len(df_accounting)} rows\")\n",
    "print(f\"Market dataset: {len(df_market)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset: 1551 rows\n",
      "\n",
      "Column count:\n",
      "  Accounting: 16\n",
      "  Market: 60\n",
      "  Merged: 74\n",
      "\n",
      "Sample merged data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instrument</th>\n",
       "      <th>year</th>\n",
       "      <th>DD_a</th>\n",
       "      <th>PD_a</th>\n",
       "      <th>DD_m</th>\n",
       "      <th>PD_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2016</td>\n",
       "      <td>16.169315</td>\n",
       "      <td>4.150843e-59</td>\n",
       "      <td>10.347490</td>\n",
       "      <td>2.147999e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2017</td>\n",
       "      <td>17.455646</td>\n",
       "      <td>1.559156e-68</td>\n",
       "      <td>10.495263</td>\n",
       "      <td>4.541225e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2018</td>\n",
       "      <td>16.682782</td>\n",
       "      <td>8.744138e-63</td>\n",
       "      <td>10.192319</td>\n",
       "      <td>1.072919e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2019</td>\n",
       "      <td>7.407534</td>\n",
       "      <td>6.433465e-14</td>\n",
       "      <td>5.073253</td>\n",
       "      <td>1.955359e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2020</td>\n",
       "      <td>11.296038</td>\n",
       "      <td>6.863643e-30</td>\n",
       "      <td>6.419505</td>\n",
       "      <td>6.835930e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2021</td>\n",
       "      <td>7.544877</td>\n",
       "      <td>2.263566e-14</td>\n",
       "      <td>4.520901</td>\n",
       "      <td>3.078854e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2022</td>\n",
       "      <td>7.175878</td>\n",
       "      <td>3.592236e-13</td>\n",
       "      <td>3.844074</td>\n",
       "      <td>6.050423e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2023</td>\n",
       "      <td>9.613568</td>\n",
       "      <td>3.503867e-22</td>\n",
       "      <td>5.679425</td>\n",
       "      <td>6.757405e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ACNB</td>\n",
       "      <td>2019</td>\n",
       "      <td>17.035894</td>\n",
       "      <td>2.224464e-65</td>\n",
       "      <td>10.447757</td>\n",
       "      <td>7.501612e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACNB</td>\n",
       "      <td>2020</td>\n",
       "      <td>15.467251</td>\n",
       "      <td>2.885981e-54</td>\n",
       "      <td>8.615988</td>\n",
       "      <td>3.467036e-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instrument  year       DD_a          PD_a       DD_m          PD_m\n",
       "0       ABCB  2016  16.169315  4.150843e-59  10.347490  2.147999e-25\n",
       "1       ABCB  2017  17.455646  1.559156e-68  10.495263  4.541225e-26\n",
       "2       ABCB  2018  16.682782  8.744138e-63  10.192319  1.072919e-24\n",
       "3       ABCB  2019   7.407534  6.433465e-14   5.073253  1.955359e-07\n",
       "4       ABCB  2020  11.296038  6.863643e-30   6.419505  6.835930e-11\n",
       "5       ABCB  2021   7.544877  2.263566e-14   4.520901  3.078854e-06\n",
       "6       ABCB  2022   7.175878  3.592236e-13   3.844074  6.050423e-05\n",
       "7       ABCB  2023   9.613568  3.503867e-22   5.679425  6.757405e-09\n",
       "8       ACNB  2019  17.035894  2.224464e-65  10.447757  7.501612e-26\n",
       "9       ACNB  2020  15.467251  2.885981e-54   8.615988  3.467036e-18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge datasets on instrument and year\n",
    "merge_keys = ['instrument', 'year']\n",
    "\n",
    "# Add prefixes to distinguish variables (except merge keys and final DD/PD)\n",
    "accounting_cols_to_prefix = [c for c in df_accounting.columns \n",
    "                             if c not in merge_keys + ['DD_a', 'PD_a']]\n",
    "market_cols_to_prefix = [c for c in df_market.columns \n",
    "                         if c not in merge_keys + ['DD_m', 'PD_m']]\n",
    "\n",
    "df_accounting_prefixed = df_accounting.rename(\n",
    "    columns={c: f'a_{c}' for c in accounting_cols_to_prefix}\n",
    ")\n",
    "df_market_prefixed = df_market.rename(\n",
    "    columns={c: f'm_{c}' for c in market_cols_to_prefix}\n",
    ")\n",
    "\n",
    "# Perform outer merge to keep all observations\n",
    "df_merged = pd.merge(\n",
    "    df_accounting_prefixed,\n",
    "    df_market_prefixed,\n",
    "    on=merge_keys,\n",
    "    how='outer',\n",
    "    suffixes=('_a', '_m')\n",
    ")\n",
    "\n",
    "# Clean up: Remove any unnamed columns\n",
    "unnamed_cols = [col for col in df_merged.columns if col.startswith('Unnamed')]\n",
    "if unnamed_cols:\n",
    "    print(f'[WARN] Dropping {len(unnamed_cols)} unnamed columns: {unnamed_cols[:5]}...')\n",
    "    df_merged = df_merged.drop(columns=unnamed_cols)\n",
    "\n",
    "# Remove duplicate columns (keep first occurrence)\n",
    "df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]\n",
    "\n",
    "print(f\"Merged dataset: {len(df_merged)} rows\")\n",
    "print(f\"\\nColumn count:\")\n",
    "print(f\"  Accounting: {len(df_accounting.columns)}\")\n",
    "print(f\"  Market: {len(df_market.columns)}\")\n",
    "print(f\"  Merged: {len(df_merged.columns)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample merged data:\")\n",
    "display(df_merged[['instrument', 'year', 'DD_a', 'PD_a', 'DD_m', 'PD_m']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARCHIVE] Moved to archive: merged_20251011_042434.csv\n",
      "[CLEANUP] Removed old archive: merged_with_exclusions_20251008_013301.csv\n",
      "[INFO] Merged dataset saved to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/merged_20251011_043202.csv\n",
      "[INFO] Total rows: 1551\n",
      "[INFO] Total columns: 74\n"
     ]
    }
   ],
   "source": [
    "# Archive old merged files and save new one with timestamp\n",
    "archive_old_files(output_dir, archive_dir, 'merged', max_keep=5)\n",
    "\n",
    "timestamp = get_timestamp_cdt()\n",
    "merged_output = output_dir / f'merged_{timestamp}.csv'\n",
    "df_merged.to_csv(merged_output, index=False)\n",
    "\n",
    "print(f\"[INFO] Merged dataset saved to: {merged_output}\")\n",
    "print(f\"[INFO] Total rows: {len(df_merged)}\")\n",
    "print(f\"[INFO] Total columns: {len(df_merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating ESG dataset with DD/PD columns...\n",
      "  [WARN] Removing old DD/PD columns from ESG: ['DD_a', 'PD_a', 'DD_m', 'PD_m', 'status']\n",
      "  Loaded ESG data: 1424 rows, 32 columns\n",
      "  [INFO] Removed 127 duplicate DD/PD rows\n",
      "  Merged ESG+DD/PD: 1424 rows, 36 columns\n",
      "  New columns: DD_a, PD_a, DD_m, PD_m\n",
      "[ARCHIVE] Moved to archive: esg_dd_pd_20251011_042434.csv\n",
      "[CLEANUP] Removed old archive: esg_dd_pd_20251004_051328.csv\n",
      "\n",
      "[INFO] ESG+DD/PD dataset saved to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/esg_dd_pd_20251011_043202.csv\n",
      "[INFO] Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instrument</th>\n",
       "      <th>year</th>\n",
       "      <th>lnta</th>\n",
       "      <th>esg_score</th>\n",
       "      <th>DD_a</th>\n",
       "      <th>PD_a</th>\n",
       "      <th>DD_m</th>\n",
       "      <th>PD_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2016</td>\n",
       "      <td>14.728184</td>\n",
       "      <td>81.521252</td>\n",
       "      <td>11.422373</td>\n",
       "      <td>1.616415e-30</td>\n",
       "      <td>6.268524</td>\n",
       "      <td>1.822427e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2017</td>\n",
       "      <td>14.745152</td>\n",
       "      <td>82.353064</td>\n",
       "      <td>12.014904</td>\n",
       "      <td>1.483567e-33</td>\n",
       "      <td>6.208168</td>\n",
       "      <td>2.680295e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2018</td>\n",
       "      <td>14.779651</td>\n",
       "      <td>80.046519</td>\n",
       "      <td>10.890277</td>\n",
       "      <td>6.412505e-28</td>\n",
       "      <td>5.877820</td>\n",
       "      <td>2.078521e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2019</td>\n",
       "      <td>14.804077</td>\n",
       "      <td>83.907682</td>\n",
       "      <td>10.173661</td>\n",
       "      <td>1.299758e-24</td>\n",
       "      <td>6.445351</td>\n",
       "      <td>5.766643e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2020</td>\n",
       "      <td>15.034793</td>\n",
       "      <td>85.545384</td>\n",
       "      <td>12.565140</td>\n",
       "      <td>1.641474e-36</td>\n",
       "      <td>6.314757</td>\n",
       "      <td>1.352930e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2021</td>\n",
       "      <td>15.135549</td>\n",
       "      <td>82.867509</td>\n",
       "      <td>8.535697</td>\n",
       "      <td>6.965624e-18</td>\n",
       "      <td>4.699618</td>\n",
       "      <td>1.303246e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2022</td>\n",
       "      <td>15.114542</td>\n",
       "      <td>78.672800</td>\n",
       "      <td>9.835565</td>\n",
       "      <td>3.956130e-23</td>\n",
       "      <td>4.670777</td>\n",
       "      <td>1.500313e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2023</td>\n",
       "      <td>15.170158</td>\n",
       "      <td>79.512383</td>\n",
       "      <td>7.353921</td>\n",
       "      <td>9.623768e-14</td>\n",
       "      <td>4.130434</td>\n",
       "      <td>1.810397e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2016</td>\n",
       "      <td>14.598529</td>\n",
       "      <td>69.918468</td>\n",
       "      <td>9.078653</td>\n",
       "      <td>5.496324e-20</td>\n",
       "      <td>5.386901</td>\n",
       "      <td>3.584151e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2017</td>\n",
       "      <td>14.640227</td>\n",
       "      <td>75.384317</td>\n",
       "      <td>8.031137</td>\n",
       "      <td>4.828675e-16</td>\n",
       "      <td>4.205528</td>\n",
       "      <td>1.302365e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instrument  year       lnta  esg_score       DD_a          PD_a      DD_m  \\\n",
       "0        JPM  2016  14.728184  81.521252  11.422373  1.616415e-30  6.268524   \n",
       "1        JPM  2017  14.745152  82.353064  12.014904  1.483567e-33  6.208168   \n",
       "2        JPM  2018  14.779651  80.046519  10.890277  6.412505e-28  5.877820   \n",
       "3        JPM  2019  14.804077  83.907682  10.173661  1.299758e-24  6.445351   \n",
       "4        JPM  2020  15.034793  85.545384  12.565140  1.641474e-36  6.314757   \n",
       "5        JPM  2021  15.135549  82.867509   8.535697  6.965624e-18  4.699618   \n",
       "6        JPM  2022  15.114542  78.672800   9.835565  3.956130e-23  4.670777   \n",
       "7        JPM  2023  15.170158  79.512383   7.353921  9.623768e-14  4.130434   \n",
       "8        BAC  2016  14.598529  69.918468   9.078653  5.496324e-20  5.386901   \n",
       "9        BAC  2017  14.640227  75.384317   8.031137  4.828675e-16  4.205528   \n",
       "\n",
       "           PD_m  \n",
       "0  1.822427e-10  \n",
       "1  2.680295e-10  \n",
       "2  2.078521e-09  \n",
       "3  5.766643e-11  \n",
       "4  1.352930e-10  \n",
       "5  1.303246e-06  \n",
       "6  1.500313e-06  \n",
       "7  1.810397e-05  \n",
       "8  3.584151e-08  \n",
       "9  1.302365e-05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create ESG dataset with DD/PD columns appended\n",
    "print('[INFO] Creating ESG dataset with DD/PD columns...')\n",
    "\n",
    "# Load ESG data\n",
    "esg_file = base_dir / 'data' / 'outputs' / 'datasheet' / 'esg_0718.csv'\n",
    "if not esg_file.exists():\n",
    "    esg_file = base_dir / 'data' / 'esg_0718.csv'\n",
    "\n",
    "if not esg_file.exists():\n",
    "    print(f'[ERROR] ESG file not found: {esg_file}')\n",
    "else:\n",
    "    df_esg = pd.read_csv(esg_file)\n",
    "    \n",
    "    # CRITICAL: Remove old DD/PD columns from ESG file if they exist\n",
    "    old_cols = ['DD_a', 'PD_a', 'DD_m', 'PD_m', 'status']\n",
    "    old_cols_found = [col for col in old_cols if col in df_esg.columns]\n",
    "    if old_cols_found:\n",
    "        print(f'  [WARN] Removing old DD/PD columns from ESG: {old_cols_found}')\n",
    "        df_esg = df_esg.drop(columns=old_cols_found)\n",
    "    \n",
    "    # Clean up any unnamed columns\n",
    "    unnamed_cols = [col for col in df_esg.columns if col.startswith('Unnamed')]\n",
    "    if unnamed_cols:\n",
    "        print(f'  [WARN] Dropping {len(unnamed_cols)} unnamed columns from ESG')\n",
    "        df_esg = df_esg.drop(columns=unnamed_cols)\n",
    "    \n",
    "    print(f'  Loaded ESG data: {len(df_esg)} rows, {len(df_esg.columns)} columns')\n",
    "    \n",
    "    # Extract DD/PD columns from merged dataset\n",
    "    dd_pd_data = df_merged[['instrument', 'year', 'DD_a', 'PD_a', 'DD_m', 'PD_m']].copy()\n",
    "    \n",
    "    # Remove duplicates from DD/PD data (keep first occurrence)\n",
    "    before_dedup = len(dd_pd_data)\n",
    "    dd_pd_data = dd_pd_data.drop_duplicates(subset=['instrument', 'year'], keep='first')\n",
    "    after_dedup = len(dd_pd_data)\n",
    "    if before_dedup > after_dedup:\n",
    "        print(f'  [INFO] Removed {before_dedup - after_dedup} duplicate DD/PD rows')\n",
    "    \n",
    "    # Merge ESG data with DD/PD (now clean, no conflicts)\n",
    "    df_esg_dd = pd.merge(\n",
    "        df_esg,\n",
    "        dd_pd_data,\n",
    "        on=['instrument', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Final cleanup: remove any unnamed columns\n",
    "    unnamed_cols = [col for col in df_esg_dd.columns if col.startswith('Unnamed')]\n",
    "    if unnamed_cols:\n",
    "        print(f'  [WARN] Dropping {len(unnamed_cols)} unnamed columns after merge')\n",
    "        df_esg_dd = df_esg_dd.drop(columns=unnamed_cols)\n",
    "    \n",
    "    # Remove duplicate rows (keep first)\n",
    "    before_dedup = len(df_esg_dd)\n",
    "    df_esg_dd = df_esg_dd.drop_duplicates(subset=['instrument', 'year'], keep='first')\n",
    "    after_dedup = len(df_esg_dd)\n",
    "    if before_dedup > after_dedup:\n",
    "        print(f'  [INFO] Removed {before_dedup - after_dedup} duplicate rows from final dataset')\n",
    "    \n",
    "    print(f'  Merged ESG+DD/PD: {len(df_esg_dd)} rows, {len(df_esg_dd.columns)} columns')\n",
    "    print(f'  New columns: DD_a, PD_a, DD_m, PD_m')\n",
    "    \n",
    "    # Archive old ESG+DD files\n",
    "    archive_old_files(output_dir, archive_dir, 'esg_dd_pd', max_keep=5)\n",
    "    \n",
    "    # Save with timestamp\n",
    "    esg_output = output_dir / f'esg_dd_pd_{timestamp}.csv'\n",
    "    df_esg_dd.to_csv(esg_output, index=False)\n",
    "    \n",
    "    print(f'\\n[INFO] ESG+DD/PD dataset saved to: {esg_output}')\n",
    "    print(f'[INFO] Sample data:')\n",
    "    \n",
    "    # Display only columns that exist\n",
    "    display_cols = ['instrument', 'year']\n",
    "    for col in ['lnta', 'esg_score', 'DD_a', 'PD_a', 'DD_m', 'PD_m']:\n",
    "        if col in df_esg_dd.columns:\n",
    "            display_cols.append(col)\n",
    "    display(df_esg_dd[display_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESG+DD/PD DATASET SUMMARY ===\n",
      "\n",
      "Total observations: 1424\n",
      "Observations with DD_a: 1290\n",
      "Observations with DD_m: 1290\n",
      "Observations with both DD_a and DD_m: 1290\n",
      "\n",
      "Unique instruments: 244\n",
      "Year range: 2016 - 2023\n"
     ]
    }
   ],
   "source": [
    "# Summary of ESG+DD/PD dataset\n",
    "if 'df_esg_dd' in locals():\n",
    "    print('=== ESG+DD/PD DATASET SUMMARY ===')\n",
    "    print(f'\\nTotal observations: {len(df_esg_dd)}')\n",
    "    print(f'Observations with DD_a: {df_esg_dd[\"DD_a\"].notna().sum()}')\n",
    "    print(f'Observations with DD_m: {df_esg_dd[\"DD_m\"].notna().sum()}')\n",
    "    print(f'Observations with both DD_a and DD_m: {df_esg_dd[[\"DD_a\", \"DD_m\"]].dropna().shape[0]}')\n",
    "    print(f'\\nUnique instruments: {df_esg_dd[\"instrument\"].nunique()}')\n",
    "    print(f'Year range: {df_esg_dd[\"year\"].min()} - {df_esg_dd[\"year\"].max()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MERGED DATASET SUMMARY ===\n",
      "\n",
      "Observations with both DD_a and DD_m: 1417\n",
      "Observations with only DD_a: 0\n",
      "Observations with only DD_m: 0\n",
      "\n",
      "DD_a statistics:\n",
      "count    1417.000000\n",
      "mean       13.083009\n",
      "std         5.635417\n",
      "min        -6.042400\n",
      "25%         9.793822\n",
      "50%        11.768417\n",
      "75%        15.365677\n",
      "max        59.282332\n",
      "Name: DD_a, dtype: float64\n",
      "\n",
      "DD_m statistics:\n",
      "count    1417.000000\n",
      "mean        7.759768\n",
      "std         3.840419\n",
      "min         0.899636\n",
      "25%         5.514719\n",
      "50%         7.139359\n",
      "75%         8.964577\n",
      "max        35.000000\n",
      "Name: DD_m, dtype: float64\n",
      "\n",
      "PD_a statistics:\n",
      "count    1.417000e+03\n",
      "mean     8.102017e-04\n",
      "std      2.665497e-02\n",
      "min      0.000000e+00\n",
      "25%      1.390587e-53\n",
      "50%      2.839007e-32\n",
      "75%      5.984201e-23\n",
      "max      1.000000e+00\n",
      "Name: PD_a, dtype: float64\n",
      "\n",
      "PD_m statistics:\n",
      "count     1.417000e+03\n",
      "mean      4.708101e-04\n",
      "std       7.880232e-03\n",
      "min      1.124911e-268\n",
      "25%       1.557376e-19\n",
      "50%       4.688373e-13\n",
      "75%       1.746687e-08\n",
      "max       1.841571e-01\n",
      "Name: PD_m, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"=== MERGED DATASET SUMMARY ===\")\n",
    "print(f\"\\nObservations with both DD_a and DD_m: {df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_a: {df_merged['DD_a'].notna().sum() - df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_m: {df_merged['DD_m'].notna().sum() - df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "\n",
    "print(f\"\\nDD_a statistics:\")\n",
    "print(df_merged['DD_a'].describe())\n",
    "\n",
    "print(f\"\\nDD_m statistics:\")\n",
    "print(df_merged['DD_m'].describe())\n",
    "\n",
    "print(f\"\\nPD_a statistics:\")\n",
    "print(df_merged['PD_a'].describe())\n",
    "\n",
    "print(f\"\\nPD_m statistics:\")\n",
    "print(df_merged['PD_m'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
