{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Accounting and Market DD/PD Datasets\n",
    "\n",
    "This notebook merges the accounting-based and market-based distance-to-default (DD) and probability-of-default (PD) datasets into a single combined dataset.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load the latest accounting and market datasets from `data/outputs/datasheet/`\n",
    "2. Merge on `['instrument', 'year']`\n",
    "3. Apply clear labeling to distinguish accounting vs market variables\n",
    "4. Save timestamped merged dataset with archiving (max 5 archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Find repository root\n",
    "def find_repo_root(start: Path, marker: str = '.git') -> Path:\n",
    "    current = start.resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "output_dir = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "archive_dir = base_dir / 'archive' / 'datasets'\n",
    "\n",
    "print(f\"Repository root: {base_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Archive directory: {archive_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    # Move all existing files to archive\n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] Moved to archive: {os.path.basename(old_file)}\")\n",
    "    \n",
    "    # Clean up archive to keep only max_keep files\n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] Removed old archive: {os.path.basename(old_archive)}\")\n",
    "\n",
    "def get_latest_file(output_dir, dataset_type):\n",
    "    \"\"\"Get the most recent file of given dataset_type\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {dataset_type} files found in {output_dir}\")\n",
    "    return files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest accounting and market datasets\n",
    "accounting_file = get_latest_file(output_dir, 'accounting')\n",
    "market_file = get_latest_file(output_dir, 'market')\n",
    "\n",
    "print(f\"Loading accounting data from: {os.path.basename(accounting_file)}\")\n",
    "print(f\"Loading market data from: {os.path.basename(market_file)}\")\n",
    "\n",
    "df_accounting = pd.read_csv(accounting_file)\n",
    "df_market = pd.read_csv(market_file)\n",
    "\n",
    "print(f\"\\nAccounting dataset: {len(df_accounting)} rows\")\n",
    "print(f\"Market dataset: {len(df_market)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on instrument and year\n",
    "merge_keys = ['instrument', 'year']\n",
    "\n",
    "# Add prefixes to distinguish variables (except merge keys and final DD/PD)\n",
    "accounting_cols_to_prefix = [c for c in df_accounting.columns \n",
    "                             if c not in merge_keys + ['DD_a', 'PD_a']]\n",
    "market_cols_to_prefix = [c for c in df_market.columns \n",
    "                         if c not in merge_keys + ['DD_m', 'PD_m']]\n",
    "\n",
    "df_accounting_prefixed = df_accounting.rename(\n",
    "    columns={c: f'a_{c}' for c in accounting_cols_to_prefix}\n",
    ")\n",
    "df_market_prefixed = df_market.rename(\n",
    "    columns={c: f'm_{c}' for c in market_cols_to_prefix}\n",
    ")\n",
    "\n",
    "# Perform outer merge to keep all observations\n",
    "df_merged = pd.merge(\n",
    "    df_accounting_prefixed,\n",
    "    df_market_prefixed,\n",
    "    on=merge_keys,\n",
    "    how='outer',\n",
    "    suffixes=('_a', '_m')\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset: {len(df_merged)} rows\")\n",
    "print(f\"\\nColumn count:\")\n",
    "print(f\"  Accounting: {len(df_accounting.columns)}\")\n",
    "print(f\"  Market: {len(df_market.columns)}\")\n",
    "print(f\"  Merged: {len(df_merged.columns)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample merged data:\")\n",
    "display(df_merged[['instrument', 'year', 'DD_a', 'PD_a', 'DD_m', 'PD_m']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive old merged files and save new one with timestamp\n",
    "archive_old_files(output_dir, archive_dir, 'merged', max_keep=5)\n",
    "\n",
    "timestamp = get_timestamp_cdt()\n",
    "merged_output = output_dir / f'merged_{timestamp}.csv'\n",
    "df_merged.to_csv(merged_output, index=False)\n",
    "\n",
    "print(f\"[INFO] Merged dataset saved to: {merged_output}\")\n",
    "print(f\"[INFO] Total rows: {len(df_merged)}\")\n",
    "print(f\"[INFO] Total columns: {len(df_merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== MERGED DATASET SUMMARY ===\")\n",
    "print(f\"\\nObservations with both DD_a and DD_m: {df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_a: {df_merged['DD_a'].notna().sum() - df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_m: {df_merged['DD_m'].notna().sum() - df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "\n",
    "print(f\"\\nDD_a statistics:\")\n",
    "print(df_merged['DD_a'].describe())\n",
    "\n",
    "print(f\"\\nDD_m statistics:\")\n",
    "print(df_merged['DD_m'].describe())\n",
    "\n",
    "print(f\"\\nPD_a statistics:\")\n",
    "print(df_merged['PD_a'].describe())\n",
    "\n",
    "print(f\"\\nPD_m statistics:\")\n",
    "print(df_merged['PD_m'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
