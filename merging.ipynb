{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Accounting and Market DD/PD Datasets\n",
    "\n",
    "This notebook merges the accounting-based and market-based distance-to-default (DD) and probability-of-default (PD) datasets into a single combined dataset.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load the latest accounting and market datasets from `data/outputs/datasheet/`\n",
    "2. Merge on `['instrument', 'year']`\n",
    "3. Apply clear labeling to distinguish accounting vs market variables\n",
    "4. Save timestamped merged dataset with archiving (max 5 archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank\n",
      "Output directory: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet\n",
      "Archive directory: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/archive/datasets\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Find repository root\n",
    "def find_repo_root(start: Path, marker: str = '.git') -> Path:\n",
    "    current = start.resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "output_dir = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "archive_dir = base_dir / 'archive' / 'datasets'\n",
    "\n",
    "print(f\"Repository root: {base_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Archive directory: {archive_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    # Move all existing files to archive\n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] Moved to archive: {os.path.basename(old_file)}\")\n",
    "    \n",
    "    # Clean up archive to keep only max_keep files\n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] Removed old archive: {os.path.basename(old_archive)}\")\n",
    "\n",
    "def get_latest_file(output_dir, dataset_type):\n",
    "    \"\"\"Get the most recent file of given dataset_type\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {dataset_type} files found in {output_dir}\")\n",
    "    return files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading accounting data from: accounting_20251014_022117.csv\n",
      "Loading market data from: market_20251014_022125.csv\n",
      "\n",
      "Accounting dataset: 1366 rows\n",
      "Market dataset: 1344 rows\n"
     ]
    }
   ],
   "source": [
    "# Load latest accounting and market datasets\n",
    "accounting_file = get_latest_file(output_dir, 'accounting')\n",
    "market_file = get_latest_file(output_dir, 'market')\n",
    "\n",
    "print(f\"Loading accounting data from: {os.path.basename(accounting_file)}\")\n",
    "print(f\"Loading market data from: {os.path.basename(market_file)}\")\n",
    "\n",
    "df_accounting = pd.read_csv(accounting_file)\n",
    "df_market = pd.read_csv(market_file)\n",
    "\n",
    "print(f\"\\nAccounting dataset: {len(df_accounting)} rows\")\n",
    "print(f\"Market dataset: {len(df_market)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset: 1390 rows\n",
      "\n",
      "Column count:\n",
      "  Accounting: 16\n",
      "  Market: 60\n",
      "  Merged: 74\n",
      "\n",
      "Sample merged data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instrument</th>\n",
       "      <th>year</th>\n",
       "      <th>DD_a</th>\n",
       "      <th>PD_a</th>\n",
       "      <th>DD_m</th>\n",
       "      <th>PD_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2016</td>\n",
       "      <td>13.743862</td>\n",
       "      <td>2.771898e-43</td>\n",
       "      <td>8.770464</td>\n",
       "      <td>8.896580e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2017</td>\n",
       "      <td>16.600072</td>\n",
       "      <td>3.480372e-62</td>\n",
       "      <td>9.974939</td>\n",
       "      <td>9.811087e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2018</td>\n",
       "      <td>15.685909</td>\n",
       "      <td>9.442022e-56</td>\n",
       "      <td>9.576303</td>\n",
       "      <td>5.029064e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2019</td>\n",
       "      <td>8.181287</td>\n",
       "      <td>1.404143e-16</td>\n",
       "      <td>5.610304</td>\n",
       "      <td>1.009855e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2020</td>\n",
       "      <td>14.341664</td>\n",
       "      <td>6.006968e-47</td>\n",
       "      <td>8.180068</td>\n",
       "      <td>1.418415e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2021</td>\n",
       "      <td>4.142793</td>\n",
       "      <td>1.715509e-05</td>\n",
       "      <td>2.420868</td>\n",
       "      <td>7.741753e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2022</td>\n",
       "      <td>8.344845</td>\n",
       "      <td>3.565382e-17</td>\n",
       "      <td>4.484592</td>\n",
       "      <td>3.652677e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>2023</td>\n",
       "      <td>11.779771</td>\n",
       "      <td>2.481398e-32</td>\n",
       "      <td>6.985270</td>\n",
       "      <td>1.421542e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ACNB</td>\n",
       "      <td>2019</td>\n",
       "      <td>11.230132</td>\n",
       "      <td>1.450220e-29</td>\n",
       "      <td>6.840633</td>\n",
       "      <td>3.942208e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACNB</td>\n",
       "      <td>2020</td>\n",
       "      <td>13.378153</td>\n",
       "      <td>4.056918e-41</td>\n",
       "      <td>7.435770</td>\n",
       "      <td>5.198017e-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instrument  year       DD_a          PD_a      DD_m          PD_m\n",
       "0       ABCB  2016  13.743862  2.771898e-43  8.770464  8.896580e-19\n",
       "1       ABCB  2017  16.600072  3.480372e-62  9.974939  9.811087e-24\n",
       "2       ABCB  2018  15.685909  9.442022e-56  9.576303  5.029064e-22\n",
       "3       ABCB  2019   8.181287  1.404143e-16  5.610304  1.009855e-08\n",
       "4       ABCB  2020  14.341664  6.006968e-47  8.180068  1.418415e-16\n",
       "5       ABCB  2021   4.142793  1.715509e-05  2.420868  7.741753e-03\n",
       "6       ABCB  2022   8.344845  3.565382e-17  4.484592  3.652677e-06\n",
       "7       ABCB  2023  11.779771  2.481398e-32  6.985270  1.421542e-12\n",
       "8       ACNB  2019  11.230132  1.450220e-29  6.840633  3.942208e-12\n",
       "9       ACNB  2020  13.378153  4.056918e-41  7.435770  5.198017e-14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge datasets on instrument and year\n",
    "merge_keys = ['instrument', 'year']\n",
    "\n",
    "# Add prefixes to distinguish variables (except merge keys and final DD/PD)\n",
    "accounting_cols_to_prefix = [c for c in df_accounting.columns \n",
    "                             if c not in merge_keys + ['DD_a', 'PD_a']]\n",
    "market_cols_to_prefix = [c for c in df_market.columns \n",
    "                         if c not in merge_keys + ['DD_m', 'PD_m']]\n",
    "\n",
    "df_accounting_prefixed = df_accounting.rename(\n",
    "    columns={c: f'a_{c}' for c in accounting_cols_to_prefix}\n",
    ")\n",
    "df_market_prefixed = df_market.rename(\n",
    "    columns={c: f'm_{c}' for c in market_cols_to_prefix}\n",
    ")\n",
    "\n",
    "# Perform outer merge to keep all observations\n",
    "df_merged = pd.merge(\n",
    "    df_accounting_prefixed,\n",
    "    df_market_prefixed,\n",
    "    on=merge_keys,\n",
    "    how='outer',\n",
    "    suffixes=('_a', '_m')\n",
    ")\n",
    "\n",
    "# Clean up: Remove any unnamed columns\n",
    "unnamed_cols = [col for col in df_merged.columns if col.startswith('Unnamed')]\n",
    "if unnamed_cols:\n",
    "    print(f'[WARN] Dropping {len(unnamed_cols)} unnamed columns: {unnamed_cols[:5]}...')\n",
    "    df_merged = df_merged.drop(columns=unnamed_cols)\n",
    "\n",
    "# Remove duplicate columns (keep first occurrence)\n",
    "df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]\n",
    "\n",
    "print(f\"Merged dataset: {len(df_merged)} rows\")\n",
    "print(f\"\\nColumn count:\")\n",
    "print(f\"  Accounting: {len(df_accounting.columns)}\")\n",
    "print(f\"  Market: {len(df_market.columns)}\")\n",
    "print(f\"  Merged: {len(df_merged.columns)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample merged data:\")\n",
    "display(df_merged[['instrument', 'year', 'DD_a', 'PD_a', 'DD_m', 'PD_m']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARCHIVE] Moved to archive: merged_20251011_043202.csv\n",
      "[CLEANUP] Removed old archive: merged_trimmed_20251008_013301.csv\n",
      "[INFO] Merged dataset saved to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/merged_20251014_022322.csv\n",
      "[INFO] Total rows: 1390\n",
      "[INFO] Total columns: 74\n"
     ]
    }
   ],
   "source": [
    "# Archive old merged files and save new one with timestamp\n",
    "archive_old_files(output_dir, archive_dir, 'merged', max_keep=5)\n",
    "\n",
    "timestamp = get_timestamp_cdt()\n",
    "merged_output = output_dir / f'merged_{timestamp}.csv'\n",
    "df_merged.to_csv(merged_output, index=False)\n",
    "\n",
    "print(f\"[INFO] Merged dataset saved to: {merged_output}\")\n",
    "print(f\"[INFO] Total rows: {len(df_merged)}\")\n",
    "print(f\"[INFO] Total columns: {len(df_merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating ESG dataset with DD/PD columns...\n",
      "  [WARN] Removing old DD/PD columns from ESG: ['DD_a', 'PD_a', 'DD_m', 'PD_m', 'status']\n",
      "  Loaded ESG data: 1424 rows, 32 columns\n",
      "  [INFO] Removed 31 duplicate DD/PD rows\n",
      "  Merged ESG+DD/PD: 1424 rows, 36 columns\n",
      "  New columns: DD_a, PD_a, DD_m, PD_m\n",
      "[ARCHIVE] Moved to archive: esg_dd_pd_20251011_043202.csv\n",
      "[CLEANUP] Removed old archive: esg_dd_pd_20251005_033622.csv\n",
      "\n",
      "[INFO] ESG+DD/PD dataset saved to: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/esg_dd_pd_20251014_022322.csv\n",
      "[INFO] Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instrument</th>\n",
       "      <th>year</th>\n",
       "      <th>lnta</th>\n",
       "      <th>esg_score</th>\n",
       "      <th>DD_a</th>\n",
       "      <th>PD_a</th>\n",
       "      <th>DD_m</th>\n",
       "      <th>PD_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2016</td>\n",
       "      <td>14.728184</td>\n",
       "      <td>81.521252</td>\n",
       "      <td>9.894375</td>\n",
       "      <td>2.201751e-23</td>\n",
       "      <td>5.401864</td>\n",
       "      <td>3.297592e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2017</td>\n",
       "      <td>14.745152</td>\n",
       "      <td>82.353064</td>\n",
       "      <td>9.852858</td>\n",
       "      <td>3.331100e-23</td>\n",
       "      <td>5.061931</td>\n",
       "      <td>2.075159e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2018</td>\n",
       "      <td>14.779651</td>\n",
       "      <td>80.046519</td>\n",
       "      <td>13.690534</td>\n",
       "      <td>5.782807e-43</td>\n",
       "      <td>7.462842</td>\n",
       "      <td>4.233791e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2019</td>\n",
       "      <td>14.804077</td>\n",
       "      <td>83.907682</td>\n",
       "      <td>9.282871</td>\n",
       "      <td>8.248642e-21</td>\n",
       "      <td>5.868066</td>\n",
       "      <td>2.204540e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2020</td>\n",
       "      <td>15.034793</td>\n",
       "      <td>85.545384</td>\n",
       "      <td>13.461467</td>\n",
       "      <td>1.318151e-41</td>\n",
       "      <td>6.778450</td>\n",
       "      <td>6.073599e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2021</td>\n",
       "      <td>15.135549</td>\n",
       "      <td>82.867509</td>\n",
       "      <td>4.273943</td>\n",
       "      <td>9.602325e-06</td>\n",
       "      <td>2.329472</td>\n",
       "      <td>9.917039e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2022</td>\n",
       "      <td>15.114542</td>\n",
       "      <td>78.672800</td>\n",
       "      <td>12.671562</td>\n",
       "      <td>4.250215e-37</td>\n",
       "      <td>6.051731</td>\n",
       "      <td>7.164898e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2023</td>\n",
       "      <td>15.170158</td>\n",
       "      <td>79.512383</td>\n",
       "      <td>7.746285</td>\n",
       "      <td>4.731000e-15</td>\n",
       "      <td>4.353941</td>\n",
       "      <td>6.685585e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2016</td>\n",
       "      <td>14.598529</td>\n",
       "      <td>69.918468</td>\n",
       "      <td>7.727500</td>\n",
       "      <td>5.483976e-15</td>\n",
       "      <td>4.557007</td>\n",
       "      <td>2.594390e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2017</td>\n",
       "      <td>14.640227</td>\n",
       "      <td>75.384317</td>\n",
       "      <td>7.150699</td>\n",
       "      <td>4.316846e-13</td>\n",
       "      <td>3.735720</td>\n",
       "      <td>9.358942e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instrument  year       lnta  esg_score       DD_a          PD_a      DD_m  \\\n",
       "0        JPM  2016  14.728184  81.521252   9.894375  2.201751e-23  5.401864   \n",
       "1        JPM  2017  14.745152  82.353064   9.852858  3.331100e-23  5.061931   \n",
       "2        JPM  2018  14.779651  80.046519  13.690534  5.782807e-43  7.462842   \n",
       "3        JPM  2019  14.804077  83.907682   9.282871  8.248642e-21  5.868066   \n",
       "4        JPM  2020  15.034793  85.545384  13.461467  1.318151e-41  6.778450   \n",
       "5        JPM  2021  15.135549  82.867509   4.273943  9.602325e-06  2.329472   \n",
       "6        JPM  2022  15.114542  78.672800  12.671562  4.250215e-37  6.051731   \n",
       "7        JPM  2023  15.170158  79.512383   7.746285  4.731000e-15  4.353941   \n",
       "8        BAC  2016  14.598529  69.918468   7.727500  5.483976e-15  4.557007   \n",
       "9        BAC  2017  14.640227  75.384317   7.150699  4.316846e-13  3.735720   \n",
       "\n",
       "           PD_m  \n",
       "0  3.297592e-08  \n",
       "1  2.075159e-07  \n",
       "2  4.233791e-14  \n",
       "3  2.204540e-09  \n",
       "4  6.073599e-12  \n",
       "5  9.917039e-03  \n",
       "6  7.164898e-10  \n",
       "7  6.685585e-06  \n",
       "8  2.594390e-06  \n",
       "9  9.358942e-05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create ESG dataset with DD/PD columns appended\n",
    "print('[INFO] Creating ESG dataset with DD/PD columns...')\n",
    "\n",
    "# Load ESG data\n",
    "esg_file = base_dir / 'data' / 'outputs' / 'datasheet' / 'esg_0718.csv'\n",
    "if not esg_file.exists():\n",
    "    esg_file = base_dir / 'data' / 'esg_0718.csv'\n",
    "\n",
    "if not esg_file.exists():\n",
    "    print(f'[ERROR] ESG file not found: {esg_file}')\n",
    "else:\n",
    "    df_esg = pd.read_csv(esg_file)\n",
    "    \n",
    "    # CRITICAL: Remove old DD/PD columns from ESG file if they exist\n",
    "    old_cols = ['DD_a', 'PD_a', 'DD_m', 'PD_m', 'status']\n",
    "    old_cols_found = [col for col in old_cols if col in df_esg.columns]\n",
    "    if old_cols_found:\n",
    "        print(f'  [WARN] Removing old DD/PD columns from ESG: {old_cols_found}')\n",
    "        df_esg = df_esg.drop(columns=old_cols_found)\n",
    "    \n",
    "    # Clean up any unnamed columns\n",
    "    unnamed_cols = [col for col in df_esg.columns if col.startswith('Unnamed')]\n",
    "    if unnamed_cols:\n",
    "        print(f'  [WARN] Dropping {len(unnamed_cols)} unnamed columns from ESG')\n",
    "        df_esg = df_esg.drop(columns=unnamed_cols)\n",
    "    \n",
    "    print(f'  Loaded ESG data: {len(df_esg)} rows, {len(df_esg.columns)} columns')\n",
    "    \n",
    "    # Extract DD/PD columns from merged dataset\n",
    "    dd_pd_data = df_merged[['instrument', 'year', 'DD_a', 'PD_a', 'DD_m', 'PD_m']].copy()\n",
    "    \n",
    "    # Remove duplicates from DD/PD data (keep first occurrence)\n",
    "    before_dedup = len(dd_pd_data)\n",
    "    dd_pd_data = dd_pd_data.drop_duplicates(subset=['instrument', 'year'], keep='first')\n",
    "    after_dedup = len(dd_pd_data)\n",
    "    if before_dedup > after_dedup:\n",
    "        print(f'  [INFO] Removed {before_dedup - after_dedup} duplicate DD/PD rows')\n",
    "    \n",
    "    # Merge ESG data with DD/PD (now clean, no conflicts)\n",
    "    df_esg_dd = pd.merge(\n",
    "        df_esg,\n",
    "        dd_pd_data,\n",
    "        on=['instrument', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Final cleanup: remove any unnamed columns\n",
    "    unnamed_cols = [col for col in df_esg_dd.columns if col.startswith('Unnamed')]\n",
    "    if unnamed_cols:\n",
    "        print(f'  [WARN] Dropping {len(unnamed_cols)} unnamed columns after merge')\n",
    "        df_esg_dd = df_esg_dd.drop(columns=unnamed_cols)\n",
    "    \n",
    "    # Remove duplicate rows (keep first)\n",
    "    before_dedup = len(df_esg_dd)\n",
    "    df_esg_dd = df_esg_dd.drop_duplicates(subset=['instrument', 'year'], keep='first')\n",
    "    after_dedup = len(df_esg_dd)\n",
    "    if before_dedup > after_dedup:\n",
    "        print(f'  [INFO] Removed {before_dedup - after_dedup} duplicate rows from final dataset')\n",
    "    \n",
    "    print(f'  Merged ESG+DD/PD: {len(df_esg_dd)} rows, {len(df_esg_dd.columns)} columns')\n",
    "    print(f'  New columns: DD_a, PD_a, DD_m, PD_m')\n",
    "    \n",
    "    # Archive old ESG+DD files\n",
    "    archive_old_files(output_dir, archive_dir, 'esg_dd_pd', max_keep=5)\n",
    "    \n",
    "    # Save with timestamp\n",
    "    esg_output = output_dir / f'esg_dd_pd_{timestamp}.csv'\n",
    "    df_esg_dd.to_csv(esg_output, index=False)\n",
    "    \n",
    "    print(f'\\n[INFO] ESG+DD/PD dataset saved to: {esg_output}')\n",
    "    print(f'[INFO] Sample data:')\n",
    "    \n",
    "    # Display only columns that exist\n",
    "    display_cols = ['instrument', 'year']\n",
    "    for col in ['lnta', 'esg_score', 'DD_a', 'PD_a', 'DD_m', 'PD_m']:\n",
    "        if col in df_esg_dd.columns:\n",
    "            display_cols.append(col)\n",
    "    display(df_esg_dd[display_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESG+DD/PD DATASET SUMMARY ===\n",
      "\n",
      "Total observations: 1424\n",
      "Observations with DD_a: 1343\n",
      "Observations with DD_m: 1341\n",
      "Observations with both DD_a and DD_m: 1341\n",
      "\n",
      "Unique instruments: 244\n",
      "Year range: 2016 - 2023\n"
     ]
    }
   ],
   "source": [
    "# Summary of ESG+DD/PD dataset\n",
    "if 'df_esg_dd' in locals():\n",
    "    print('=== ESG+DD/PD DATASET SUMMARY ===')\n",
    "    print(f'\\nTotal observations: {len(df_esg_dd)}')\n",
    "    print(f'Observations with DD_a: {df_esg_dd[\"DD_a\"].notna().sum()}')\n",
    "    print(f'Observations with DD_m: {df_esg_dd[\"DD_m\"].notna().sum()}')\n",
    "    print(f'Observations with both DD_a and DD_m: {df_esg_dd[[\"DD_a\", \"DD_m\"]].dropna().shape[0]}')\n",
    "    print(f'\\nUnique instruments: {df_esg_dd[\"instrument\"].nunique()}')\n",
    "    print(f'Year range: {df_esg_dd[\"year\"].min()} - {df_esg_dd[\"year\"].max()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MERGED DATASET SUMMARY ===\n",
      "\n",
      "Observations with both DD_a and DD_m: 1372\n",
      "Observations with only DD_a: 2\n",
      "Observations with only DD_m: 0\n",
      "\n",
      "DD_a statistics:\n",
      "count    1374.000000\n",
      "mean       11.810329\n",
      "std         5.450671\n",
      "min        -5.719526\n",
      "25%         8.648352\n",
      "50%        11.306519\n",
      "75%        14.208120\n",
      "max        61.440271\n",
      "Name: DD_a, dtype: float64\n",
      "\n",
      "DD_m statistics:\n",
      "count    1372.000000\n",
      "mean        6.971720\n",
      "std         3.657484\n",
      "min         0.864582\n",
      "25%         4.906810\n",
      "50%         6.456122\n",
      "75%         8.124763\n",
      "max        35.000000\n",
      "Name: DD_m, dtype: float64\n",
      "\n",
      "PD_a statistics:\n",
      "count    1.374000e+03\n",
      "mean     8.359838e-04\n",
      "std      2.705217e-02\n",
      "min      0.000000e+00\n",
      "25%      4.085492e-46\n",
      "50%      6.113618e-30\n",
      "75%      2.615497e-18\n",
      "max      1.000000e+00\n",
      "Name: PD_a, dtype: float64\n",
      "\n",
      "PD_m statistics:\n",
      "count     1.372000e+03\n",
      "mean      1.471776e-03\n",
      "std       8.440291e-03\n",
      "min      1.124911e-268\n",
      "25%       2.241202e-16\n",
      "50%       5.375106e-11\n",
      "75%       4.628475e-07\n",
      "max       1.936342e-01\n",
      "Name: PD_m, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"=== MERGED DATASET SUMMARY ===\")\n",
    "print(f\"\\nObservations with both DD_a and DD_m: {df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_a: {df_merged['DD_a'].notna().sum() - df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "print(f\"Observations with only DD_m: {df_merged['DD_m'].notna().sum() - df_merged[['DD_a', 'DD_m']].dropna().shape[0]}\")\n",
    "\n",
    "print(f\"\\nDD_a statistics:\")\n",
    "print(df_merged['DD_a'].describe())\n",
    "\n",
    "print(f\"\\nDD_m statistics:\")\n",
    "print(df_merged['DD_m'].describe())\n",
    "\n",
    "print(f\"\\nPD_a statistics:\")\n",
    "print(df_merged['PD_a'].describe())\n",
    "\n",
    "print(f\"\\nPD_m statistics:\")\n",
    "print(df_merged['PD_m'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
