{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c99aa5",
   "metadata": {},
   "source": [
    "# DD and PD Calculation Using Accounting Data (Book Values)\n",
    "\n",
    "This notebook walks through the `dd_pd_accounting.py` script step by step.  \n",
    "We will:\n",
    "\n",
    "1.  Install dependencies  \n",
    "2.  Set up imports and file paths  \n",
    "3.  Load and inspect the accounting data  \n",
    "4.  Standardize column names, extract years  \n",
    "5.  Load and merge market cap and equity volatility  \n",
    "6.  Define the Accounting-Based Merton Solver\n",
    "7.  Run the Accounting-Based Solver and Compute DDa/PDa\n",
    "8.  Compute Distance to Default (DDa) and Probability of Default (PDa)  \n",
    "9.  Export results and append diagnostics to log  \n",
    "10. Summarize next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a5f5d",
   "metadata": {},
   "source": [
    "## 1.  Install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51155d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install needed packages (run once per environment)\n",
    "%pip install pandas numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e4966",
   "metadata": {},
   "source": [
    "## 2. Imports and File Paths\n",
    "\n",
    "Import libraries and define all paths relative to the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265bd56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace root: /Users/guillaumebld/Library/CloudStorage/OneDrive-LoyolaUniversityChicago/Group Quantitative Research Workspace – Summer 2025 - Results\n",
      "Accounting input     → FOUND\n",
      "Market cap input     → FOUND\n",
      "Equity vol input     → FOUND\n",
      "Risk-free input      → FOUND\n",
      "Account log file     → FOUND\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import root\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Helper function for standardizing tickers\n",
    "def standardize_ticker(t):\n",
    "    return str(t).split('.', 1)[0] if pd.notnull(t) else t\n",
    "\n",
    "# 1. Dynamically locate your workspace root\n",
    "base_dir = Path().resolve()\n",
    "print(\"Workspace root:\", base_dir)\n",
    "\n",
    "# 2. Time horizon for the Merton model (1 year)\n",
    "T = 1.0\n",
    "\n",
    "# 3. Define all file paths relative to base_dir\n",
    "model_fp      = base_dir / 'data' / 'clean' / 'Book2_clean.csv'\n",
    "marketcap_fp  = base_dir / 'data' / 'clean' / 'all_banks_marketcap_annual_2016_2023.csv'\n",
    "vol_fp        = base_dir / 'data' / 'clean' / 'equity_volatility_by_year.csv'\n",
    "rf_fp         = base_dir / 'data' / 'clean' / 'fama_french_factors_annual_clean.csv'\n",
    "log_fp        = base_dir / 'data' / 'logs' / 'dd_pd_accounting_log.txt'\n",
    "output_fp     = base_dir / 'data' / 'merged_inputs' / 'dd_pd_accounting.csv'\n",
    "\n",
    "# 4. Ensure directories exist\n",
    "for directory in (model_fp.parent, marketcap_fp.parent, vol_fp.parent, rf_fp.parent, log_fp.parent, output_fp.parent):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 5. Quick existence check for inputs and outputs\n",
    "for name, fp in [\n",
    "    ('Accounting input',    model_fp),\n",
    "    ('Market cap input',    marketcap_fp),\n",
    "    ('Equity vol input',    vol_fp),\n",
    "    ('Risk-free input',     rf_fp),\n",
    "    ('Account log file',    log_fp)\n",
    "]:\n",
    "    status = \"FOUND\" if fp.exists() else f\"MISSING ({fp.name})\"\n",
    "    print(f\"{name:20s} → {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f843706",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect Accounting Data\n",
    "\n",
    "- Read `Book2_clean.csv` which must contain `total_assets` and `debt_total`.  \n",
    "- Display row and unique `(instrument, year)` counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1062fa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading accounting data...\n",
      "→ 1425 rows, 1424 unique (instrument, year)\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Loading accounting data...')\n",
    "df = pd.read_csv(model_fp)\n",
    "print(f\"→ {df.shape[0]} rows, {df[['instrument','year']].drop_duplicates().shape[0]} unique (instrument, year)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849640b7",
   "metadata": {},
   "source": [
    "## 4. Standardize Column Names & Extract Year\n",
    "\n",
    "- Lowercase and replace spaces/dashes with underscores  \n",
    "- If `date` exists, parse it and extract year; otherwise ensure `year` is integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcc2287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instrument</th>\n",
       "      <th>year</th>\n",
       "      <th>rit_rf</th>\n",
       "      <th>rit</th>\n",
       "      <th>new_wacc</th>\n",
       "      <th>unnamed:_5</th>\n",
       "      <th>weighted_average_cost_of_capital,_(%)</th>\n",
       "      <th>beta_levered</th>\n",
       "      <th>beta_unlevered</th>\n",
       "      <th>environmental_pillar_score</th>\n",
       "      <th>...</th>\n",
       "      <th>wacc_tax_rate,_(%)</th>\n",
       "      <th>wacc_cost_of_debt,_(%)</th>\n",
       "      <th>wacc_debt_weight,_(%)</th>\n",
       "      <th>wacc_equity_weight,_(%)</th>\n",
       "      <th>total_assets</th>\n",
       "      <th>debt_total</th>\n",
       "      <th>d/e</th>\n",
       "      <th>dummylarge</th>\n",
       "      <th>dummymid</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.245917</td>\n",
       "      <td>0.247917</td>\n",
       "      <td>1.584416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.864093</td>\n",
       "      <td>1.536081</td>\n",
       "      <td>0.606612</td>\n",
       "      <td>81.766775</td>\n",
       "      <td>...</td>\n",
       "      <td>29.05903</td>\n",
       "      <td>2.126534</td>\n",
       "      <td>66.065751</td>\n",
       "      <td>30.587868</td>\n",
       "      <td>2490972.0</td>\n",
       "      <td>495354.0</td>\n",
       "      <td>2.159868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.223383</td>\n",
       "      <td>0.231383</td>\n",
       "      <td>1.657157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.749802</td>\n",
       "      <td>1.210898</td>\n",
       "      <td>0.579144</td>\n",
       "      <td>83.079770</td>\n",
       "      <td>...</td>\n",
       "      <td>28.38487</td>\n",
       "      <td>2.189746</td>\n",
       "      <td>58.567477</td>\n",
       "      <td>38.450277</td>\n",
       "      <td>2533600.0</td>\n",
       "      <td>494798.0</td>\n",
       "      <td>1.523200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2018</td>\n",
       "      <td>-0.227840</td>\n",
       "      <td>-0.209740</td>\n",
       "      <td>1.869560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.749097</td>\n",
       "      <td>1.141127</td>\n",
       "      <td>0.577758</td>\n",
       "      <td>79.426260</td>\n",
       "      <td>...</td>\n",
       "      <td>28.38487</td>\n",
       "      <td>2.730852</td>\n",
       "      <td>55.921812</td>\n",
       "      <td>41.071387</td>\n",
       "      <td>2622532.0</td>\n",
       "      <td>533627.0</td>\n",
       "      <td>1.361576</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.161746</td>\n",
       "      <td>0.183146</td>\n",
       "      <td>1.548871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.696227</td>\n",
       "      <td>1.223974</td>\n",
       "      <td>0.561719</td>\n",
       "      <td>89.750677</td>\n",
       "      <td>...</td>\n",
       "      <td>26.62674</td>\n",
       "      <td>2.017936</td>\n",
       "      <td>59.874307</td>\n",
       "      <td>37.262556</td>\n",
       "      <td>2687379.0</td>\n",
       "      <td>516093.0</td>\n",
       "      <td>1.606822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.128976</td>\n",
       "      <td>-0.124576</td>\n",
       "      <td>0.989988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.732273</td>\n",
       "      <td>1.217916</td>\n",
       "      <td>0.487232</td>\n",
       "      <td>90.723262</td>\n",
       "      <td>...</td>\n",
       "      <td>21.07742</td>\n",
       "      <td>1.306948</td>\n",
       "      <td>63.283852</td>\n",
       "      <td>33.304242</td>\n",
       "      <td>3384757.0</td>\n",
       "      <td>542102.0</td>\n",
       "      <td>1.900174</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  instrument  year    rit_rf       rit  new_wacc  unnamed:_5  \\\n",
       "0        JPM  2016  0.245917  0.247917  1.584416         NaN   \n",
       "1        JPM  2017  0.223383  0.231383  1.657157         NaN   \n",
       "2        JPM  2018 -0.227840 -0.209740  1.869560         NaN   \n",
       "3        JPM  2019  0.161746  0.183146  1.548871         NaN   \n",
       "4        JPM  2020 -0.128976 -0.124576  0.989988         NaN   \n",
       "\n",
       "   weighted_average_cost_of_capital,_(%)  beta_levered  beta_unlevered  \\\n",
       "0                               4.864093      1.536081        0.606612   \n",
       "1                               4.749802      1.210898        0.579144   \n",
       "2                               5.749097      1.141127        0.577758   \n",
       "3                               4.696227      1.223974        0.561719   \n",
       "4                               3.732273      1.217916        0.487232   \n",
       "\n",
       "   environmental_pillar_score  ...  wacc_tax_rate,_(%)  \\\n",
       "0                   81.766775  ...            29.05903   \n",
       "1                   83.079770  ...            28.38487   \n",
       "2                   79.426260  ...            28.38487   \n",
       "3                   89.750677  ...            26.62674   \n",
       "4                   90.723262  ...            21.07742   \n",
       "\n",
       "   wacc_cost_of_debt,_(%)  wacc_debt_weight,_(%)  wacc_equity_weight,_(%)  \\\n",
       "0                2.126534              66.065751                30.587868   \n",
       "1                2.189746              58.567477                38.450277   \n",
       "2                2.730852              55.921812                41.071387   \n",
       "3                2.017936              59.874307                37.262556   \n",
       "4                1.306948              63.283852                33.304242   \n",
       "\n",
       "   total_assets  debt_total       d/e  dummylarge  dummymid  Year  \n",
       "0     2490972.0    495354.0  2.159868         1.0       0.0  2016  \n",
       "1     2533600.0    494798.0  1.523200         1.0       0.0  2017  \n",
       "2     2622532.0    533627.0  1.361576         1.0       0.0  2018  \n",
       "3     2687379.0    516093.0  1.606822         1.0       0.0  2019  \n",
       "4     3384757.0    542102.0  1.900174         1.0       0.0  2020  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize column names\n",
    "col_map = {c: re.sub(r'_+', '_', c.strip().lower()\n",
    "                        .replace(' ', '_')\n",
    "                        .replace('-', '_'))\n",
    "           for c in df.columns}\n",
    "df = df.rename(columns=col_map)\n",
    "\n",
    "# Extract or clean 'year'\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['year'] = df['date'].dt.year\n",
    "else:\n",
    "    df = df[df['year'].notnull()].copy()\n",
    "    df['year'] = df['year'].astype(float).astype(int)\n",
    "\n",
    "df['Year'] = df['year']  # for merges down the line\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bfcb5",
   "metadata": {},
   "source": [
    "## 5. Load and Merge Market Cap & Equity Volatility\n",
    "\n",
    "- Load annual market cap, standardize tickers, merge on `(symbol, Year)`  \n",
    "- Load equity volatility, standardize, merge on `(ticker_prefix, Year)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d6025f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After prep, df columns: ['instrument', 'year', 'rit_rf', 'rit', 'new_wacc', 'unnamed:_5', 'weighted_average_cost_of_capital,_(%)', 'beta_levered', 'beta_unlevered', 'environmental_pillar_score', 'social_pillar_score', 'governance_pillar_score', 'esg_score', 'esg_combined_score', 'environmental_pillar_score_1', 'social_pillar_score_1', 'governance_pillar_score_1', 'esg_score_1', 'esg_combined_score_1', 'lnta', 'td/ta', 'price_to_book_value_per_share', 'capital_adequacy_total_(%)', 'wacc_tax_rate,_(%)', 'wacc_cost_of_debt,_(%)', 'wacc_debt_weight,_(%)', 'wacc_equity_weight,_(%)', 'total_assets', 'debt_total', 'd/e', 'dummylarge', 'dummymid', 'Year', 'ticker_prefix']\n",
      "Unique df keys:   ticker_prefix  Year\n",
      "0           JPM  2016\n",
      "1           JPM  2017\n",
      "2           JPM  2018\n",
      "3           JPM  2019\n",
      "4           JPM  2020\n",
      "mc columns: ['symbol', 'year', 'market_cap', 'ticker_prefix', 'Year']\n",
      "Sample mc keys:   ticker_prefix  Year    market_cap\n",
      "0          ABCB  2016  3.004515e+09\n",
      "1          ABCB  2017  3.321505e+09\n",
      "2          ABCB  2018  2.182408e+09\n",
      "3          ABCB  2019  2.931470e+09\n",
      "4          ABCB  2020  2.623438e+09\n",
      "ev columns: ['symbol', 'year', 'equity_volatility', 'equity_volatility_note', 'ticker_prefix', 'Year']\n",
      "Sample ev keys:   ticker_prefix  Year  equity_volatility\n",
      "0           SSB  2015                NaN\n",
      "1           SSB  2016           0.229291\n",
      "2           SSB  2017           0.145237\n",
      "3           SSB  2018           0.266805\n",
      "4           SSB  2019           0.272685\n",
      "After merges, sample df:\n",
      "  ticker_prefix  Year    market_cap  equity_vol\n",
      "0           JPM  2016  2.398077e+11    0.206735\n",
      "1           JPM  2017  2.971959e+11    0.169109\n",
      "2           JPM  2018  2.712948e+11    0.185040\n",
      "3           JPM  2019  3.874051e+11    0.227006\n",
      "4           JPM  2020  3.531390e+11    0.382475\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Prepare merge keys in df ---\n",
    "# 5.0 Ensure df has ticker_prefix & Year\n",
    "if 'ticker_prefix' not in df.columns:\n",
    "    df['ticker_prefix'] = df['instrument'].apply(standardize_ticker)\n",
    "if 'Year' not in df.columns:\n",
    "    # prefer existing year column\n",
    "    if 'year' in df.columns:\n",
    "        df['Year'] = df['year']\n",
    "    else:\n",
    "        df['Year'] = pd.to_datetime(df['date'], errors='coerce').dt.year\n",
    "\n",
    "# Quick debug\n",
    "print(\"After prep, df columns:\", df.columns.tolist())\n",
    "print(\"Unique df keys:\", df[['ticker_prefix','Year']].drop_duplicates().head())\n",
    "\n",
    "# --- 5.1 Load and merge market capitalization ---\n",
    "mc = pd.read_csv(marketcap_fp)\n",
    "\n",
    "# Compute market_cap if needed\n",
    "if 'market_cap' not in mc.columns:\n",
    "    mc['market_cap'] = mc['dec_price'] * mc['shares_outstanding'] / 1_000_000\n",
    "\n",
    "# Ensure mc has the merge keys\n",
    "if 'ticker_prefix' not in mc.columns:\n",
    "    mc['ticker_prefix'] = mc['symbol'].apply(standardize_ticker)\n",
    "if 'Year' not in mc.columns:\n",
    "    mc['Year'] = mc['year']\n",
    "\n",
    "# Debug\n",
    "print(\"mc columns:\", mc.columns.tolist())\n",
    "print(\"Sample mc keys:\", mc[['ticker_prefix','Year','market_cap']].head())\n",
    "\n",
    "# Merge\n",
    "df = df.merge(\n",
    "    mc[['ticker_prefix','Year','market_cap']],\n",
    "    on=['ticker_prefix','Year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 5.2 Load and merge equity volatility ---\n",
    "ev = pd.read_csv(vol_fp)\n",
    "\n",
    "# Determine ticker column in ev\n",
    "ticker_col = 'symbol' if 'symbol' in ev.columns else 'Bank'\n",
    "if 'ticker_prefix' not in ev.columns:\n",
    "    ev['ticker_prefix'] = ev[ticker_col].apply(standardize_ticker)\n",
    "if 'Year' not in ev.columns:\n",
    "    ev['Year'] = ev['year'] if 'year' in ev.columns else ev['Year']\n",
    "\n",
    "# Debug\n",
    "print(\"ev columns:\", ev.columns.tolist())\n",
    "print(\"Sample ev keys:\", ev[['ticker_prefix','Year','equity_volatility']].head())\n",
    "\n",
    "# Merge\n",
    "df = df.merge(\n",
    "    ev[['ticker_prefix','Year','equity_volatility']],\n",
    "    on=['ticker_prefix','Year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Final back-compatibility\n",
    "df['equity_vol'] = df['equity_volatility'].fillna(0.25)\n",
    "\n",
    "# Confirm merge\n",
    "print(\"After merges, sample df:\")\n",
    "print(df[['ticker_prefix','Year','market_cap','equity_vol']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef256acd",
   "metadata": {},
   "source": [
    "## 6. Define the Accounting-Based Merton Solver\n",
    "\n",
    "We treat **net equity** $E = A - F$ (assets minus debt) as a call option on the firm's assets. Given:\n",
    "\n",
    "- $A$: total assets (book value)  \n",
    "- $F$: total debt (book value)  \n",
    "- $\\sigma_E$: observed equity volatility  \n",
    "- $r_f$: risk-free rate  \n",
    "- $T$: time horizon (1 year)  \n",
    "\n",
    "we solve for:\n",
    "\n",
    "- $V$: total asset value  \n",
    "- $\\sigma_V$: asset volatility  \n",
    "\n",
    "by enforcing the two Merton equations:\n",
    "\n",
    "1. **Option-pricing relation**  \n",
    "   $$\n",
    "   V\\Phi(d_1) - Fe^{-r_fT}\\Phi(d_2) - (A - F) = 0\n",
    "   $$\n",
    "\n",
    "2. **Volatility link**  \n",
    "   $$\n",
    "   \\sigma_E - \\frac{V}{(A - F)}\\Phi(d_1)\\sigma_V = 0\n",
    "   $$\n",
    "\n",
    "with  \n",
    "$$\n",
    "d_1 = \\frac{\\ln(V/F) + (r_f + \\frac{1}{2}\\sigma_V^2)T}{\\sigma_V\\sqrt{T}}, \n",
    "\\quad\n",
    "d_2 = d_1 - \\sigma_V\\sqrt{T}\n",
    "$$\n",
    "\n",
    "We will use `scipy.optimize.root` to find $(V,\\sigma_V)$ that makes both expressions zero, similarly as the calculation using market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23556a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " JPM 2016: V = 2489755337209.08, σ_V = 0.1657, status = converged\n",
      "  instrument  year             V       σ_V     status tag\n",
      "0        JPM  2016  2.489755e+12  0.165704  converged    \n",
      "1        JPM  2017  2.532496e+12  0.136142  converged    \n",
      "2        JPM  2018  2.623749e+12  0.147320  converged    \n",
      "3        JPM  2019  2.686545e+12  0.183468  converged    \n",
      "4        JPM  2020  3.385457e+12  0.321152  converged    \n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.optimize import root\n",
    "\n",
    "def merton_solver_accounting(row, T=T):\n",
    "    \"\"\"\n",
    "    Solve for asset value V and volatility sigma_V using accounting data.\n",
    "    Returns (V, sigma_V, status_flag, tag).\n",
    "    \n",
    "     Convert accounting values from millions to actual USD for consistency.\n",
    "    \"\"\"\n",
    "    # 1. Extract inputs (convert millions to actual USD)\n",
    "    rf  = row['rit_rf'] / 100               # convert % to decimal, using rit_rf column\n",
    "    A   = row['total_assets'] * 1_000_000   # book assets in USD (was millions)\n",
    "    F   = row['debt_total'] * 1_000_000     # book debt in USD (was millions)\n",
    "    σ_E = row['equity_vol']                 # equity volatility\n",
    "    E   = A - F                             # net equity in USD\n",
    "\n",
    "    # 2. Validate inputs\n",
    "    if pd.isna(A) or pd.isna(F) or pd.isna(σ_E) or A <= 0 or σ_E <= 0 or F < 0:\n",
    "        tag = 'missing_or_invalid'\n",
    "        return np.nan, np.nan, 'invalid', tag\n",
    "    if F == 0:\n",
    "        return np.nan, np.nan, 'no_debt', 'no_debt'\n",
    "    if E <= 0:  # Net equity must be positive\n",
    "        return np.nan, np.nan, 'negative_equity', 'negative_equity'\n",
    "\n",
    "    # 3. Define the two Merton equations\n",
    "    def equations(x):\n",
    "        V, σ_V = x\n",
    "        if V <= 0 or σ_V <= 0:\n",
    "            return [1e6, 1e6]  # Large error for invalid values\n",
    "        d1 = (np.log(V/F) + (rf + 0.5*σ_V**2)*T) / (σ_V * np.sqrt(T))\n",
    "        d2 = d1 - σ_V * np.sqrt(T)\n",
    "        eq1 = V * norm.cdf(d1) - F * np.exp(-rf*T) * norm.cdf(d2) - E\n",
    "        eq2 = σ_E - (V/E) * norm.cdf(d1) * σ_V\n",
    "        return [eq1, eq2]\n",
    "\n",
    "    # 4. Initial guess and solve\n",
    "    try:\n",
    "        sol = root(equations, [A, σ_E], method='hybr')\n",
    "        if sol.success and sol.x[0] > 0 and sol.x[1] > 0:\n",
    "            V_opt, σ_V_opt = sol.x\n",
    "            return V_opt, σ_V_opt, 'converged', ''\n",
    "        else:\n",
    "            return np.nan, np.nan, 'no_converge', ''\n",
    "    except:\n",
    "        return np.nan, np.nan, 'error', 'solver_error'\n",
    "\n",
    "# Demo on the first row\n",
    "demo = df.iloc[0]\n",
    "V0, σ_V0, status0, tag0 = merton_solver_accounting(demo)\n",
    "print(f\" {demo['instrument']} {demo['year']}: V = {V0:.2f}, σ_V = {σ_V0:.4f}, status = {status0}\")\n",
    "\n",
    "\n",
    "# ---- Apply to all rows ----\n",
    "# Convert apply results to DataFrame with proper columns\n",
    "results = pd.DataFrame(\n",
    "    df.apply(merton_solver_accounting, axis=1).tolist(), #  Changed merton_solver to merton_solver_accounting\n",
    "    columns=['V', 'σ_V', 'status', 'tag'], #  Added 'tag' column to match function output\n",
    "    index=df.index\n",
    ")\n",
    "df[['V', 'σ_V', 'status', 'tag']] = results #  Added 'tag' column assignment\n",
    "\n",
    "# ---- Check results ----\n",
    "print(df[['instrument','year','V','σ_V','status','tag']].head()) # Added 'tag' to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db494057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input data with shape: (1425, 32)\n"
     ]
    }
   ],
   "source": [
    "# 7. Define solver, run demo, and compute all results from raw input\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import root\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Load the raw INPUT data (not output!) ---\n",
    "base_dir = Path().resolve()\n",
    "input_fp = base_dir / 'data' / 'clean' / 'Book2_clean.csv'   # Change if needed!\n",
    "df = pd.read_csv(input_fp)\n",
    "print(\"Loaded input data with shape:\", df.shape)\n",
    "\n",
    "# --- 2. Define the Merton solver ---\n",
    "T = 1.0\n",
    "\n",
    "def merton_solver(row, T=T):\n",
    "    \"\"\"\n",
    "    Solve for asset value V and asset volatility sigma_V.\n",
    "    Returns (V, sigma_V, status_flag).\n",
    "    Converts debt_total from millions to USD for calculations.\n",
    "    \"\"\"\n",
    "    E   = row['market_cap']\n",
    "    σ_E = row['equity_vol']\n",
    "    F   = row['debt_total'] * 1_000_000  # Convert from millions to USD\n",
    "    r_f = row['rf']\n",
    "\n",
    "    # Input validation\n",
    "    if pd.isna(E) or pd.isna(σ_E) or pd.isna(F):\n",
    "        return np.nan, np.nan, 'missing_input'\n",
    "    if E <= 0 or σ_E <= 0 or F < 0:\n",
    "        return np.nan, np.nan, 'invalid_value'\n",
    "    if F == 0:\n",
    "        return np.nan, np.nan, 'no_debt'\n",
    "\n",
    "    # Merton equations\n",
    "    def equations(x):\n",
    "        V, σ_V = x\n",
    "        d1 = (np.log(V/F) + (r_f + 0.5*σ_V**2)*T) / (σ_V * np.sqrt(T))\n",
    "        d2 = d1 - σ_V * np.sqrt(T)\n",
    "        eq1 = V * norm.cdf(d1) - F * np.exp(-r_f*T) * norm.cdf(d2) - E\n",
    "        eq2 = σ_E - (V/E) * norm.cdf(d1) * σ_V\n",
    "        return [eq1, eq2]\n",
    "\n",
    "    initial = [E + F, σ_E]\n",
    "    sol     = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d264c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input data with shape: (1425, 32)\n"
     ]
    }
   ],
   "source": [
    "# 7. Define solver, run demo, and compute all results from raw input\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import root\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Load the raw INPUT data (not output!) ---\n",
    "base_dir = Path().resolve()\n",
    "input_fp = base_dir / 'data' / 'clean' / 'Book2_clean.csv'   # Change if needed!\n",
    "df = pd.read_csv(input_fp)\n",
    "print(\"Loaded input data with shape:\", df.shape)\n",
    "\n",
    "# --- 2. Define the Merton solver ---\n",
    "T = 1.0\n",
    "\n",
    "def merton_solver(row, T=T):\n",
    "    \"\"\"\n",
    "    Solve for asset value V and asset volatility sigma_V.\n",
    "    Returns (V, sigma_V, status_flag).\n",
    "    Converts debt_total from millions to USD for calculations.\n",
    "    \"\"\"\n",
    "    E   = row['market_cap']\n",
    "    σ_E = row['equity_vol']\n",
    "    F   = row['debt_total'] * 1_000_000  # Convert from millions to USD\n",
    "    r_f = row['rf']\n",
    "\n",
    "    # Input validation\n",
    "    if pd.isna(E) or pd.isna(σ_E) or pd.isna(F):\n",
    "        return np.nan, np.nan, 'missing_input'\n",
    "    if E <= 0 or σ_E <= 0 or F < 0:\n",
    "        return np.nan, np.nan, 'invalid_value'\n",
    "    if F == 0:\n",
    "        return np.nan, np.nan, 'no_debt'\n",
    "\n",
    "    # Merton equations\n",
    "    def equations(x):\n",
    "        V, σ_V = x\n",
    "        d1 = (np.log(V/F) + (r_f + 0.5*σ_V**2)*T) / (σ_V * np.sqrt(T))\n",
    "        d2 = d1 - σ_V * np.sqrt(T)\n",
    "        eq1 = V * norm.cdf(d1) - F * np.exp(-r_f*T) * norm.cdf(d2) - E\n",
    "        eq2 = σ_E - (V/E) * norm.cdf(d1) * σ_V\n",
    "        return [eq1, eq2]\n",
    "\n",
    "    initial = [E + F, σ_E]\n",
    "    sol     = root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b734210",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73ab29a3",
   "metadata": {},
   "source": [
    "## 7. Run the Accounting-Based Solver and Compute DDa/PDa\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Apply `merton_solver_accounting` to every row of `df` to get  \n",
    "   - `asset_value` (V)  \n",
    "   - `asset_vol` (σ_V)  \n",
    "   - `merton_status` (convergence flag)  \n",
    "   - `dd_pd_tag` (e.g. \"no_debt\")  \n",
    "2. Compute **Distance to Default** (DDa):  \n",
    "   $$\n",
    "     \\mathrm{DDa}\n",
    "     = \\frac{\\ln(V / F)\\;+\\;(0 - \\tfrac12\\,\\sigma_V^2)\\,T}\n",
    "            {\\sigma_V\\,\\sqrt{T}}\n",
    "   $$\n",
    "3. Compute **Probability of Default** (PDa):  \n",
    "   $$\n",
    "     \\mathrm{PDa} = \\Phi\\bigl(-\\mathrm{DDa}\\bigr)\n",
    "   $$\n",
    "4. Set both to `NaN` when `dd_pd_tag == 'no_debt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Applying accounting-based Merton solver to each row…\n",
      "  instrument  year   asset_value  asset_vol        DDa           PDa\n",
      "0        JPM  2016  2.489755e+12   0.165704   9.661405  2.198945e-22\n",
      "1        JPM  2017  2.532496e+12   0.136142  11.925326  4.367087e-33\n",
      "2        JPM  2018  2.623749e+12   0.147320  10.737247  3.402157e-27\n",
      "3        JPM  2019  2.686545e+12   0.183468   8.900168  2.788118e-19\n",
      "4        JPM  2020  3.385457e+12   0.321152   5.543240  1.484631e-08\n",
      "\n",
      "Solver status counts:\n",
      " merton_status\n",
      "converged      1405\n",
      "no_debt          16\n",
      "no_converge       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Apply the solver\n",
    "print(\"[INFO] Applying accounting-based Merton solver to each row…\")\n",
    "results = df.apply(merton_solver_accounting, axis=1, result_type='expand')\n",
    "df[['asset_value','asset_vol','merton_status','dd_pd_tag']] = results\n",
    "\n",
    "# 7.2 Compute DDa and PDa\n",
    "V   = df['asset_value']\n",
    "F   = df['debt_total'] * 1_000_000  #  Convert debt to actual USD for consistent calculation\n",
    "σ_V = df['asset_vol']\n",
    "\n",
    "# Numerator: log distance + drift (drift term uses r_f=0 here)\n",
    "num = np.log(V / F) + (0.0 - 0.5 * σ_V**2) * T\n",
    "# Denominator: volatility × sqrt(T)\n",
    "den = σ_V * np.sqrt(T)\n",
    "\n",
    "df['DDa'] = np.where(\n",
    "    (df['dd_pd_tag']=='no_debt') | (df['dd_pd_tag']=='negative_equity'), \n",
    "    np.nan, \n",
    "    num / den\n",
    ")\n",
    "df['PDa'] = np.where(\n",
    "    (df['dd_pd_tag']=='no_debt') | (df['dd_pd_tag']=='negative_equity'), \n",
    "    np.nan, \n",
    "    norm.cdf(-df['DDa'])\n",
    ")\n",
    "\n",
    "# 7.3 Quick check of results\n",
    "print(df[['instrument','year','asset_value','asset_vol','DDa','PDa']].head())\n",
    "print(\"\\nSolver status counts:\\n\", df['merton_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62b3af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa231fe",
   "metadata": {},
   "source": [
    "## 8. Export Results & Append Diagnostics\n",
    "\n",
    "In this final step we will:\n",
    "\n",
    "1. **Save** the full DataFrame (including `DDa` and `PDa`) to CSV at `output_fp`.  \n",
    "2. **Append** a diagnostics summary to the accounting log file (`log_fp`), including:  \n",
    "   - Total rows processed  \n",
    "   - Solver status counts  \n",
    "   - Summary statistics for `DDa` and `PDa`  \n",
    "   - Counts of missing or failed estimates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Diagnostics appended to:\n",
      "  /Users/guillaumebld/Library/CloudStorage/OneDrive-LoyolaUniversityChicago/Group Quantitative Research Workspace – Summer 2025 - Results/data/logs/dd_pd_accounting_log.txt\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Determine the actual DD and PD column names\n",
    "cols = df.columns.str.lower().tolist()\n",
    "# find candidates\n",
    "dd_candidates = [c for c in df.columns if 'distance' in c.lower()]\n",
    "pd_candidates = [c for c in df.columns if 'probability' in c.lower()]\n",
    "\n",
    "dd_col = dd_candidates[0] if dd_candidates else None\n",
    "pd_col = pd_candidates[0] if pd_candidates else None\n",
    "\n",
    "# 8.2 Append diagnostics using the discovered column names\n",
    "with open(log_fp, 'a') as log:\n",
    "    log.write(\"\\n=== Accounting-Based DD/PD Diagnostics ===\\n\")\n",
    "    log.write(f\"Total rows processed: {len(df)}\\n\")\n",
    "\n",
    "    # Solver status\n",
    "    status_col = 'merton_status' if 'merton_status' in df.columns \\\n",
    "                 else 'solver_status' if 'solver_status' in df.columns \\\n",
    "                 else None\n",
    "    if status_col:\n",
    "        log.write(f\"\\n{status_col} counts:\\n\")\n",
    "        log.write(df[status_col].value_counts().to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nNo status column found. Available columns:\\n\")\n",
    "        log.write(\", \".join(df.columns) + \"\\n\")\n",
    "\n",
    "    # DD summary\n",
    "    if dd_col:\n",
    "        log.write(f\"\\nDistance to Default ({dd_col}) summary:\\n\")\n",
    "        log.write(df[dd_col].describe().to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nNo DD column found. Available columns:\\n\")\n",
    "        log.write(\", \".join(df.columns) + \"\\n\")\n",
    "\n",
    "    # PD summary\n",
    "    if pd_col:\n",
    "        log.write(f\"\\nProbability of Default ({pd_col}) summary:\\n\")\n",
    "        log.write(df[pd_col].describe().to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nNo PD column found. Available columns:\\n\")\n",
    "        log.write(\", \".join(df.columns) + \"\\n\")\n",
    "\n",
    "    # Missing counts, if found\n",
    "    if dd_col:\n",
    "        log.write(f\"\\nRows with missing {dd_col}: {df[dd_col].isna().sum()}\\n\")\n",
    "    if pd_col:\n",
    "        log.write(f\"Rows with missing {pd_col}: {df[pd_col].isna().sum()}\\n\")\n",
    "\n",
    "print(f\"[INFO] Diagnostics appended to:\\n  {log_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4ef07",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "**What we’ve accomplished**  \n",
    "- Loaded and cleaned the accounting data  \n",
    "- Merged in market caps, equity volatilities, and risk-free rates  \n",
    "- Defined and ran an accounting-based Merton solver  \n",
    "- Computed annual **DDa** and **PDa**  \n",
    "- Exported results and logged diagnostics  \n",
    "\n",
    "**Next steps**  \n",
    "1. **Review the log file** (`dd_pd_accounting_log.txt`) for any convergence warnings or missing inputs.  \n",
    "2. **Compare** accounting-based metrics (`DDa/PDa`) with market-based (`DDm/PDm`) to identify discrepancies.  \n",
    "3. **Visualize** the distributions of `DDa` and `PDa` across firms and years.  \n",
    "4. **Incorporate** these default-risk measures into your credit models or presentations for Professor Abel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a103560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Accounting-based DD/PD results saved to:\n",
      "  /Users/guillaumebld/Library/CloudStorage/OneDrive-LoyolaUniversityChicago/Group Quantitative Research Workspace – Summer 2025 - Results/data/merged_inputs/dd_pd_accounting.csv\n",
      "[INFO] Diagnostics appended to:\n",
      "  /Users/guillaumebld/Library/CloudStorage/OneDrive-LoyolaUniversityChicago/Group Quantitative Research Workspace – Summer 2025 - Results/data/logs/dd_pd_accounting_log.txt\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Export the DataFrame to CSV (accounting-based)\n",
    "output_fp = base_dir / 'data' / 'merged_inputs' / 'dd_pd_accounting.csv'\n",
    "log_fp    = base_dir / 'data' / 'logs' / 'dd_pd_accounting_log.txt'\n",
    "\n",
    "# Write the main CSV\n",
    "df.to_csv(output_fp, index=False)\n",
    "print(f\"[INFO] Accounting-based DD/PD results saved to:\\n  {output_fp.resolve()}\")\n",
    "\n",
    "# 8.2 Append diagnostics to the log\n",
    "# Detect the actual DD/PD and status column names\n",
    "dd_col = 'DDa' if 'DDa' in df.columns else next((c for c in df.columns if 'distance' in c.lower()), None)\n",
    "pd_col = 'PDa' if 'PDa' in df.columns else next((c for c in df.columns if 'probability' in c.lower()), None)\n",
    "status_col = ('merton_status' if 'merton_status' in df.columns\n",
    "              else 'solver_status' if 'solver_status' in df.columns\n",
    "              else None)\n",
    "\n",
    "with open(log_fp, 'a') as log:\n",
    "    log.write(\"\\n=== Accounting-Based DD/PD Diagnostics ===\\n\")\n",
    "    log.write(f\"Total rows processed: {len(df)}\\n\")\n",
    "\n",
    "    # Solver status breakdown\n",
    "    if status_col:\n",
    "        log.write(f\"\\n{status_col} counts:\\n\")\n",
    "        log.write(df[status_col].value_counts().to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nNo solver-status column found. Available columns:\\n\")\n",
    "        log.write(\", \".join(df.columns) + \"\\n\")\n",
    "\n",
    "    # Distance to Default summary\n",
    "    if dd_col:\n",
    "        log.write(f\"\\nDistance to Default ({dd_col}) summary:\\n\")\n",
    "        log.write(df[dd_col].describe().to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nNo DD column found. Available columns:\\n\")\n",
    "        log.write(\", \".join(df.columns) + \"\\n\")\n",
    "\n",
    "    # Probability of Default summary\n",
    "    if pd_col:\n",
    "        log.write(f\"\\nProbability of Default ({pd_col}) summary:\\n\")\n",
    "        log.write(df[pd_col].describe().to_string() + \"\\n\")\n",
    "    else:\n",
    "        log.write(\"\\nNo PD column found. Available columns:\\n\")\n",
    "        log.write(\", \".join(df.columns) + \"\\n\")\n",
    "\n",
    "    # Missing-value counts\n",
    "    if dd_col:\n",
    "        log.write(f\"\\nRows with missing {dd_col}: {df[dd_col].isna().sum()}\\n\")\n",
    "    if pd_col:\n",
    "        log.write(f\"Rows with missing {pd_col}: {df[pd_col].isna().sum()}\\n\")\n",
    "\n",
    "print(f\"[INFO] Diagnostics appended to:\\n  {log_fp.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
