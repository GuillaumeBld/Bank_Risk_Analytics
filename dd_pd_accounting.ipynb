{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01969415",
   "metadata": {},
   "source": [
    "\n",
    "# Accounting-approach, Naive DD per Bharath and Shumway (2008). No solver.\n",
    "\n",
    "Implements Bharath and Shumway naive DD. \n",
    "Uses $\\hat V = E + F$,\n",
    "$\\hat\\sigma_D = 0.05 + 0.25 \\sigma_E$,\n",
    "value-weighted $\\hat\\sigma_V$,\n",
    "and $\\hat\\mu = r_{i,t−1}$. No solver.\n",
    "\n",
    "This notebook follows the Naive DD approach issue from Bharath and Shumway (2008) which we call the accounting-approach distance-to-default (DD) workflow as it involves the use of proxies for the default intensity and volatility, without invoking a numerical solver. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ad8dc",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "Install the minimal dependencies required to reproduce the accounting workflow locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7bc248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install needed packages (run once per environment)\n",
    "%pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40de7ce",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports, configuration, and helper utilities\n",
    "\n",
    "Load core libraries, set display defaults, and define helper functions used throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7adc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank\n",
      "Accounting input -> FOUND\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from scipy.stats import norm\n",
    "    Phi = norm.cdf\n",
    "except Exception:\n",
    "    from math import erf\n",
    "    def Phi(x):  # normal CDF fallback\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return 0.5*(1.0 + np.vectorize(erf)(x/np.sqrt(2.0)))\n",
    "\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "\n",
    "MM = 1_000_000.0\n",
    "T = 1.0\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path, marker: str = '.git') -> Path:\n",
    "    \"\"\"Walk up from *start* until a directory containing *marker* is found.\"\"\"\n",
    "    current = start.resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "\n",
    "def winsorize_series(series: pd.Series, lower: float = 0.01, upper: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Clip *series* to the given quantile range, ignoring NaNs.\"\"\"\n",
    "    clean = series.dropna()\n",
    "    if clean.empty:\n",
    "        return series\n",
    "    ql, qh = np.nanpercentile(clean, [lower * 100, upper * 100])\n",
    "    if not np.isfinite(ql) or not np.isfinite(qh) or ql > qh:\n",
    "        return series\n",
    "    return series.clip(ql, qh)\n",
    "\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "print(f\"Repository root: {base_dir}\")\n",
    "\n",
    "model_fp   = base_dir / 'data' / 'clean' / 'Book2_clean.csv'\n",
    "output_dir = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "log_dir    = base_dir / 'data' / 'logs'\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Accounting input -> {'FOUND' if model_fp.exists() else 'MISSING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d6c37",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Load and normalise accounting inputs\n",
    "\n",
    "Read the accounting file, harmonise column names, and enforce expected data types for key fields such as instrument, year, and balance sheet magnitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2512288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading accounting data…\n",
      "→ 1425 rows before cleaning\n",
      "[WARN] rit outside [-1,1]. Ensure rit is a decimal return, not percent.\n",
      "  instrument  year  total_assets  debt_total\n",
      "0        JPM  2016     2490972.0    495354.0\n",
      "1        JPM  2017     2533600.0    494798.0\n",
      "2        JPM  2018     2622532.0    533627.0\n",
      "3        JPM  2019     2687379.0    516093.0\n",
      "4        JPM  2020     3384757.0    542102.0\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Loading accounting data…')\n",
    "df_raw = pd.read_csv(model_fp)\n",
    "print(f\"→ {df_raw.shape[0]} rows before cleaning\")\n",
    "\n",
    "# Standardise column names we rely on\n",
    "rename_map = {\n",
    "    'nstrument': 'instrument',\n",
    "    'weighted_average_cost_of_capital,_(%)': 'wacc_pct',\n",
    "    'wacc_tax_rate,_(%)': 'wacc_tax_rate_pct',\n",
    "    'wacc_cost_of_debt,_(%)': 'wacc_cost_of_debt_pct',\n",
    "    'wacc_debt_weight,_(%)': 'wacc_debt_weight_pct',\n",
    "    'wacc_equity_weight,_(%)': 'wacc_equity_weight_pct',\n",
    "}\n",
    "\n",
    "df = df_raw.rename(columns=rename_map).copy()\n",
    "\n",
    "# Drop columns that are clearly placeholders\n",
    "unnamed_cols = [c for c in df.columns if c.lower().startswith('unnamed')]\n",
    "if unnamed_cols:\n",
    "    df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "# Instrument string cleanup\n",
    "if 'instrument' in df.columns:\n",
    "    df['instrument'] = (df['instrument']\n",
    "                        .astype(str)\n",
    "                        .str.strip()\n",
    "                        .str.replace('\"', '', regex=False)\n",
    "                        .str.upper())\n",
    "else:\n",
    "    raise KeyError('`instrument` column not found after renaming.')\n",
    "\n",
    "# Year as integer panel key\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Numeric conversions for balance sheet figures\n",
    "for col in ['total_assets', 'debt_total', 'price_to_book_value_per_share', 'd/e', 'rit', 'rit_rf', 'new_wacc']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convert WACC weights from percentages to decimals when present\n",
    "for src, dest in [('wacc_equity_weight_pct', 'wacc_equity_weight'),\n",
    "                  ('wacc_debt_weight_pct', 'wacc_debt_weight')]:\n",
    "    if src in df.columns:\n",
    "        df[dest] = pd.to_numeric(df[src], errors='coerce') / 100.0\n",
    "\n",
    "assert (df['total_assets'].dropna() >= 0).all() and (df['debt_total'].dropna() >= 0).all(), 'Assets/debt must be nonnegative'\n",
    "if not df['rit'].between(-1, 1).all(skipna=True):\n",
    "    print('[WARN] rit outside [-1,1]. Ensure rit is a decimal return, not percent.')\n",
    "\n",
    "print(df[['instrument', 'year', 'total_assets', 'debt_total']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6051a16",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Construct book equity and market equity proxies\n",
    "\n",
    "Compute book equity along with the three Bharath–Shumway equity proxies (price-to-book, D/E, and WACC weights). Select the best available proxy and record its source for auditability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b088650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instrument  year        be_usd             E E_source             F\n",
      "0        JPM  2016  1.995618e+12  2.688226e+12     E_pb  4.953540e+11\n",
      "1        JPM  2017  2.038802e+12  3.252329e+12     E_pb  4.947980e+11\n",
      "2        JPM  2018  2.088905e+12  2.898674e+12     E_pb  5.336270e+11\n",
      "3        JPM  2019  2.171286e+12  3.983422e+12     E_pb  5.160930e+11\n",
      "4        JPM  2020  2.842655e+12  4.418551e+12     E_pb  5.421020e+11\n"
     ]
    }
   ],
   "source": [
    "# Book equity in USD\n",
    "required_cols = ['total_assets', 'debt_total']\n",
    "# For banks: debt_total represents total liabilities\n",
    "missing_required = [c for c in required_cols if c not in df.columns]\n",
    "if missing_required:\n",
    "    raise KeyError(f\"Missing required columns: {missing_required}\")\n",
    "\n",
    "df['assets_usd'] = pd.to_numeric(df['total_assets'], errors='coerce') * MM\n",
    "df['debt_usd'] = pd.to_numeric(df['debt_total'], errors='coerce') * MM\n",
    "df['be_usd'] = (df['total_assets'] - df['debt_total']) * MM\n",
    "\n",
    "# Market equity proxies\n",
    "df['E_pb'] = np.where(\n",
    "    (df['price_to_book_value_per_share'] > 0) & (df['be_usd'] > 0),\n",
    "    df['price_to_book_value_per_share'] * df['be_usd'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df['E_de'] = np.where(\n",
    "    (df['d/e'] > 0) & (df['debt_usd'] > 0),\n",
    "    df['debt_usd'] / df['d/e'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "equity_weight = pd.to_numeric(\n",
    "    df.get('wacc_equity_weight', pd.Series(index=df.index, dtype=float)),\n",
    "    errors='coerce'\n",
    ")\n",
    "debt_weight = pd.to_numeric(\n",
    "    df.get('wacc_debt_weight', pd.Series(index=df.index, dtype=float)),\n",
    "    errors='coerce'\n",
    ")\n",
    "tolerance = 1e-3\n",
    "weights_sum = equity_weight + debt_weight\n",
    "wacc_mask = (\n",
    "    (equity_weight > 0)\n",
    "    & (debt_weight > 0)\n",
    "    & (np.abs(weights_sum - 1) <= tolerance)\n",
    ")\n",
    "\n",
    "df['E_wacc'] = np.where(\n",
    "    wacc_mask,\n",
    "    df['assets_usd'] * equity_weight,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Prioritise proxies: price-to-book, then D/E, then WACC\n",
    "values = []\n",
    "sources = []\n",
    "for _, row in df[['E_pb', 'E_de', 'E_wacc']].iterrows():\n",
    "    value = np.nan\n",
    "    source = 'missing'\n",
    "    for key in ['E_pb', 'E_de', 'E_wacc']:\n",
    "        v = row[key]\n",
    "        if pd.notna(v) and v > 0:\n",
    "            value = v\n",
    "            source = key\n",
    "            break\n",
    "    values.append(value)\n",
    "    sources.append(source)\n",
    "\n",
    "df['E'] = values\n",
    "df['E_source'] = sources\n",
    "\n",
    "df['weak_E_proxy'] = df['E_source'].isin(['E_de', 'E_wacc'])\n",
    "\n",
    "df['F'] = df['debt_usd']\n",
    "\n",
    "print(df[['instrument', 'year', 'be_usd', 'E', 'E_source', 'F']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271d387",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Load Equity Volatility (Daily Returns, 252-Day Window)\n",
    "\n",
    "Load pre-calculated equity volatility following Bharath & Shumway (2008): Daily returns with 252-day window (year t-1 only), annualized using √252. Primary method uses all trading days from year t-1 (minimum 180 days). Fallbacks include partial year data and peer median imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "time_tagging_accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading sigma_E from equity_volatility_by_year_DAILY.csv (252-day window)...\n",
      "  sigma_E_tminus1: 1366 non-null values\n",
      "  Window validation:\n",
      "    - All end years = t-1: True\n",
      "    - All start <= end: True\n",
      "  sigma_E method distribution:\n",
      "sigma_E_method\n",
      "daily_252        1295\n",
      "imputed_peer       55\n",
      "daily_partial      16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Applying data quality filter: Dropping bank-years without complete volatility...\n",
      "  Initial bank-years: 1,431\n",
      "  With complete volatility: 1,366 (95.5%)\n",
      "  Dropped (no volatility): 65 (4.5%)\n",
      "  → Only bank-years with complete daily volatility will proceed to DD/PD calculation\n",
      "\n",
      "  Bank-years retained by year:\n",
      "    2016: 68 banks\n",
      "    2017: 131 banks\n",
      "    2018: 194 banks\n",
      "    2019: 203 banks\n",
      "    2020: 207 banks\n",
      "    2021: 207 banks\n",
      "    2022: 208 banks\n",
      "    2023: 148 banks\n",
      "[INFO] Computing mu_hat_t = r_{i,t-1} with fallbacks...\n",
      "[INFO] Creating size buckets...\n",
      "  Size bucket counts: {'small': 1270, 'mid': 64, 'large': 32}\n",
      "  Imputed sigma_E (imputed_peer): 55 rows\n"
     ]
    }
   ],
   "source": [
    "# Load DAILY equity volatility from pre-calculated file (Bharath & Shumway 2008)\n",
    "print('[INFO] Loading sigma_E from equity_volatility_by_year_DAILY.csv (252-day window)...')\n",
    "\n",
    "# Load the daily volatility file\n",
    "vol_fp = base_dir / 'data' / 'clean' / 'equity_volatility_by_year_DAILY.csv'\n",
    "equity_vol = pd.read_csv(vol_fp)\n",
    "\n",
    "# Rename columns to match expected format\n",
    "equity_vol_merge = equity_vol.rename(columns={\n",
    "    'ticker': 'instrument',\n",
    "    'sigma_E': 'sigma_E_tminus1',\n",
    "    'method': 'sigma_E_method',\n",
    "    'days_used': 'sigma_E_days'\n",
    "})\n",
    "\n",
    "# Merge into main DataFrame\n",
    "df = df.merge(\n",
    "    equity_vol_merge[['instrument', 'year', 'sigma_E_tminus1', \n",
    "                      'sigma_E_method', 'sigma_E_days']],\n",
    "    on=['instrument', 'year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate window provenance for daily method\n",
    "# Daily method uses year t-1 only (252 trading days)\n",
    "# Example: year=2018 uses all of 2017 (window start = end = 2017)\n",
    "df['sigmaE_window_end_year'] = df['year'] - 1\n",
    "df['sigmaE_window_start_year'] = df['year'] - 1  # Daily uses year t-1 only\n",
    "df['sigma_E_days'] = df['sigma_E_days'].fillna(0)\n",
    "\n",
    "# Use sigma_E_tminus1 for calculations\n",
    "df['sigma_E'] = df['sigma_E_tminus1']\n",
    "\n",
    "print(f'  sigma_E_tminus1: {df[\"sigma_E_tminus1\"].notna().sum()} non-null values')\n",
    "print(f'  Window validation:')\n",
    "print(f'    - All end years = t-1: {(df[\"sigmaE_window_end_year\"] == df[\"year\"] - 1).all()}')\n",
    "print(f'    - All start <= end: {(df[\"sigmaE_window_start_year\"] <= df[\"sigmaE_window_end_year\"]).all()}')\n",
    "print(f'  sigma_E method distribution:')\n",
    "print(df[\"sigma_E_method\"].value_counts())\n",
    "\n",
    "# DATA QUALITY FILTER: Drop bank-years without complete volatility\n",
    "print('\\n[INFO] Applying data quality filter: Dropping bank-years without complete volatility...')\n",
    "initial_count = len(df)\n",
    "df = df[df['sigma_E'].notna()].copy()\n",
    "filtered_count = len(df)\n",
    "dropped_count = initial_count - filtered_count\n",
    "\n",
    "print(f'  Initial bank-years: {initial_count:,}')\n",
    "print(f'  With complete volatility: {filtered_count:,} ({filtered_count/initial_count*100:.1f}%)')\n",
    "print(f'  Dropped (no volatility): {dropped_count:,} ({dropped_count/initial_count*100:.1f}%)')\n",
    "print(f'  → Only bank-years with complete daily volatility will proceed to DD/PD calculation')\n",
    "\n",
    "# Show retained bank-years by year\n",
    "if dropped_count > 0:\n",
    "    retained_by_year = df.groupby('year').size()\n",
    "    print(f'\\n  Bank-years retained by year:')\n",
    "    for year, count in retained_by_year.items():\n",
    "        print(f'    {year}: {count} banks')\n",
    "\n",
    "# Build mu_hat_t = r_{i,t-1} with provenance tracking\n",
    "print('[INFO] Computing mu_hat_t = r_{i,t-1} with fallbacks...')\n",
    "\n",
    "df['mu_hat_from'] = 'rit_tminus1'\n",
    "df['mu_source_year'] = df['year'] - 1\n",
    "df['mu_hat'] = df.groupby('instrument', group_keys=False)['rit'].shift(1)\n",
    "\n",
    "# Create size buckets for later imputation\n",
    "print('[INFO] Creating size buckets...')\n",
    "df = df.sort_values(['instrument', 'year']).reset_index(drop=True)\n",
    "\n",
    "size_bucket = np.select(\n",
    "    [df.get('dummylarge', 0) == 1, df.get('dummymid', 0) == 1],\n",
    "    ['large', 'mid'],\n",
    "    default='small'\n",
    ")\n",
    "df['size_bucket'] = size_bucket\n",
    "print(f'  Size bucket counts: {df[\"size_bucket\"].value_counts().to_dict()}')\n",
    "\n",
    "# Create flags that were in old sigma_E calculation (now dummy flags)\n",
    "# Since we're using pre-calculated sigma_E from file, these are all False\n",
    "df['insufficient_returns'] = False  # All sigma_E come from file, none insufficient\n",
    "df['imputed_sigmaE_sizebucket'] = False  # Will be set properly based on sigma_E_method\n",
    "\n",
    "# Mark imputed values based on method from equity_volatility file\n",
    "if 'sigma_E_method' in df.columns:\n",
    "    df['imputed_sigmaE_sizebucket'] = df['sigma_E_method'] == 'imputed_peer'\n",
    "    print(f'  Imputed sigma_E (imputed_peer): {df[\"imputed_sigmaE_sizebucket\"].sum()} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "time_assertions_accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Validating time integrity for accounting approach...\n",
      "[PASS] All time integrity assertions passed\n",
      "  - sigma_E uses only data up to t-1: True\n",
      "  - mu_hat uses t-1 data: True\n"
     ]
    }
   ],
   "source": [
    "# TIME INTEGRITY ASSERTIONS\n",
    "print('[INFO] Validating time integrity for accounting approach...')\n",
    "\n",
    "# Import time checks\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from utils.time_checks import assert_time_integrity\n",
    "\n",
    "# Assertion 1: sigma_E window must end at t-1\n",
    "assert (df['sigmaE_window_end_year'] == df['year'] - 1).all(), \\\n",
    "    'sigma_E window end must be t-1 (no lookahead)'\n",
    "\n",
    "# Assertion 2: mu_hat provenance is set\n",
    "assert df['mu_hat_from'].notna().all(), 'mu_hat provenance must be tracked'\n",
    "\n",
    "# Assertion 3: When using rit_tminus1, source year must be t-1\n",
    "uses_lag = df['mu_hat_from'].eq('rit_tminus1')\n",
    "if uses_lag.any():\n",
    "    assert (df.loc[uses_lag, 'mu_source_year'] == df.loc[uses_lag, 'year'] - 1).all(), \\\n",
    "        'mu_hat source year must be t-1 when using lagged return'\n",
    "\n",
    "# Run comprehensive time integrity check\n",
    "assert_time_integrity(df)\n",
    "\n",
    "print('[PASS] All time integrity assertions passed')\n",
    "print(f'  - sigma_E uses only data up to t-1: {(df[\"sigmaE_window_end_year\"] == df[\"year\"] - 1).all()}')\n",
    "print(f'  - mu_hat uses t-1 data: {(df[df[\"mu_hat_from\"].eq(\"rit_tminus1\")][\"mu_source_year\"] == df[df[\"mu_hat_from\"].eq(\"rit_tminus1\")][\"year\"] - 1).all()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4058966",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Debt volatility proxy, asset proxies, and drift proxy\n",
    "\n",
    "Derive the Bharath–Shumway debt volatility proxy, approximate asset value/volatility, and compute the drift proxy based on lagged equity returns with firm and size-bucket fallbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa142b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instrument  year  sigma_D_hat  sigma_V_hat    mu_hat\n",
      "0       ABCB  2016     0.109790     0.233860  0.046529\n",
      "1       ABCB  2017     0.113656     0.251598  0.366828\n",
      "2       ABCB  2018     0.114907     0.256425  0.244121\n",
      "3       ABCB  2019     0.121581     0.274489 -0.350111\n",
      "4       ABCB  2020     0.113675     0.250830  0.033435\n"
     ]
    }
   ],
   "source": [
    "# Debt and asset volatility proxies\n",
    "df['sigma_D_hat'] = 0.05 + 0.25 * df['sigma_E']\n",
    "df['V_hat'] = df['E'] + df['F']\n",
    "valid_v = df['V_hat'] > 0\n",
    "\n",
    "sigma_V_components = np.where(\n",
    "    valid_v,\n",
    "    (df['E'] / df['V_hat']) * df['sigma_E'] + (df['F'] / df['V_hat']) * df['sigma_D_hat'],\n",
    "    np.nan\n",
    ")\n",
    "df['sigma_V_hat'] = sigma_V_components\n",
    "\n",
    "df['sigma_V_hat'] = df['sigma_V_hat'].clip(lower=1e-6)\n",
    "\n",
    "# Drift proxy using lagged returns\n",
    "lagged_rit = df.groupby('instrument', group_keys=False)['rit'].shift(1)\n",
    "firm_mean = (\n",
    "    df.groupby('instrument', group_keys=False)['rit']\n",
    "      .apply(lambda s: s.expanding().mean().shift(1))\n",
    ")\n",
    "\n",
    "df['mu_hat'] = lagged_rit\n",
    "mask_mu = df['mu_hat'].isna()\n",
    "df.loc[mask_mu, 'mu_hat'] = firm_mean[mask_mu]\n",
    "\n",
    "size_median_mu = df.groupby('size_bucket')['mu_hat'].transform('median')\n",
    "df['mu_hat'] = df['mu_hat'].fillna(size_median_mu)\n",
    "df['mu_hat'] = df['mu_hat'].fillna(df['mu_hat'].median())\n",
    "\n",
    "print(df[['instrument', 'year', 'sigma_D_hat', 'sigma_V_hat', 'mu_hat']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4440792",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Compute naive distance to default (DD) and probability of default (PD)\n",
    "\n",
    "Apply the Bharath–Shumway naive formulas using the proxies above. Probability of default is clipped to the [0, 1] interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a36201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instrument  year   DD_naive      PD_naive\n",
      "0       ABCB  2016  13.743862  2.771898e-43\n",
      "1       ABCB  2017  16.600072  3.480372e-62\n",
      "2       ABCB  2018  15.685909  9.442022e-56\n",
      "3       ABCB  2019   8.181287  1.404143e-16\n",
      "4       ABCB  2020  14.341664  6.006968e-47\n",
      "PD==0 count: 3, PD==1 count: 0\n"
     ]
    }
   ],
   "source": [
    "valid_sigmaV = np.isfinite(df['sigma_V_hat']) & (df['sigma_V_hat'] > 0)\n",
    "valid_inputs = df['E'].gt(0) & df['F'].gt(0) & valid_sigmaV & df['mu_hat'].notna()\n",
    "\n",
    "# Bharath & Shumway naive DD: V_hat=E+F, sigma_D_hat=0.05+0.25*sigma_E,\n",
    "# sigma_V_hat = value-weighted mix, mu_hat = lagged equity return. No solver.\n",
    "df['DD_naive'] = np.where(\n",
    "    valid_inputs,\n",
    "    (np.log(df['V_hat'] / df['F']) + (df['mu_hat'] - 0.5 * df['sigma_V_hat'] ** 2) * T)\n",
    "    / (df['sigma_V_hat'] * math.sqrt(T)),\n",
    "    np.nan,\n",
    ")\n",
    "df['PD_naive'] = np.where(np.isfinite(df['DD_naive']), Phi(-df['DD_naive']), np.nan)\n",
    "df['invalid_sigmaV'] = ~valid_sigmaV\n",
    "\n",
    "print(df[['instrument', 'year', 'DD_naive', 'PD_naive']].head())\n",
    "print(f\"PD==0 count: {(df['PD_naive'] == 0).sum()}, PD==1 count: {(df['PD_naive'] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf02f4d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Data-quality flags and status tracking\n",
    "\n",
    "Capture the first applicable status flag (missing inputs, fallbacks, or imputations) as `naive_status` so downstream users understand how each observation was derived.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a50bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive status counts:\n",
      "naive_status\n",
      "imputed_sigmaE_sizebucket      55\n",
      "nonpos_EF                      16\n",
      "ok                           1295\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fallback indicator counts:\n",
      "invalid_sigmaV: 0\n",
      "fallback_E_from_de: 0\n",
      "fallback_E_from_wacc: 0\n",
      "imputed_sigmaE_sizebucket: 55\n",
      "insufficient_returns: 0\n",
      "\n",
      "Equity source mix:\n",
      "E_source\n",
      "E_pb    1366\n",
      "Name: count, dtype: int64\n",
      "Weak equity proxy count: 0\n",
      "\n",
      "DD_naive summary:\n",
      "count    1350.000000\n",
      "mean       11.808152\n",
      "std         5.497494\n",
      "min        -5.719526\n",
      "10%         5.391005\n",
      "25%         8.547141\n",
      "50%        11.238610\n",
      "75%        14.281865\n",
      "90%        17.737170\n",
      "max        61.440271\n",
      "Name: DD_naive, dtype: float64\n",
      "Rows with missing DD_naive: 16\n",
      "\n",
      "PD_naive summary:\n",
      "count    1.350000e+03\n",
      "mean     8.508458e-04\n",
      "std      2.729152e-02\n",
      "min      0.000000e+00\n",
      "10%      1.213645e-70\n",
      "25%      1.426443e-46\n",
      "50%      1.317532e-29\n",
      "75%      6.309154e-18\n",
      "90%      3.508941e-08\n",
      "max      1.000000e+00\n",
      "Name: PD_naive, dtype: float64\n",
      "Rows with missing PD_naive: 16\n",
      "[INFO] Wrote diagnostics to /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/logs/dd_pd_accounting_log.txt\n"
     ]
    }
   ],
   "source": [
    "missing_E = ~np.isfinite(df['E'])\n",
    "missing_F = ~np.isfinite(df['F'])\n",
    "nonpos_EF = (df['E'] <= 0) | (df['F'] <= 0)\n",
    "\n",
    "flag_specs = [\n",
    "    ('invalid_sigmaV', df['invalid_sigmaV']),\n",
    "    ('missing_E', missing_E | (df['E_source'] == 'missing')),\n",
    "    ('missing_F', missing_F),\n",
    "    ('nonpos_EF', nonpos_EF),\n",
    "    ('insufficient_returns', df['insufficient_returns']),\n",
    "    ('imputed_sigmaE_sizebucket', df['imputed_sigmaE_sizebucket']),\n",
    "    ('fallback_E_from_de', df['E_source'] == 'E_de'),\n",
    "    ('fallback_E_from_wacc', df['E_source'] == 'E_wacc'),\n",
    "]\n",
    "\n",
    "for name, mask in flag_specs:\n",
    "    df[name] = mask.astype(bool)\n",
    "\n",
    "\n",
    "def assign_status(idx: int) -> str:\n",
    "    for name, _ in flag_specs:\n",
    "        if bool(df.iloc[idx][name]):\n",
    "            return name\n",
    "    return 'ok'\n",
    "\n",
    "\n",
    "naive_status = [assign_status(i) for i in range(len(df))]\n",
    "df['naive_status'] = naive_status\n",
    "\n",
    "status_counts = df['naive_status'].value_counts(dropna=False).sort_index()\n",
    "print('Naive status counts:')\n",
    "print(status_counts)\n",
    "\n",
    "fallback_summary = {\n",
    "    'invalid_sigmaV': int(df['invalid_sigmaV'].sum()),\n",
    "    'fallback_E_from_de': int(df['fallback_E_from_de'].sum()),\n",
    "    'fallback_E_from_wacc': int(df['fallback_E_from_wacc'].sum()),\n",
    "    'imputed_sigmaE_sizebucket': int(df['imputed_sigmaE_sizebucket'].sum()),\n",
    "    'insufficient_returns': int(df['insufficient_returns'].sum()),\n",
    "}\n",
    "print(\"\\nFallback indicator counts:\")\n",
    "for label, count in fallback_summary.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "e_source_counts = df['E_source'].value_counts(dropna=False).sort_index()\n",
    "print(\"\\nEquity source mix:\")\n",
    "print(e_source_counts)\n",
    "weak_proxy_count = int(df['weak_E_proxy'].sum())\n",
    "print(f\"Weak equity proxy count: {weak_proxy_count}\")\n",
    "\n",
    "percentiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "dd_stats = df['DD_naive'].describe(percentiles=percentiles)\n",
    "pd_stats = df['PD_naive'].describe(percentiles=percentiles)\n",
    "dd_missing = int(df['DD_naive'].isna().sum())\n",
    "pd_missing = int(df['PD_naive'].isna().sum())\n",
    "\n",
    "print(\"\\nDD_naive summary:\")\n",
    "print(dd_stats)\n",
    "print(f\"Rows with missing DD_naive: {dd_missing}\")\n",
    "\n",
    "print(\"\\nPD_naive summary:\")\n",
    "print(pd_stats)\n",
    "print(f\"Rows with missing PD_naive: {pd_missing}\")\n",
    "\n",
    "log_lines = []\n",
    "log_lines.append('=== Naive DD/PD Diagnostics ===')\n",
    "log_lines.append(f'Total rows processed: {len(df)}')\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Naive status counts:')\n",
    "log_lines.extend([f\"{status}: {count}\" for status, count in status_counts.items()])\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Fallback indicator counts:')\n",
    "for label, count in fallback_summary.items():\n",
    "    log_lines.append(f\"{label}: {count}\")\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Equity source mix:')\n",
    "log_lines.extend([f\"{source}: {count}\" for source, count in e_source_counts.items()])\n",
    "log_lines.append('')\n",
    "log_lines.append(f\"Weak equity proxy count: {weak_proxy_count}\")\n",
    "\n",
    "log_lines.append('DD_naive summary:')\n",
    "log_lines.extend([f\"{idx}: {value}\" for idx, value in dd_stats.items()])\n",
    "log_lines.append(f'Rows with missing DD_naive: {dd_missing}')\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('PD_naive summary:')\n",
    "log_lines.extend([f\"{idx}: {value}\" for idx, value in pd_stats.items()])\n",
    "log_lines.append(f'Rows with missing PD_naive: {pd_missing}')\n",
    "log_lines.append(f\"PD==0 count: {(df['PD_naive'] == 0).sum()}, PD==1 count: {(df['PD_naive'] == 1).sum()}\")\n",
    "\n",
    "log_path = log_dir / 'dd_pd_accounting_log.txt'\n",
    "log_path.write_text(\"\\n\".join(log_lines))\n",
    "print(f\"[INFO] Wrote diagnostics to {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63725a0c",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Persist outputs and quick diagnostics\n",
    "\n",
    "Save the naive DD/PD results and a percentile summary by year for quick reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e98c2df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARCHIVE] Moved to archive: accounting_20251011_042604.csv\n",
      "[CLEANUP] Removed old archive: accounting_20251005_033611.csv\n",
      "[INFO] Saved accounting DD/PD results to /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/accounting_20251014_022117.csv\n",
      "[INFO] Saved percentile summary to /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/analysis/accounting_20251014_022117_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>metric</th>\n",
       "      <th>p10</th>\n",
       "      <th>p25</th>\n",
       "      <th>p50</th>\n",
       "      <th>p75</th>\n",
       "      <th>p90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>9.790408</td>\n",
       "      <td>11.161703</td>\n",
       "      <td>13.084508</td>\n",
       "      <td>15.147648</td>\n",
       "      <td>18.107119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>8.812868</td>\n",
       "      <td>10.182653</td>\n",
       "      <td>12.274538</td>\n",
       "      <td>14.784404</td>\n",
       "      <td>17.406708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>8.874889</td>\n",
       "      <td>10.493215</td>\n",
       "      <td>12.210076</td>\n",
       "      <td>14.917300</td>\n",
       "      <td>18.541829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>7.706350</td>\n",
       "      <td>9.222354</td>\n",
       "      <td>11.874427</td>\n",
       "      <td>14.785628</td>\n",
       "      <td>18.330070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>9.007549</td>\n",
       "      <td>10.955405</td>\n",
       "      <td>13.502497</td>\n",
       "      <td>17.318781</td>\n",
       "      <td>20.837078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year metric       p10        p25        p50        p75        p90\n",
       "0  2016   DD_a  9.790408  11.161703  13.084508  15.147648  18.107119\n",
       "1  2017   DD_a  8.812868  10.182653  12.274538  14.784404  17.406708\n",
       "2  2018   DD_a  8.874889  10.493215  12.210076  14.917300  18.541829\n",
       "3  2019   DD_a  7.706350   9.222354  11.874427  14.785628  18.330070\n",
       "4  2020   DD_a  9.007549  10.955405  13.502497  17.318781  20.837078"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Archiving and timestamped output setup\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    # Move all existing files to archive\n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] Moved to archive: {os.path.basename(old_file)}\")\n",
    "    \n",
    "    # Clean up archive to keep only max_keep files\n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] Removed old archive: {os.path.basename(old_archive)}\")\n",
    "\n",
    "# Rename columns to standard naming convention\n",
    "df = df.rename(columns={'DD_naive': 'DD_a', 'PD_naive': 'PD_a'})\n",
    "\n",
    "result_cols = [\n",
    "    'instrument', 'year', 'E', 'E_source', 'weak_E_proxy', 'E_pb', 'E_de', 'E_wacc',\n",
    "    'F', 'sigma_E', 'sigma_D_hat', 'sigma_V_hat', 'mu_hat',\n",
    "    'DD_a', 'PD_a', 'naive_status'\n",
    "]\n",
    "\n",
    "# Setup archive directory\n",
    "archive_dir = base_dir / 'archive' / 'datasets'\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Archive old accounting files and save new one with timestamp\n",
    "archive_old_files(output_dir, archive_dir, 'accounting', max_keep=5)\n",
    "\n",
    "timestamp = get_timestamp_cdt()\n",
    "dd_output = output_dir / f'accounting_{timestamp}.csv'\n",
    "# Provenance columns for time integrity audit\n",
    "provenance_cols = [\"sigma_E_tminus1\", \"sigmaE_window_start_year\", \n",
    "                   \"sigmaE_window_end_year\", \"mu_hat\", \"mu_hat_from\", \n",
    "                   \"mu_source_year\", \"DD_a\", \"PD_a\"]\n",
    "\n",
    "df[result_cols].to_csv(dd_output, index=False)\n",
    "print(f\"[INFO] Saved accounting DD/PD results to {dd_output}\")\n",
    "\n",
    "cfg = {'T': T, 'ROLL_YEARS': 3, 'WINSOR_P': [0.01, 0.99], 'Phi': 'scipy' if 'norm' in globals() else 'erf_fallback', 'spec': 'Bharath–Shumway naive, no solver, v1'}\n",
    "(Path(output_dir) / 'dd_pd_naive_config.json').write_text(pd.Series(cfg).to_json())\n",
    "\n",
    "percentiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "percentile_columns = [f\"p{int(p * 100)}\" for p in percentiles]\n",
    "\n",
    "\n",
    "def build_percentile_table(metric: str) -> pd.DataFrame:\n",
    "    clean = df[['year', metric]].dropna()\n",
    "    if clean.empty:\n",
    "        empty = {'year': ['overall'], 'metric': [metric]}\n",
    "        for col in percentile_columns:\n",
    "            empty[col] = [np.nan]\n",
    "        return pd.DataFrame(empty)\n",
    "\n",
    "    by_year = (\n",
    "        clean.groupby('year')[metric]\n",
    "             .quantile(percentiles)\n",
    "             .unstack(level=-1)\n",
    "    )\n",
    "    by_year.columns = percentile_columns\n",
    "    by_year = by_year.reset_index()\n",
    "    by_year.insert(0, 'metric', metric)\n",
    "\n",
    "    overall_values = df[metric].dropna()\n",
    "    overall_row = {'metric': metric, 'year': 'overall'}\n",
    "    for col, pct in zip(percentile_columns, percentiles):\n",
    "        overall_row[col] = overall_values.quantile(pct) if not overall_values.empty else np.nan\n",
    "\n",
    "    combined = pd.concat([by_year, pd.DataFrame([overall_row])], ignore_index=True)\n",
    "    ordered_columns = ['year', 'metric', *percentile_columns]\n",
    "    return combined[ordered_columns]\n",
    "\n",
    "\n",
    "summary_frames = [build_percentile_table(metric) for metric in ['DD_a', 'PD_a']]\n",
    "summary = pd.concat(summary_frames, ignore_index=True)\n",
    "\n",
    "summary_output = base_dir / 'data' / 'outputs' / 'analysis' / f'accounting_{timestamp}_summary.csv'\n",
    "summary.to_csv(summary_output, index=False)\n",
    "print(f\"[INFO] Saved percentile summary to {summary_output}\")\n",
    "\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61faab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
