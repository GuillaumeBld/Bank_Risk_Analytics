{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01969415",
   "metadata": {},
   "source": [
    "\n",
    "# Accounting approach, Naive DD per Bharath and Shumway (2008). No solver.\n",
    "\n",
    "Implements Bharath and Shumway naive DD. Uses \\(\\hat V = E + F\\), \\(\\hat\\sigma_D = 0.05 + 0.25 \\sigma_E\\), value-weighted \\(\\hat\\sigma_V\\), and \\(\\hat\\mu = r_{i,t−1}\\). No solver.\n",
    "\n",
    "This notebook rebuilds the accounting-based distance-to-default (DD) workflow without invoking a numerical solver. It follows the Bharath and Shumway (2008) naive specification using only the accounting file columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ad8dc",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "Install the minimal dependencies required to reproduce the accounting workflow locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install needed packages (run once per environment)\n",
    "%pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40de7ce",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports, configuration, and helper utilities\n",
    "\n",
    "Load core libraries, set display defaults, and define helper functions used throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7adc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from scipy.stats import norm\n",
    "    Phi = norm.cdf\n",
    "except Exception:\n",
    "    from math import erf\n",
    "    def Phi(x):  # normal CDF fallback\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return 0.5*(1.0 + np.vectorize(erf)(x/np.sqrt(2.0)))\n",
    "\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "\n",
    "MM = 1_000_000.0\n",
    "T = 1.0\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path, marker: str = '.git') -> Path:\n",
    "    \"\"\"Walk up from *start* until a directory containing *marker* is found.\"\"\"\n",
    "    current = start.resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "\n",
    "def winsorize_series(series: pd.Series, lower: float = 0.01, upper: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Clip *series* to the given quantile range, ignoring NaNs.\"\"\"\n",
    "    clean = series.dropna()\n",
    "    if clean.empty:\n",
    "        return series\n",
    "    ql, qh = np.nanpercentile(clean, [lower * 100, upper * 100])\n",
    "    if not np.isfinite(ql) or not np.isfinite(qh) or ql > qh:\n",
    "        return series\n",
    "    return series.clip(ql, qh)\n",
    "\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "print(f\"Repository root: {base_dir}\")\n",
    "\n",
    "model_fp   = base_dir / 'data' / 'clean' / 'Book2_clean.csv'\n",
    "output_dir = base_dir / 'data' / 'merged_inputs'\n",
    "log_dir    = base_dir / 'data' / 'logs'\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Accounting input -> {'FOUND' if model_fp.exists() else 'MISSING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d6c37",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Load and normalise accounting inputs\n",
    "\n",
    "Read the accounting file, harmonise column names, and enforce expected data types for key fields such as instrument, year, and balance sheet magnitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2512288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Loading accounting data…')\n",
    "df_raw = pd.read_csv(model_fp)\n",
    "print(f\"→ {df_raw.shape[0]} rows before cleaning\")\n",
    "\n",
    "# Standardise column names we rely on\n",
    "rename_map = {\n",
    "    'nstrument': 'instrument',\n",
    "    'weighted_average_cost_of_capital,_(%)': 'wacc_pct',\n",
    "    'wacc_tax_rate,_(%)': 'wacc_tax_rate_pct',\n",
    "    'wacc_cost_of_debt,_(%)': 'wacc_cost_of_debt_pct',\n",
    "    'wacc_debt_weight,_(%)': 'wacc_debt_weight_pct',\n",
    "    'wacc_equity_weight,_(%)': 'wacc_equity_weight_pct',\n",
    "}\n",
    "\n",
    "df = df_raw.rename(columns=rename_map).copy()\n",
    "\n",
    "# Drop columns that are clearly placeholders\n",
    "unnamed_cols = [c for c in df.columns if c.lower().startswith('unnamed')]\n",
    "if unnamed_cols:\n",
    "    df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "# Instrument string cleanup\n",
    "if 'instrument' in df.columns:\n",
    "    df['instrument'] = (df['instrument']\n",
    "                        .astype(str)\n",
    "                        .str.strip()\n",
    "                        .str.replace('\"', '', regex=False)\n",
    "                        .str.upper())\n",
    "else:\n",
    "    raise KeyError('`instrument` column not found after renaming.')\n",
    "\n",
    "# Year as integer panel key\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Numeric conversions for balance sheet figures\n",
    "for col in ['total_assets', 'debt_total', 'price_to_book_value_per_share', 'd/e', 'rit', 'rit_rf', 'new_wacc']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convert WACC weights from percentages to decimals when present\n",
    "for src, dest in [('wacc_equity_weight_pct', 'wacc_equity_weight'),\n",
    "                  ('wacc_debt_weight_pct', 'wacc_debt_weight')]:\n",
    "    if src in df.columns:\n",
    "        df[dest] = pd.to_numeric(df[src], errors='coerce') / 100.0\n",
    "\n",
    "assert (df['total_assets'].dropna() >= 0).all() and (df['debt_total'].dropna() >= 0).all(), 'Assets/debt must be nonnegative'\n",
    "if not df['rit'].between(-1, 1).all(skipna=True):\n",
    "    print('[WARN] rit outside [-1,1]. Ensure rit is a decimal return, not percent.')\n",
    "\n",
    "print(df[['instrument', 'year', 'total_assets', 'debt_total']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6051a16",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Construct book equity and market equity proxies\n",
    "\n",
    "Compute book equity along with the three Bharath–Shumway equity proxies (price-to-book, D/E, and WACC weights). Select the best available proxy and record its source for auditability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b088650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book equity in USD\n",
    "required_cols = ['total_assets', 'debt_total']\n",
    "missing_required = [c for c in required_cols if c not in df.columns]\n",
    "if missing_required:\n",
    "    raise KeyError(f\"Missing required columns: {missing_required}\")\n",
    "\n",
    "df['assets_usd'] = pd.to_numeric(df['total_assets'], errors='coerce') * MM\n",
    "df['debt_usd'] = pd.to_numeric(df['debt_total'], errors='coerce') * MM\n",
    "df['be_usd'] = (df['total_assets'] - df['debt_total']) * MM\n",
    "\n",
    "# Market equity proxies\n",
    "df['E_pb'] = np.where(\n",
    "    (df['price_to_book_value_per_share'] > 0) & (df['be_usd'] > 0),\n",
    "    df['price_to_book_value_per_share'] * df['be_usd'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df['E_de'] = np.where(\n",
    "    (df['d/e'] > 0) & (df['debt_usd'] > 0),\n",
    "    df['debt_usd'] / df['d/e'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "equity_weight = pd.to_numeric(\n",
    "    df.get('wacc_equity_weight', pd.Series(index=df.index, dtype=float)),\n",
    "    errors='coerce'\n",
    ")\n",
    "debt_weight = pd.to_numeric(\n",
    "    df.get('wacc_debt_weight', pd.Series(index=df.index, dtype=float)),\n",
    "    errors='coerce'\n",
    ")\n",
    "tolerance = 1e-3\n",
    "weights_sum = equity_weight + debt_weight\n",
    "wacc_mask = (\n",
    "    (equity_weight > 0)\n",
    "    & (debt_weight > 0)\n",
    "    & (np.abs(weights_sum - 1) <= tolerance)\n",
    ")\n",
    "\n",
    "df['E_wacc'] = np.where(\n",
    "    wacc_mask,\n",
    "    df['assets_usd'] * equity_weight,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Prioritise proxies: price-to-book, then D/E, then WACC\n",
    "values = []\n",
    "sources = []\n",
    "for _, row in df[['E_pb', 'E_de', 'E_wacc']].iterrows():\n",
    "    value = np.nan\n",
    "    source = 'missing'\n",
    "    for key in ['E_pb', 'E_de', 'E_wacc']:\n",
    "        v = row[key]\n",
    "        if pd.notna(v) and v > 0:\n",
    "            value = v\n",
    "            source = key\n",
    "            break\n",
    "    values.append(value)\n",
    "    sources.append(source)\n",
    "\n",
    "df['E'] = values\n",
    "df['E_source'] = sources\n",
    "\n",
    "df['weak_E_proxy'] = df['E_source'].isin(['E_de', 'E_wacc'])\n",
    "\n",
    "df['F'] = df['debt_usd']\n",
    "\n",
    "print(df[['instrument', 'year', 'be_usd', 'E', 'E_source', 'F']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271d387",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Equity volatility proxy with rolling window, imputation, and winsorisation\n",
    "\n",
    "Following the reference guidance, compute a three-year rolling standard deviation of annual equity returns, fall back to size-bucket medians when the rolling window lacks sufficient history, and winsorise the result at the 1st and 99th percentiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb22799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['instrument', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Size buckets from dummy indicators (default to 'small')\n",
    "size_bucket = np.select(\n",
    "    [df.get('dummylarge', 0) == 1, df.get('dummymid', 0) == 1],\n",
    "    ['large', 'mid'],\n",
    "    default='small'\n",
    ")\n",
    "df['size_bucket'] = size_bucket\n",
    "\n",
    "# Rolling volatility of prior returns\n",
    "def rolling_sigma(series: pd.Series) -> pd.Series:\n",
    "    shifted = series.shift(1)\n",
    "    return shifted.rolling(window=3, min_periods=2).std()\n",
    "\n",
    "def rolling_count(series: pd.Series) -> pd.Series:\n",
    "    shifted = series.shift(1)\n",
    "    return shifted.rolling(window=3, min_periods=1).count()\n",
    "\n",
    "df['sigma_E_raw'] = (\n",
    "    df.groupby('instrument', group_keys=False)['rit']\n",
    "      .apply(rolling_sigma)\n",
    ")\n",
    "\n",
    "df['sigma_E_count'] = (\n",
    "    df.groupby('instrument', group_keys=False)['rit']\n",
    "      .apply(rolling_count)\n",
    ")\n",
    "\n",
    "df['insufficient_returns'] = df['sigma_E_count'] < 2\n",
    "\n",
    "# Impute using size bucket medians, then overall median\n",
    "bucket_median = (\n",
    "    df.groupby('size_bucket')['sigma_E_raw']\n",
    "      .transform('median')\n",
    ")\n",
    "df['sigma_E'] = df['sigma_E_raw'].copy()\n",
    "mask_impute = df['sigma_E'].isna()\n",
    "df.loc[mask_impute, 'sigma_E'] = bucket_median[mask_impute]\n",
    "\n",
    "overall_median = df['sigma_E'].median()\n",
    "df['sigma_E'] = df['sigma_E'].fillna(overall_median)\n",
    "\n",
    "df['imputed_sigmaE_sizebucket'] = mask_impute & df['sigma_E'].notna()\n",
    "\n",
    "df['sigma_E'] = winsorize_series(df['sigma_E'], 0.01, 0.99)\n",
    "\n",
    "df['sigma_E'] = df['sigma_E'].clip(lower=1e-6)\n",
    "\n",
    "print(df[['instrument', 'year', 'sigma_E', 'insufficient_returns', 'imputed_sigmaE_sizebucket']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4058966",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Debt volatility proxy, asset proxies, and drift proxy\n",
    "\n",
    "Derive the Bharath–Shumway debt volatility proxy, approximate asset value/volatility, and compute the drift proxy based on lagged equity returns with firm and size-bucket fallbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa142b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debt and asset volatility proxies\n",
    "df['sigma_D_hat'] = 0.05 + 0.25 * df['sigma_E']\n",
    "df['V_hat'] = df['E'] + df['F']\n",
    "valid_v = df['V_hat'] > 0\n",
    "\n",
    "sigma_V_components = np.where(\n",
    "    valid_v,\n",
    "    (df['E'] / df['V_hat']) * df['sigma_E'] + (df['F'] / df['V_hat']) * df['sigma_D_hat'],\n",
    "    np.nan\n",
    ")\n",
    "df['sigma_V_hat'] = sigma_V_components\n",
    "\n",
    "df['sigma_V_hat'] = df['sigma_V_hat'].clip(lower=1e-6)\n",
    "\n",
    "# Drift proxy using lagged returns\n",
    "lagged_rit = df.groupby('instrument', group_keys=False)['rit'].shift(1)\n",
    "firm_mean = (\n",
    "    df.groupby('instrument', group_keys=False)['rit']\n",
    "      .apply(lambda s: s.expanding().mean().shift(1))\n",
    ")\n",
    "\n",
    "df['mu_hat'] = lagged_rit\n",
    "mask_mu = df['mu_hat'].isna()\n",
    "df.loc[mask_mu, 'mu_hat'] = firm_mean[mask_mu]\n",
    "\n",
    "size_median_mu = df.groupby('size_bucket')['mu_hat'].transform('median')\n",
    "df['mu_hat'] = df['mu_hat'].fillna(size_median_mu)\n",
    "df['mu_hat'] = df['mu_hat'].fillna(df['mu_hat'].median())\n",
    "\n",
    "print(df[['instrument', 'year', 'sigma_D_hat', 'sigma_V_hat', 'mu_hat']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4440792",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Compute naive distance to default (DD) and probability of default (PD)\n",
    "\n",
    "Apply the Bharath–Shumway naive formulas using the proxies above. Probability of default is clipped to the [0, 1] interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a36201",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sigmaV = np.isfinite(df['sigma_V_hat']) & (df['sigma_V_hat'] > 0)\n",
    "valid_inputs = df['E'].gt(0) & df['F'].gt(0) & valid_sigmaV & df['mu_hat'].notna()\n",
    "\n",
    "# Bharath & Shumway naive DD: V_hat=E+F, sigma_D_hat=0.05+0.25*sigma_E,\n",
    "# sigma_V_hat = value-weighted mix, mu_hat = lagged equity return. No solver.\n",
    "df['DD_naive'] = np.where(\n",
    "    valid_inputs,\n",
    "    (np.log(df['V_hat'] / df['F']) + (df['mu_hat'] - 0.5 * df['sigma_V_hat'] ** 2) * T)\n",
    "    / (df['sigma_V_hat'] * math.sqrt(T)),\n",
    "    np.nan,\n",
    ")\n",
    "df['PD_naive'] = np.where(np.isfinite(df['DD_naive']), Phi(-df['DD_naive']), np.nan)\n",
    "df['invalid_sigmaV'] = ~valid_sigmaV\n",
    "\n",
    "print(df[['instrument', 'year', 'DD_naive', 'PD_naive']].head())\n",
    "print(f\"PD==0 count: {(df['PD_naive'] == 0).sum()}, PD==1 count: {(df['PD_naive'] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf02f4d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Data-quality flags and status tracking\n",
    "\n",
    "Capture the first applicable status flag (missing inputs, fallbacks, or imputations) as `naive_status` so downstream users understand how each observation was derived.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_E = ~np.isfinite(df['E'])\n",
    "missing_F = ~np.isfinite(df['F'])\n",
    "nonpos_EF = (df['E'] <= 0) | (df['F'] <= 0)\n",
    "\n",
    "flag_specs = [\n",
    "    ('invalid_sigmaV', df['invalid_sigmaV']),\n",
    "    ('missing_E', missing_E | (df['E_source'] == 'missing')),\n",
    "    ('missing_F', missing_F),\n",
    "    ('nonpos_EF', nonpos_EF),\n",
    "    ('insufficient_returns', df['insufficient_returns']),\n",
    "    ('imputed_sigmaE_sizebucket', df['imputed_sigmaE_sizebucket']),\n",
    "    ('fallback_E_from_de', df['E_source'] == 'E_de'),\n",
    "    ('fallback_E_from_wacc', df['E_source'] == 'E_wacc'),\n",
    "]\n",
    "\n",
    "for name, mask in flag_specs:\n",
    "    df[name] = mask.astype(bool)\n",
    "\n",
    "\n",
    "def assign_status(idx: int) -> str:\n",
    "    for name, _ in flag_specs:\n",
    "        if bool(df.iloc[idx][name]):\n",
    "            return name\n",
    "    return 'ok'\n",
    "\n",
    "\n",
    "naive_status = [assign_status(i) for i in range(len(df))]\n",
    "df['naive_status'] = naive_status\n",
    "\n",
    "status_counts = df['naive_status'].value_counts(dropna=False).sort_index()\n",
    "print('Naive status counts:')\n",
    "print(status_counts)\n",
    "\n",
    "fallback_summary = {\n",
    "    'invalid_sigmaV': int(df['invalid_sigmaV'].sum()),\n",
    "    'fallback_E_from_de': int(df['fallback_E_from_de'].sum()),\n",
    "    'fallback_E_from_wacc': int(df['fallback_E_from_wacc'].sum()),\n",
    "    'imputed_sigmaE_sizebucket': int(df['imputed_sigmaE_sizebucket'].sum()),\n",
    "    'insufficient_returns': int(df['insufficient_returns'].sum()),\n",
    "}\n",
    "print(\"\\nFallback indicator counts:\")\n",
    "for label, count in fallback_summary.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "e_source_counts = df['E_source'].value_counts(dropna=False).sort_index()\n",
    "print(\"\\nEquity source mix:\")\n",
    "print(e_source_counts)\n",
    "weak_proxy_count = int(df['weak_E_proxy'].sum())\n",
    "print(f\"Weak equity proxy count: {weak_proxy_count}\")\n",
    "\n",
    "percentiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "dd_stats = df['DD_naive'].describe(percentiles=percentiles)\n",
    "pd_stats = df['PD_naive'].describe(percentiles=percentiles)\n",
    "dd_missing = int(df['DD_naive'].isna().sum())\n",
    "pd_missing = int(df['PD_naive'].isna().sum())\n",
    "\n",
    "print(\"\\nDD_naive summary:\")\n",
    "print(dd_stats)\n",
    "print(f\"Rows with missing DD_naive: {dd_missing}\")\n",
    "\n",
    "print(\"\\nPD_naive summary:\")\n",
    "print(pd_stats)\n",
    "print(f\"Rows with missing PD_naive: {pd_missing}\")\n",
    "\n",
    "log_lines = []\n",
    "log_lines.append('=== Naive DD/PD Diagnostics ===')\n",
    "log_lines.append(f'Total rows processed: {len(df)}')\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Naive status counts:')\n",
    "log_lines.extend([f\"{status}: {count}\" for status, count in status_counts.items()])\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Fallback indicator counts:')\n",
    "for label, count in fallback_summary.items():\n",
    "    log_lines.append(f\"{label}: {count}\")\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Equity source mix:')\n",
    "log_lines.extend([f\"{source}: {count}\" for source, count in e_source_counts.items()])\n",
    "log_lines.append('')\n",
    "log_lines.append(f\"Weak equity proxy count: {weak_proxy_count}\")\n",
    "\n",
    "log_lines.append('DD_naive summary:')\n",
    "log_lines.extend([f\"{idx}: {value}\" for idx, value in dd_stats.items()])\n",
    "log_lines.append(f'Rows with missing DD_naive: {dd_missing}')\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('PD_naive summary:')\n",
    "log_lines.extend([f\"{idx}: {value}\" for idx, value in pd_stats.items()])\n",
    "log_lines.append(f'Rows with missing PD_naive: {pd_missing}')\n",
    "log_lines.append(f\"PD==0 count: {(df['PD_naive'] == 0).sum()}, PD==1 count: {(df['PD_naive'] == 1).sum()}\")\n",
    "\n",
    "log_path = log_dir / 'dd_pd_accounting_log.txt'\n",
    "log_path.write_text(\"\\n\".join(log_lines))\n",
    "print(f\"[INFO] Wrote diagnostics to {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63725a0c",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Persist outputs and quick diagnostics\n",
    "\n",
    "Save the naive DD/PD results and a percentile summary by year for quick reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = [\n",
    "    'instrument', 'year', 'E', 'E_source', 'weak_E_proxy', 'E_pb', 'E_de', 'E_wacc',\n",
    "    'F', 'sigma_E', 'sigma_D_hat', 'sigma_V_hat', 'mu_hat',\n",
    "    'DD_naive', 'PD_naive', 'naive_status'\n",
    "]\n",
    "\n",
    "dd_output = output_dir / 'dd_pd_naive.csv'\n",
    "df[result_cols].to_csv(dd_output, index=False)\n",
    "print(f\"[INFO] Saved naive DD/PD results to {dd_output}\")\n",
    "\n",
    "cfg = {'T': T, 'ROLL_YEARS': 3, 'WINSOR_P': [0.01, 0.99], 'Phi': 'scipy' if 'norm' in globals() else 'erf_fallback', 'spec': 'Bharath–Shumway naive, no solver, v1'}\n",
    "(Path(output_dir) / 'dd_pd_naive_config.json').write_text(pd.Series(cfg).to_json())\n",
    "\n",
    "percentiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "percentile_columns = [f\"p{int(p * 100)}\" for p in percentiles]\n",
    "\n",
    "\n",
    "def build_percentile_table(metric: str) -> pd.DataFrame:\n",
    "    clean = df[['year', metric]].dropna()\n",
    "    if clean.empty:\n",
    "        empty = {'year': ['overall'], 'metric': [metric]}\n",
    "        for col in percentile_columns:\n",
    "            empty[col] = [np.nan]\n",
    "        return pd.DataFrame(empty)\n",
    "\n",
    "    by_year = (\n",
    "        clean.groupby('year')[metric]\n",
    "             .quantile(percentiles)\n",
    "             .unstack(level=-1)\n",
    "    )\n",
    "    by_year.columns = percentile_columns\n",
    "    by_year = by_year.reset_index()\n",
    "    by_year.insert(0, 'metric', metric)\n",
    "\n",
    "    overall_values = df[metric].dropna()\n",
    "    overall_row = {'metric': metric, 'year': 'overall'}\n",
    "    for col, pct in zip(percentile_columns, percentiles):\n",
    "        overall_row[col] = overall_values.quantile(pct) if not overall_values.empty else np.nan\n",
    "\n",
    "    combined = pd.concat([by_year, pd.DataFrame([overall_row])], ignore_index=True)\n",
    "    ordered_columns = ['year', 'metric', *percentile_columns]\n",
    "    return combined[ordered_columns]\n",
    "\n",
    "\n",
    "summary_frames = [build_percentile_table(metric) for metric in ['DD_naive', 'PD_naive']]\n",
    "summary = pd.concat(summary_frames, ignore_index=True)\n",
    "\n",
    "summary_output = output_dir / 'dd_pd_naive_summary.csv'\n",
    "summary.to_csv(summary_output, index=False)\n",
    "print(f\"[INFO] Saved percentile summary to {summary_output}\")\n",
    "\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96127a25",
   "metadata": {},
   "source": [
    "# Diagnostic visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_plot_df = df.dropna(subset=['DD_naive', 'year']).copy()\n",
    "if dd_plot_df.empty:\n",
    "    print('No DD_naive data available for plotting.')\n",
    "else:\n",
    "    dd_plot_df['year'] = dd_plot_df['year'].astype(str)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    dd_plot_df.boxplot(column='DD_naive', by='year', ax=ax)\n",
    "    ax.set_title('DD_naive distribution by year')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('DD_naive')\n",
    "    fig.suptitle('')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71188d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_path = output_dir / 'dd_pd_market.csv'\n",
    "if not market_path.exists():\n",
    "    print('Market DD file not found; skipping DDm vs. DD_naive scatter.')\n",
    "else:\n",
    "    market_df = pd.read_csv(market_path)\n",
    "    required_cols = {'instrument', 'year', 'DDm'}\n",
    "    if not required_cols.issubset(market_df.columns):\n",
    "        print('Market data lacks DDm column; skipping scatter plot.')\n",
    "    else:\n",
    "        merged = (\n",
    "            df[['instrument', 'year', 'DD_naive']]\n",
    "            .merge(market_df[['instrument', 'year', 'DDm']], on=['instrument', 'year'], how='inner')\n",
    "            .dropna(subset=['DD_naive', 'DDm'])\n",
    "        )\n",
    "        if merged.empty:\n",
    "            print('No overlapping DDm/DD_naive observations to plot.')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.scatter(merged['DDm'], merged['DD_naive'], alpha=0.6, edgecolors='none')\n",
    "            all_vals = np.concatenate([merged['DDm'].to_numpy(), merged['DD_naive'].to_numpy()])\n",
    "            lims = [all_vals.min(), all_vals.max()]\n",
    "            ax.plot(lims, lims, linestyle='--', color='gray', linewidth=1)\n",
    "            ax.set_xlabel('DDm (market)')\n",
    "            ax.set_ylabel('DD_naive (accounting)')\n",
    "            ax.set_title('DDm vs. DD_naive comparison')\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
