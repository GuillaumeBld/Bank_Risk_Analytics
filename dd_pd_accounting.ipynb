{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01969415",
   "metadata": {},
   "source": [
    "\n",
    "# Accounting-approach, Naive DD per Bharath and Shumway (2008). No solver.\n",
    "\n",
    "Implements Bharath and Shumway naive DD. \n",
    "Uses $\\hat V = E + F$,\n",
    "$\\hat\\sigma_D = 0.05 + 0.25 \\sigma_E$,\n",
    "value-weighted $\\hat\\sigma_V$,\n",
    "and $\\hat\\mu = r_{i,t\u22121}$. No solver.\n",
    "\n",
    "This notebook follows the Naive DD approach issue from Bharath and Shumway (2008) which we call the accounting-approach distance-to-default (DD) workflow as it involves the use of proxies for the default intensity and volatility, without invoking a numerical solver. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ad8dc",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "Install the minimal dependencies required to reproduce the accounting workflow locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7bc248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guillaumebld/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install needed packages (run once per environment)\n",
    "%pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40de7ce",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports, configuration, and helper utilities\n",
    "\n",
    "Load core libraries, set display defaults, and define helper functions used throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7adc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank\n",
      "Accounting input -> FOUND\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from scipy.stats import norm\n",
    "    Phi = norm.cdf\n",
    "except Exception:\n",
    "    from math import erf\n",
    "    def Phi(x):  # normal CDF fallback\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return 0.5*(1.0 + np.vectorize(erf)(x/np.sqrt(2.0)))\n",
    "\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "\n",
    "MM = 1_000_000.0\n",
    "T = 1.0\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path, marker: str = '.git') -> Path:\n",
    "    \"\"\"Walk up from *start* until a directory containing *marker* is found.\"\"\"\n",
    "    current = start.resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "\n",
    "def winsorize_series(series: pd.Series, lower: float = 0.01, upper: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Clip *series* to the given quantile range, ignoring NaNs.\"\"\"\n",
    "    clean = series.dropna()\n",
    "    if clean.empty:\n",
    "        return series\n",
    "    ql, qh = np.nanpercentile(clean, [lower * 100, upper * 100])\n",
    "    if not np.isfinite(ql) or not np.isfinite(qh) or ql > qh:\n",
    "        return series\n",
    "    return series.clip(ql, qh)\n",
    "\n",
    "\n",
    "base_dir = find_repo_root(Path.cwd())\n",
    "print(f\"Repository root: {base_dir}\")\n",
    "\n",
    "model_fp   = base_dir / 'data' / 'clean' / 'Book2_clean.csv'\n",
    "output_dir = base_dir / 'data' / 'outputs' / 'datasheet'\n",
    "log_dir    = base_dir / 'data' / 'logs'\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Accounting input -> {'FOUND' if model_fp.exists() else 'MISSING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d6c37",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Load and normalise accounting inputs\n",
    "\n",
    "Read the accounting file, harmonise column names, and enforce expected data types for key fields such as instrument, year, and balance sheet magnitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2512288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading accounting data\u2026\n",
      "\u2192 1425 rows before cleaning\n",
      "[WARN] rit outside [-1,1]. Ensure rit is a decimal return, not percent.\n",
      "  instrument  year  total_assets  debt_total\n",
      "0        JPM  2016     2490972.0    495354.0\n",
      "1        JPM  2017     2533600.0    494798.0\n",
      "2        JPM  2018     2622532.0    533627.0\n",
      "3        JPM  2019     2687379.0    516093.0\n",
      "4        JPM  2020     3384757.0    542102.0\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Loading accounting data\u2026')\n",
    "df_raw = pd.read_csv(model_fp)\n",
    "print(f\"\u2192 {df_raw.shape[0]} rows before cleaning\")\n",
    "\n",
    "# Standardise column names we rely on\n",
    "rename_map = {\n",
    "    'nstrument': 'instrument',\n",
    "    'weighted_average_cost_of_capital,_(%)': 'wacc_pct',\n",
    "    'wacc_tax_rate,_(%)': 'wacc_tax_rate_pct',\n",
    "    'wacc_cost_of_debt,_(%)': 'wacc_cost_of_debt_pct',\n",
    "    'wacc_debt_weight,_(%)': 'wacc_debt_weight_pct',\n",
    "    'wacc_equity_weight,_(%)': 'wacc_equity_weight_pct',\n",
    "}\n",
    "\n",
    "df = df_raw.rename(columns=rename_map).copy()\n",
    "\n",
    "# Drop columns that are clearly placeholders\n",
    "unnamed_cols = [c for c in df.columns if c.lower().startswith('unnamed')]\n",
    "if unnamed_cols:\n",
    "    df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "# Instrument string cleanup\n",
    "if 'instrument' in df.columns:\n",
    "    df['instrument'] = (df['instrument']\n",
    "                        .astype(str)\n",
    "                        .str.strip()\n",
    "                        .str.replace('\"', '', regex=False)\n",
    "                        .str.upper())\n",
    "else:\n",
    "    raise KeyError('`instrument` column not found after renaming.')\n",
    "\n",
    "# Year as integer panel key\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Numeric conversions for balance sheet figures\n",
    "for col in ['total_assets', 'debt_total', 'price_to_book_value_per_share', 'd/e', 'rit', 'rit_rf', 'new_wacc']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convert WACC weights from percentages to decimals when present\n",
    "for src, dest in [('wacc_equity_weight_pct', 'wacc_equity_weight'),\n",
    "                  ('wacc_debt_weight_pct', 'wacc_debt_weight')]:\n",
    "    if src in df.columns:\n",
    "        df[dest] = pd.to_numeric(df[src], errors='coerce') / 100.0\n",
    "\n",
    "assert (df['total_assets'].dropna() >= 0).all() and (df['debt_total'].dropna() >= 0).all(), 'Assets/debt must be nonnegative'\n",
    "if not df['rit'].between(-1, 1).all(skipna=True):\n",
    "    print('[WARN] rit outside [-1,1]. Ensure rit is a decimal return, not percent.')\n",
    "\n",
    "print(df[['instrument', 'year', 'total_assets', 'debt_total']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6051a16",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Construct book equity and market equity proxies\n",
    "\n",
    "Compute book equity along with the three Bharath\u2013Shumway equity proxies (price-to-book, D/E, and WACC weights). Select the best available proxy and record its source for auditability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b088650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instrument  year        be_usd             E E_source             F\n",
      "0        JPM  2016  1.995618e+12  2.688226e+12     E_pb  4.953540e+11\n",
      "1        JPM  2017  2.038802e+12  3.252329e+12     E_pb  4.947980e+11\n",
      "2        JPM  2018  2.088905e+12  2.898674e+12     E_pb  5.336270e+11\n",
      "3        JPM  2019  2.171286e+12  3.983422e+12     E_pb  5.160930e+11\n",
      "4        JPM  2020  2.842655e+12  4.418551e+12     E_pb  5.421020e+11\n"
     ]
    }
   ],
   "source": [
    "# Book equity in USD\n",
    "required_cols = ['total_assets', 'debt_total']\n",
    "# For banks: debt_total represents total liabilities\n",
    "missing_required = [c for c in required_cols if c not in df.columns]\n",
    "if missing_required:\n",
    "    raise KeyError(f\"Missing required columns: {missing_required}\")\n",
    "\n",
    "df['assets_usd'] = pd.to_numeric(df['total_assets'], errors='coerce') * MM\n",
    "df['debt_usd'] = pd.to_numeric(df['debt_total'], errors='coerce') * MM\n",
    "df['be_usd'] = (df['total_assets'] - df['debt_total']) * MM\n",
    "\n",
    "# Market equity proxies\n",
    "df['E_pb'] = np.where(\n",
    "    (df['price_to_book_value_per_share'] > 0) & (df['be_usd'] > 0),\n",
    "    df['price_to_book_value_per_share'] * df['be_usd'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df['E_de'] = np.where(\n",
    "    (df['d/e'] > 0) & (df['debt_usd'] > 0),\n",
    "    df['debt_usd'] / df['d/e'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "equity_weight = pd.to_numeric(\n",
    "    df.get('wacc_equity_weight', pd.Series(index=df.index, dtype=float)),\n",
    "    errors='coerce'\n",
    ")\n",
    "debt_weight = pd.to_numeric(\n",
    "    df.get('wacc_debt_weight', pd.Series(index=df.index, dtype=float)),\n",
    "    errors='coerce'\n",
    ")\n",
    "tolerance = 1e-3\n",
    "weights_sum = equity_weight + debt_weight\n",
    "wacc_mask = (\n",
    "    (equity_weight > 0)\n",
    "    & (debt_weight > 0)\n",
    "    & (np.abs(weights_sum - 1) <= tolerance)\n",
    ")\n",
    "\n",
    "df['E_wacc'] = np.where(\n",
    "    wacc_mask,\n",
    "    df['assets_usd'] * equity_weight,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Prioritise proxies: price-to-book, then D/E, then WACC\n",
    "values = []\n",
    "sources = []\n",
    "for _, row in df[['E_pb', 'E_de', 'E_wacc']].iterrows():\n",
    "    value = np.nan\n",
    "    source = 'missing'\n",
    "    for key in ['E_pb', 'E_de', 'E_wacc']:\n",
    "        v = row[key]\n",
    "        if pd.notna(v) and v > 0:\n",
    "            value = v\n",
    "            source = key\n",
    "            break\n",
    "    values.append(value)\n",
    "    sources.append(source)\n",
    "\n",
    "df['E'] = values\n",
    "df['E_source'] = sources\n",
    "\n",
    "df['weak_E_proxy'] = df['E_source'].isin(['E_de', 'E_wacc'])\n",
    "\n",
    "df['F'] = df['debt_usd']\n",
    "\n",
    "print(df[['instrument', 'year', 'be_usd', 'E', 'E_source', 'F']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271d387",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Equity volatility proxy with rolling window, imputation, and winsorisation\n",
    "\n",
    "Following the reference guidance, compute a three-year rolling standard deviation of annual equity returns, fall back to size-bucket medians when the rolling window lacks sufficient history, and winsorise the result at the 1st and 99th percentiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_tagging_accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME INDEXING: Create explicit time-tagged columns\n",
    "print('[INFO] Creating time-tagged columns for accounting approach...')\n",
    "\n",
    "# Explicit time tags for inputs at t\n",
    "df['E_t'] = df['E']  # Market equity proxy\n",
    "df['F_t'] = df['debt_usd']  # Barrier in USD\n",
    "df['T'] = 1.0\n",
    "\n",
    "# Build sigma_E_tminus1 using only past data\n",
    "print('[INFO] Computing sigma_E_tminus1 from historical returns only...')\n",
    "df = df.sort_values(['instrument', 'year'])\n",
    "\n",
    "def rolling_sigma_prior(s):\n",
    "    \"\"\"Compute rolling std using only prior data (shift(1)).\"\"\"\n",
    "    return s.shift(1).rolling(3, min_periods=2).std()\n",
    "\n",
    "# Compute sigma_E using returns up to t-1 only\n",
    "df['sigma_E_tminus1'] = df.groupby('instrument', group_keys=False)['rit'].apply(rolling_sigma_prior)\n",
    "\n",
    "# Track window provenance\n",
    "df['sigmaE_count'] = df.groupby('instrument', group_keys=False)['rit'].apply(\n",
    "    lambda s: s.shift(1).rolling(3, min_periods=1).count()\n",
    ")\n",
    "df['sigmaE_window_end_year'] = df['year'] - 1\n",
    "df['sigmaE_window_start_year'] = df['year'] - df['sigmaE_count'].clip(upper=3).fillna(0).astype(int)\n",
    "\n",
    "# Use sigma_E_tminus1 for calculations\n",
    "df['sigma_E'] = df['sigma_E_tminus1']\n",
    "\n",
    "print(f'  sigma_E_tminus1: {df[\"sigma_E_tminus1\"].notna().sum()} non-null values')\n",
    "print(f'  Window end year: always t-1 = {(df[\"sigmaE_window_end_year\"] == df[\"year\"] - 1).all()}')\n",
    "\n",
    "# Build mu_hat_t = r_{i,t-1} with provenance tracking\n",
    "print('[INFO] Computing mu_hat_t = r_{i,t-1} with fallbacks...')\n",
    "\n",
    "df['mu_hat_from'] = 'rit_tminus1'\n",
    "df['mu_hat'] = df.groupby('instrument', group_keys=False)['rit'].shift(1)\n",
    "df['mu_source_year'] = df['year'] - 1\n",
    "\n",
    "# Fallback 1: firm mean up to t-1\n",
    "need = df['mu_hat'].isna()\n",
    "df.loc[need, 'mu_hat'] = df.groupby('instrument', group_keys=False)['rit'].apply(\n",
    "    lambda s: s.expanding().mean().shift(1)\n",
    ")\n",
    "df.loc[need & df['mu_hat'].notna(), 'mu_hat_from'] = 'firm_mean_tminus1'\n",
    "df.loc[need & df['mu_hat'].notna(), 'mu_source_year'] = df['year'] - 1\n",
    "\n",
    "# Fallback 2: size bucket median\n",
    "if 'size_bucket' in df.columns:\n",
    "    need2 = df['mu_hat'].isna()\n",
    "    df.loc[need2, 'mu_hat'] = df.groupby('size_bucket')['mu_hat'].transform('median')\n",
    "    df.loc[need2 & df['mu_hat'].notna(), 'mu_hat_from'] = 'size_median'\n",
    "    df.loc[need2 & df['mu_hat'].notna(), 'mu_source_year'] = np.nan\n",
    "\n",
    "print(f'  mu_hat sources:')\n",
    "print(df['mu_hat_from'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_assertions_accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME INTEGRITY ASSERTIONS\n",
    "print('[INFO] Validating time integrity for accounting approach...')\n",
    "\n",
    "# Import time checks\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from utils.time_checks import assert_time_integrity\n",
    "\n",
    "# Assertion 1: sigma_E window must end at t-1\n",
    "assert (df['sigmaE_window_end_year'] == df['year'] - 1).all(), \\\n",
    "    'sigma_E window end must be t-1 (no lookahead)'\n",
    "\n",
    "# Assertion 2: mu_hat provenance is set\n",
    "assert df['mu_hat_from'].notna().all(), 'mu_hat provenance must be tracked'\n",
    "\n",
    "# Assertion 3: When using rit_tminus1, source year must be t-1\n",
    "uses_lag = df['mu_hat_from'].eq('rit_tminus1')\n",
    "if uses_lag.any():\n",
    "    assert (df.loc[uses_lag, 'mu_source_year'] == df.loc[uses_lag, 'year'] - 1).all(), \\\n",
    "        'mu_hat source year must be t-1 when using lagged return'\n",
    "\n",
    "# Run comprehensive time integrity check\n",
    "assert_time_integrity(df)\n",
    "\n",
    "print('[PASS] All time integrity assertions passed')\n",
    "print(f'  - sigma_E uses only data up to t-1: {(df[\"sigmaE_window_end_year\"] == df[\"year\"] - 1).all()}')\n",
    "print(f'  - mu_hat uses t-1 data: {(df[df[\"mu_hat_from\"].eq(\"rit_tminus1\")][\"mu_source_year\"] == df[df[\"mu_hat_from\"].eq(\"rit_tminus1\")][\"year\"] - 1).all()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb22799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instrument  year   sigma_E  insufficient_returns  imputed_sigmaE_sizebucket\n",
      "0       ABCB  2016  0.191261                  True                       True\n",
      "1       ABCB  2017  0.191261                  True                       True\n",
      "2       ABCB  2018  0.086767                 False                      False\n",
      "3       ABCB  2019  0.383442                 False                      False\n",
      "4       ABCB  2020  0.301277                 False                      False\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(['instrument', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Size buckets from dummy indicators (default to 'small')\n",
    "size_bucket = np.select(\n",
    "    [df.get('dummylarge', 0) == 1, df.get('dummymid', 0) == 1],\n",
    "    ['large', 'mid'],\n",
    "    default='small'\n",
    ")\n",
    "df['size_bucket'] = size_bucket\n",
    "\n",
    "# Rolling volatility of prior returns\n",
    "def rolling_sigma(series: pd.Series) -> pd.Series:\n",
    "    shifted = series.shift(1)\n",
    "    return shifted.rolling(window=3, min_periods=2).std()\n",
    "\n",
    "def rolling_count(series: pd.Series) -> pd.Series:\n",
    "    shifted = series.shift(1)\n",
    "    return shifted.rolling(window=3, min_periods=1).count()\n",
    "\n",
    "df['sigma_E_raw'] = (\n",
    "    df.groupby('instrument', group_keys=False)['rit']\n",
    "      .apply(rolling_sigma)\n",
    ")\n",
    "\n",
    "df['sigma_E_count'] = (\n",
    "    df.groupby('instrument', group_keys=False)['rit']\n",
    "      .apply(rolling_count)\n",
    ")\n",
    "\n",
    "df['insufficient_returns'] = df['sigma_E_count'] < 2\n",
    "\n",
    "# Impute using size bucket medians, then overall median\n",
    "bucket_median = (\n",
    "    df.groupby('size_bucket')['sigma_E_raw']\n",
    "      .transform('median')\n",
    ")\n",
    "df['sigma_E'] = df['sigma_E_raw'].copy()\n",
    "mask_impute = df['sigma_E'].isna()\n",
    "df.loc[mask_impute, 'sigma_E'] = bucket_median[mask_impute]\n",
    "\n",
    "overall_median = df['sigma_E'].median()\n",
    "df['sigma_E'] = df['sigma_E'].fillna(overall_median)\n",
    "\n",
    "df['imputed_sigmaE_sizebucket'] = mask_impute & df['sigma_E'].notna()\n",
    "\n",
    "df['sigma_E'] = winsorize_series(df['sigma_E'], 0.01, 0.99)\n",
    "\n",
    "df['sigma_E'] = df['sigma_E'].clip(lower=1e-6)\n",
    "\n",
    "print(df[['instrument', 'year', 'sigma_E', 'insufficient_returns', 'imputed_sigmaE_sizebucket']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4058966",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Debt volatility proxy, asset proxies, and drift proxy\n",
    "\n",
    "Derive the Bharath\u2013Shumway debt volatility proxy, approximate asset value/volatility, and compute the drift proxy based on lagged equity returns with firm and size-bucket fallbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa142b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instrument  year  sigma_D_hat  sigma_V_hat    mu_hat\n",
      "0       ABCB  2016     0.097815     0.187433  0.044898\n",
      "1       ABCB  2017     0.097815     0.189255  0.366828\n",
      "2       ABCB  2018     0.071692     0.086434  0.244121\n",
      "3       ABCB  2019     0.145861     0.366377 -0.350111\n",
      "4       ABCB  2020     0.125319     0.296447  0.033435\n"
     ]
    }
   ],
   "source": [
    "# Debt and asset volatility proxies\n",
    "df['sigma_D_hat'] = 0.05 + 0.25 * df['sigma_E']\n",
    "df['V_hat'] = df['E'] + df['F']\n",
    "valid_v = df['V_hat'] > 0\n",
    "\n",
    "sigma_V_components = np.where(\n",
    "    valid_v,\n",
    "    (df['E'] / df['V_hat']) * df['sigma_E'] + (df['F'] / df['V_hat']) * df['sigma_D_hat'],\n",
    "    np.nan\n",
    ")\n",
    "df['sigma_V_hat'] = sigma_V_components\n",
    "\n",
    "df['sigma_V_hat'] = df['sigma_V_hat'].clip(lower=1e-6)\n",
    "\n",
    "# Drift proxy using lagged returns\n",
    "lagged_rit = df.groupby('instrument', group_keys=False)['rit'].shift(1)\n",
    "firm_mean = (\n",
    "    df.groupby('instrument', group_keys=False)['rit']\n",
    "      .apply(lambda s: s.expanding().mean().shift(1))\n",
    ")\n",
    "\n",
    "df['mu_hat'] = lagged_rit\n",
    "mask_mu = df['mu_hat'].isna()\n",
    "df.loc[mask_mu, 'mu_hat'] = firm_mean[mask_mu]\n",
    "\n",
    "size_median_mu = df.groupby('size_bucket')['mu_hat'].transform('median')\n",
    "df['mu_hat'] = df['mu_hat'].fillna(size_median_mu)\n",
    "df['mu_hat'] = df['mu_hat'].fillna(df['mu_hat'].median())\n",
    "\n",
    "print(df[['instrument', 'year', 'sigma_D_hat', 'sigma_V_hat', 'mu_hat']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4440792",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Compute naive distance to default (DD) and probability of default (PD)\n",
    "\n",
    "Apply the Bharath\u2013Shumway naive formulas using the proxies above. Probability of default is clipped to the [0, 1] interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a36201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instrument  year   DD_naive       PD_naive\n",
      "0       ABCB  2016  17.191718   1.531595e-66\n",
      "1       ABCB  2017  22.140950  6.375977e-109\n",
      "2       ABCB  2018  46.873017   0.000000e+00\n",
      "3       ABCB  2019   6.049042   7.285482e-10\n",
      "4       ABCB  2020  12.092687   5.772411e-34\n",
      "PD==0 count: 114, PD==1 count: 0\n"
     ]
    }
   ],
   "source": [
    "valid_sigmaV = np.isfinite(df['sigma_V_hat']) & (df['sigma_V_hat'] > 0)\n",
    "valid_inputs = df['E'].gt(0) & df['F'].gt(0) & valid_sigmaV & df['mu_hat'].notna()\n",
    "\n",
    "# Bharath & Shumway naive DD: V_hat=E+F, sigma_D_hat=0.05+0.25*sigma_E,\n",
    "# sigma_V_hat = value-weighted mix, mu_hat = lagged equity return. No solver.\n",
    "df['DD_naive'] = np.where(\n",
    "    valid_inputs,\n",
    "    (np.log(df['V_hat'] / df['F']) + (df['mu_hat'] - 0.5 * df['sigma_V_hat'] ** 2) * T)\n",
    "    / (df['sigma_V_hat'] * math.sqrt(T)),\n",
    "    np.nan,\n",
    ")\n",
    "df['PD_naive'] = np.where(np.isfinite(df['DD_naive']), Phi(-df['DD_naive']), np.nan)\n",
    "df['invalid_sigmaV'] = ~valid_sigmaV\n",
    "\n",
    "print(df[['instrument', 'year', 'DD_naive', 'PD_naive']].head())\n",
    "print(f\"PD==0 count: {(df['PD_naive'] == 0).sum()}, PD==1 count: {(df['PD_naive'] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf02f4d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Data-quality flags and status tracking\n",
    "\n",
    "Capture the first applicable status flag (missing inputs, fallbacks, or imputations) as `naive_status` so downstream users understand how each observation was derived.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a50bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive status counts:\n",
      "naive_status\n",
      "insufficient_returns    475\n",
      "nonpos_EF                16\n",
      "ok                      934\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fallback indicator counts:\n",
      "invalid_sigmaV: 0\n",
      "fallback_E_from_de: 0\n",
      "fallback_E_from_wacc: 0\n",
      "imputed_sigmaE_sizebucket: 483\n",
      "insufficient_returns: 483\n",
      "\n",
      "Equity source mix:\n",
      "E_source\n",
      "E_pb    1425\n",
      "Name: count, dtype: int64\n",
      "Weak equity proxy count: 0\n",
      "\n",
      "DD_naive summary:\n",
      "count    1409.000000\n",
      "mean       20.334403\n",
      "std        14.666043\n",
      "min        -3.665984\n",
      "10%         9.225150\n",
      "25%        12.242458\n",
      "50%        16.616343\n",
      "75%        22.917770\n",
      "90%        34.271849\n",
      "max       155.472624\n",
      "Name: DD_naive, dtype: float64\n",
      "Rows with missing DD_naive: 16\n",
      "\n",
      "PD_naive summary:\n",
      "count     1.409000e+03\n",
      "mean      7.469649e-04\n",
      "std       2.667248e-02\n",
      "min       0.000000e+00\n",
      "10%      1.031884e-257\n",
      "25%      1.545023e-116\n",
      "50%       2.653631e-62\n",
      "75%       9.218141e-35\n",
      "90%       1.415967e-20\n",
      "max       9.998768e-01\n",
      "Name: PD_naive, dtype: float64\n",
      "Rows with missing PD_naive: 16\n",
      "[INFO] Wrote diagnostics to /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/logs/dd_pd_accounting_log.txt\n"
     ]
    }
   ],
   "source": [
    "missing_E = ~np.isfinite(df['E'])\n",
    "missing_F = ~np.isfinite(df['F'])\n",
    "nonpos_EF = (df['E'] <= 0) | (df['F'] <= 0)\n",
    "\n",
    "flag_specs = [\n",
    "    ('invalid_sigmaV', df['invalid_sigmaV']),\n",
    "    ('missing_E', missing_E | (df['E_source'] == 'missing')),\n",
    "    ('missing_F', missing_F),\n",
    "    ('nonpos_EF', nonpos_EF),\n",
    "    ('insufficient_returns', df['insufficient_returns']),\n",
    "    ('imputed_sigmaE_sizebucket', df['imputed_sigmaE_sizebucket']),\n",
    "    ('fallback_E_from_de', df['E_source'] == 'E_de'),\n",
    "    ('fallback_E_from_wacc', df['E_source'] == 'E_wacc'),\n",
    "]\n",
    "\n",
    "for name, mask in flag_specs:\n",
    "    df[name] = mask.astype(bool)\n",
    "\n",
    "\n",
    "def assign_status(idx: int) -> str:\n",
    "    for name, _ in flag_specs:\n",
    "        if bool(df.iloc[idx][name]):\n",
    "            return name\n",
    "    return 'ok'\n",
    "\n",
    "\n",
    "naive_status = [assign_status(i) for i in range(len(df))]\n",
    "df['naive_status'] = naive_status\n",
    "\n",
    "status_counts = df['naive_status'].value_counts(dropna=False).sort_index()\n",
    "print('Naive status counts:')\n",
    "print(status_counts)\n",
    "\n",
    "fallback_summary = {\n",
    "    'invalid_sigmaV': int(df['invalid_sigmaV'].sum()),\n",
    "    'fallback_E_from_de': int(df['fallback_E_from_de'].sum()),\n",
    "    'fallback_E_from_wacc': int(df['fallback_E_from_wacc'].sum()),\n",
    "    'imputed_sigmaE_sizebucket': int(df['imputed_sigmaE_sizebucket'].sum()),\n",
    "    'insufficient_returns': int(df['insufficient_returns'].sum()),\n",
    "}\n",
    "print(\"\\nFallback indicator counts:\")\n",
    "for label, count in fallback_summary.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "e_source_counts = df['E_source'].value_counts(dropna=False).sort_index()\n",
    "print(\"\\nEquity source mix:\")\n",
    "print(e_source_counts)\n",
    "weak_proxy_count = int(df['weak_E_proxy'].sum())\n",
    "print(f\"Weak equity proxy count: {weak_proxy_count}\")\n",
    "\n",
    "percentiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "dd_stats = df['DD_naive'].describe(percentiles=percentiles)\n",
    "pd_stats = df['PD_naive'].describe(percentiles=percentiles)\n",
    "dd_missing = int(df['DD_naive'].isna().sum())\n",
    "pd_missing = int(df['PD_naive'].isna().sum())\n",
    "\n",
    "print(\"\\nDD_naive summary:\")\n",
    "print(dd_stats)\n",
    "print(f\"Rows with missing DD_naive: {dd_missing}\")\n",
    "\n",
    "print(\"\\nPD_naive summary:\")\n",
    "print(pd_stats)\n",
    "print(f\"Rows with missing PD_naive: {pd_missing}\")\n",
    "\n",
    "log_lines = []\n",
    "log_lines.append('=== Naive DD/PD Diagnostics ===')\n",
    "log_lines.append(f'Total rows processed: {len(df)}')\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Naive status counts:')\n",
    "log_lines.extend([f\"{status}: {count}\" for status, count in status_counts.items()])\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Fallback indicator counts:')\n",
    "for label, count in fallback_summary.items():\n",
    "    log_lines.append(f\"{label}: {count}\")\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('Equity source mix:')\n",
    "log_lines.extend([f\"{source}: {count}\" for source, count in e_source_counts.items()])\n",
    "log_lines.append('')\n",
    "log_lines.append(f\"Weak equity proxy count: {weak_proxy_count}\")\n",
    "\n",
    "log_lines.append('DD_naive summary:')\n",
    "log_lines.extend([f\"{idx}: {value}\" for idx, value in dd_stats.items()])\n",
    "log_lines.append(f'Rows with missing DD_naive: {dd_missing}')\n",
    "log_lines.append('')\n",
    "\n",
    "log_lines.append('PD_naive summary:')\n",
    "log_lines.extend([f\"{idx}: {value}\" for idx, value in pd_stats.items()])\n",
    "log_lines.append(f'Rows with missing PD_naive: {pd_missing}')\n",
    "log_lines.append(f\"PD==0 count: {(df['PD_naive'] == 0).sum()}, PD==1 count: {(df['PD_naive'] == 1).sum()}\")\n",
    "\n",
    "log_path = log_dir / 'dd_pd_accounting_log.txt'\n",
    "log_path.write_text(\"\\n\".join(log_lines))\n",
    "print(f\"[INFO] Wrote diagnostics to {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63725a0c",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Persist outputs and quick diagnostics\n",
    "\n",
    "Save the naive DD/PD results and a percentile summary by year for quick reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e98c2df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARCHIVE] Moved to archive: accounting_20251004_041101_summary.csv\n",
      "[ARCHIVE] Moved to archive: accounting_20251004_041101.csv\n",
      "[INFO] Saved accounting DD/PD results to /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/datasheet/accounting_20251004_041436.csv\n",
      "[INFO] Saved percentile summary to /Users/guillaumebld/Documents/Graduate_Research/Professor Abol Jalilvand/fall2025/risk_bank/risk_bank/data/outputs/analysis/accounting_20251004_041436_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>metric</th>\n",
       "      <th>p10</th>\n",
       "      <th>p25</th>\n",
       "      <th>p50</th>\n",
       "      <th>p75</th>\n",
       "      <th>p90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>11.257194</td>\n",
       "      <td>12.792081</td>\n",
       "      <td>15.904340</td>\n",
       "      <td>18.375608</td>\n",
       "      <td>21.397598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>11.464659</td>\n",
       "      <td>13.452838</td>\n",
       "      <td>16.072968</td>\n",
       "      <td>20.064904</td>\n",
       "      <td>23.278789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>11.932022</td>\n",
       "      <td>14.257901</td>\n",
       "      <td>19.569491</td>\n",
       "      <td>36.340615</td>\n",
       "      <td>63.084983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>5.756892</td>\n",
       "      <td>7.714240</td>\n",
       "      <td>11.570772</td>\n",
       "      <td>17.324584</td>\n",
       "      <td>21.724731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>DD_a</td>\n",
       "      <td>8.832195</td>\n",
       "      <td>11.431435</td>\n",
       "      <td>15.670754</td>\n",
       "      <td>22.031471</td>\n",
       "      <td>29.324350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year metric        p10        p25        p50        p75        p90\n",
       "0  2016   DD_a  11.257194  12.792081  15.904340  18.375608  21.397598\n",
       "1  2017   DD_a  11.464659  13.452838  16.072968  20.064904  23.278789\n",
       "2  2018   DD_a  11.932022  14.257901  19.569491  36.340615  63.084983\n",
       "3  2019   DD_a   5.756892   7.714240  11.570772  17.324584  21.724731\n",
       "4  2020   DD_a   8.832195  11.431435  15.670754  22.031471  29.324350"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Archiving and timestamped output setup\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_timestamp_cdt():\n",
    "    \"\"\"Generate timestamp in YYYYMMDD_HHMMSS format (CDT timezone)\"\"\"\n",
    "    cdt = pytz.timezone('America/Chicago')\n",
    "    return datetime.now(cdt).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def archive_old_files(output_dir, archive_dir, dataset_type, max_keep=5):\n",
    "    \"\"\"Move old files of dataset_type to archive, keeping only max_keep most recent\"\"\"\n",
    "    pattern = str(output_dir / f\"{dataset_type}_*.csv\")\n",
    "    old_files = sorted(glob.glob(pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    # Move all existing files to archive\n",
    "    for old_file in old_files:\n",
    "        archive_path = archive_dir / os.path.basename(old_file)\n",
    "        shutil.move(old_file, str(archive_path))\n",
    "        print(f\"[ARCHIVE] Moved to archive: {os.path.basename(old_file)}\")\n",
    "    \n",
    "    # Clean up archive to keep only max_keep files\n",
    "    archive_pattern = str(archive_dir / f\"{dataset_type}_*.csv\")\n",
    "    archive_files = sorted(glob.glob(archive_pattern), key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    \n",
    "    for old_archive in archive_files[max_keep:]:\n",
    "        os.remove(old_archive)\n",
    "        print(f\"[CLEANUP] Removed old archive: {os.path.basename(old_archive)}\")\n",
    "\n",
    "# Rename columns to standard naming convention\n",
    "df = df.rename(columns={'DD_naive': 'DD_a', 'PD_naive': 'PD_a'})\n",
    "\n",
    "result_cols = [\n",
    "    'instrument', 'year', 'E', 'E_source', 'weak_E_proxy', 'E_pb', 'E_de', 'E_wacc',\n",
    "    'F', 'sigma_E', 'sigma_D_hat', 'sigma_V_hat', 'mu_hat',\n",
    "    'DD_a', 'PD_a', 'naive_status'\n",
    "]\n",
    "\n",
    "# Setup archive directory\n",
    "archive_dir = base_dir / 'archive' / 'datasets'\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Archive old accounting files and save new one with timestamp\n",
    "archive_old_files(output_dir, archive_dir, 'accounting', max_keep=5)\n",
    "\n",
    "timestamp = get_timestamp_cdt()\n",
    "dd_output = output_dir / f'accounting_{timestamp}.csv'\n",
    "# Provenance columns for time integrity audit\n",
    "provenance_cols = [\"sigma_E_tminus1\", \"sigmaE_window_start_year\", \n",
    "                   \"sigmaE_window_end_year\", \"mu_hat\", \"mu_hat_from\", \n",
    "                   \"mu_source_year\", \"DD_a\", \"PD_a\"]\n",
    "\n",
    "df[result_cols].to_csv(dd_output, index=False)\n",
    "print(f\"[INFO] Saved accounting DD/PD results to {dd_output}\")\n",
    "\n",
    "cfg = {'T': T, 'ROLL_YEARS': 3, 'WINSOR_P': [0.01, 0.99], 'Phi': 'scipy' if 'norm' in globals() else 'erf_fallback', 'spec': 'Bharath\u2013Shumway naive, no solver, v1'}\n",
    "(Path(output_dir) / 'dd_pd_naive_config.json').write_text(pd.Series(cfg).to_json())\n",
    "\n",
    "percentiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "percentile_columns = [f\"p{int(p * 100)}\" for p in percentiles]\n",
    "\n",
    "\n",
    "def build_percentile_table(metric: str) -> pd.DataFrame:\n",
    "    clean = df[['year', metric]].dropna()\n",
    "    if clean.empty:\n",
    "        empty = {'year': ['overall'], 'metric': [metric]}\n",
    "        for col in percentile_columns:\n",
    "            empty[col] = [np.nan]\n",
    "        return pd.DataFrame(empty)\n",
    "\n",
    "    by_year = (\n",
    "        clean.groupby('year')[metric]\n",
    "             .quantile(percentiles)\n",
    "             .unstack(level=-1)\n",
    "    )\n",
    "    by_year.columns = percentile_columns\n",
    "    by_year = by_year.reset_index()\n",
    "    by_year.insert(0, 'metric', metric)\n",
    "\n",
    "    overall_values = df[metric].dropna()\n",
    "    overall_row = {'metric': metric, 'year': 'overall'}\n",
    "    for col, pct in zip(percentile_columns, percentiles):\n",
    "        overall_row[col] = overall_values.quantile(pct) if not overall_values.empty else np.nan\n",
    "\n",
    "    combined = pd.concat([by_year, pd.DataFrame([overall_row])], ignore_index=True)\n",
    "    ordered_columns = ['year', 'metric', *percentile_columns]\n",
    "    return combined[ordered_columns]\n",
    "\n",
    "\n",
    "summary_frames = [build_percentile_table(metric) for metric in ['DD_a', 'PD_a']]\n",
    "summary = pd.concat(summary_frames, ignore_index=True)\n",
    "\n",
    "summary_output = base_dir / 'data' / 'outputs' / 'analysis' / f'accounting_{timestamp}_summary.csv'\n",
    "summary.to_csv(summary_output, index=False)\n",
    "print(f\"[INFO] Saved percentile summary to {summary_output}\")\n",
    "\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61faab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}